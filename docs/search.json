[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "",
    "text": "NoteWorkshop location\n\n\n\nDate: Wednesday, 19 November 2025, 09:00–16:00\nLocation: University of Melbourne (Parkville campus), Redmond Barry Building (Building 115), Level 5 Room 516\nFormat: Registration via SASP abstract submission; in-person. Before lunch we’ll cover average treatment effects. This module is self-sufficient, and will culminate with student presentations. If you just want to get a sense of causal inference, and learn about causal graphs, the morning will be sufficient. After lunch we’ll get into estimation of heterogeneous treatment effects, and walk through a data exercise that will show you how to estimate such effects, as well as average treatment effects. You will estimate effects using causal machine learning. Building knowledge and skills in this area will be empowering.\nPreparation: Bring your curiosity and attention. If you want to do the data exercise, download R and Rstudio (or your favourite IDE) and then download the workshop R package. (Make sure you install the package first, and run through the scripts as we will not pause to work on your package installation). Bring a writing instrument and some paper, or software that allows free hand writing, as we’ll draw graphs together."
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\nTable 1: Schedule and Learning Goals\n\n\n\n\n\n\nTime\nSession\nLearning goals & activities\n\n\n\n\n09:00–09:45\nHow to ask a causal question\nState precise causal questions with the potential outcomes framework; focus on the average treatment effect (ATE).\n\n\n09:45–10:30\nThe causal workflow: from question to answer\nTrace the causal workflow: define populations, build causal diagrams, assess assumptions, analyse data, conduct sensitivity analysis, and communicate results.\n\n\n10:30–11:00\nCausal diagrams and the identification problem\nUse causal diagrams (DAGs) to identify bias from confounding, colliders, and mediators.\n\n\n11:00–11:30\nCoffee break & discussion (you might need to bring a your own coffee)\nInformal space for questions, clarifying assumptions, and networking.\n\n\n11:30–12:00\nWorked example: employer gratitude and worker wellbeing (Zahle Wisely)\nPresenter: Zahle Wisely. Apply the causal workflow to estimate the effects of employer gratitude on employee wellbeing; Q&A follows.\n\n\n12:00–12:30\nWorked example: religious service attendance and personality change (Hannah Robinson)\nPresenter: Hannah Robinson. Apply the causal workflow to estimate the effects of religious service attendance on personality; Q&A follows.\n\n\n12:30–13:45\nLunch\nBreak.\n\n\n13:45–14:30\nBeyond averages: heterogeneous treatment effects and policy trees\nIntroduce heterogeneous treatment effects (HTE), conditional ATEs, and policy trees; discuss when effects differ across people.\n\n\n14:30–15:00\nRevisiting our examples: policy tree analyses\nAnalyse heterogeneity in Zahle and Hannah's case studies; interpret tree-based decision rules useful for obtaining *policies*.\n\n\n15:00–16:00\nHands-on demonstration: formulating questions and estimating effects in R\nR demonstration using causal forests; interpret simulated ATEs and HTEs, graphs, reporting & etc."
  },
  {
    "objectID": "index.html#workshop-objectives",
    "href": "index.html#workshop-objectives",
    "title": "Beyond Averages: Discovering Who Benefits from Religion?",
    "section": "Workshop Objectives",
    "text": "Workshop Objectives\n\nWhat You’ll Learn\nBy the end of this workshop, you will be able to:\n\nFormulate clear causal questions about the social consequences of religion\nApply the causal inference workflow from question formulation through to communication of results.\nUnderstand Effect Modification and distinguish it from Moderation\nIdentify and address common pitfalls in causal inference with longitudinal data, including time-varying confounding and measurement error.\nUnderstand modern machine learning methods (such as causal forests) to discover both average and heterogenious causal effects in religion \\(\\to\\) prosociality relationships\nUnderstand how to translate causal findings into meaningful insights for diverse stakeholders (government, institutional leaders, religious communities)\nApply insights to your flagship SPARCC Proposal.\n\n\n\n\n\n\n\n\nTipPrerequisites\n\n\n\n\nBasic familiarity with R and regression analysis (someone on your team)\nExpertise in the scientific study of religion\nNo prior knowledge of causal inference required beyond SPARCC Day 1\n\n\n\n\n\n\n\n\n\nNoteWhat to Bring\n\n\n\n\nLaptop.\nDownload R, Rstudio, and the workshop package (as per the above instructions.)\n\n\n\n\n\n\n\n\n\nTipReference Materials\n\n\n\nGlossary & DAGs Reference Page.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPDF Compatibility Note: These presentations work best in Safari. Chrome users may experience issues with the PDF preview - if so, please use the “Open in New Tab” links for the best viewing experience."
  },
  {
    "objectID": "index.html#case-study-2-cate",
    "href": "index.html#case-study-2-cate",
    "title": "Beyond Averages: Discovering Who Benefits from Religion?",
    "section": "Case Study 2: CATE",
    "text": "Case Study 2: CATE\n\n\n\n\n\n\n   Open in New Tab\n\n\n\n\n\n\n\n\n\n\n© 2025 Joseph Bulbulia. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "content/01-background.html",
    "href": "content/01-background.html",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "",
    "text": "NoteSugessted Readings\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html"
  },
  {
    "objectID": "content/01-background.html#definitions",
    "href": "content/01-background.html#definitions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Definitions",
    "text": "Definitions\n\nDefinition 1 We say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as defined at baseline.\n\n\nDefinition 2 We say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as defined at baseline.\n\nThe concept of “confounding bias” helps to clarify what it is at stake when evaluating the internal validity of a study. As we shall see, there are several equivalent definitions of “confounding bias,” which we will describe during the upcoming weeks.\nThe definition of confounding bias that we will examine today is:\n\nDefinition 3 We say there is confounding bias if there is an open back-door path between the treatment and outcome or if the path between the treatment and outcome is blocked.\n\nToday, our purpose will be to clarify the meaning of each term in this definition. To that end, we will introduce the five elementary graphical structures employed in causal diagrams. We will then explain the four elementary rules that allow investigators to identify causal effects from the asserted relations in a causal diagram. First, what are causal diagrams?"
  },
  {
    "objectID": "content/01-background.html#introduction-to-causal-diagrams.",
    "href": "content/01-background.html#introduction-to-causal-diagrams.",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Introduction to Causal Diagrams.",
    "text": "Introduction to Causal Diagrams.\nCausal diagrams, also called causal graphs, Directed Acyclic Graphs, and Causal Directed Acyclic Graphs, are graphical tools whose primary purpose is to enable investigators to detect confounding biases.\nRemarkably, causal diagrams are rarely used in psychology!\nBefore describing how causal diagrams work, we first define the meanings of their symbols. Note there is no single convention for creating causal diagrams, so it is important that we are clear when defining our meanings.\n\nThe meaning of our symbols\nThe conventions that describe the meanings of our symbols are given in Figure 1.\n\n\n\n\n\n\nFigure 1: Our variable naming conventions. This figure is adapted from (Bulbulia 2024)\n\n\n\nFor us:\nX denotes a variable without reference to its role;\nA denotes the “treatment” or “exposure” variable. This is the variable for which we seek to understand the effect of intervening on it. It is the “cause;”\nY denotes the outcome or response of an intervention. It is the “effect.” Last week we considered whether marriage A causes happiness Y.\nY(a) denotes the counterfactual or potential state of Y in response to setting the level of the exposure to a specific level, A=a. As we will consider in the second half of the course, to consistently estimate causal effects we will need to evaluate counterfactual or potential states of the world. Keeping to our example, we will need to do more than evaluate marriage and happiness in people over time. We will need to evaluate how happy the unmarried people would have been had they been married and how happy the married people would have been had they not been married. Of course, these events cannot be directly observed. Thus to address fundamental questions in psychology, we need to contrast counterfactual states of the world. This might seem like science fiction; however, we are already familiar with methods for obtaining such counterfactual contrasts – namely, randomised controlled experiments! We will return to this concept later, but for now, it will be useful for you to understand the notation.\nL denotes a measured confounder or set of confounders is defined as a variable which, if conditioned upon, closes an open back-door path between the treatment A and the outcome Y. Consider the scenario where happiness at time 0 (L) affects both the probability of getting married at time 1 (A) and one’s happiness at time 2 (Y). In this case, L serves as a confounder because it influences both the treatment (marriage at time 1) and the outcome (happiness at time 2), potentially opening a back-door path that confounds the estimated effect of marriage on happiness.\nTo accurately estimate the causal effect of marriage on happiness, then, it is essential to control for L. With cross-sectional data, such control might be difficult.\nU denotes an unmeasured confounder – that is a variable that may affect both the treatment and the outcome, but for which we have no direct measurement. Suppose cultural upbringing affects both whether someone gets married and whether they are happy. If this variable is not measured, we cannot accurately estimate a causal effect of marriage on happiness.\nM denotes a mediator or a variable along the path from exposure to outcome. For example, perhaps marriage causes wealth and wealth causes happiness. As we shall see, conditioning on “wealth” when estimating the effect of marriage on happiness will make it seem that marriage does not cause happiness when it does, through wealth.\n\\bar{X} denotes a sequence of variables, for example, a sequence of treatments. Imagine we were interested in the causal effect of marriage and remarriage on well-being. In this case, there are two treatments A_0 and A_1 and four potential contrasts. For the scenario of marriage and remarriage affecting well-being, we denote the potential outcomes as Y(a_0, a_1), where a_0 and a_1 represent the specific values taken by A_0 and A_1, respectively. Given two treatments, A_0 and A_1, four primary contrasts of interest correspond to the different combinations of these treatments. These contrasts allow us to compare the causal effects of being married versus not and remarried versus not on well-being. The potential outcomes under these conditions can be specified as follows:\n\nY(0, 0): The potential outcome when there is no marriage.\nY(0, 1): The potential outcome when there is marriage.\nY(1, 0): The potential outcome when there is divorce.\nY(1, 1): The potential outcome from marriage prevalence.\n\nEach of these outcomes allows for a specific contrast to be made, comparing the well-being under different scenarios of marriage and remarriage. Which do we want to contrast? Note, the question about ‘the causal effects of marriage on happiness’ is ambiguous because we have not stated the causal contrast we are interested in.\n\\mathcal{R} denotes a randomisation or a chance event.\n\n\nElements of our Causal Graphs\nThe conventions that describe components of our causal graphs are given in Figure 2.\n\n\n\n\n\n\nFigure 2: Nodes, Edges, Conditioning Conventions. This figure is adapted from (Bulbulia 2024)\n\n\n\n\nTime indexing\nIn our causal diagrams, we will implement two conventions to accurately depict the temporal order of events.\nFirst, the layout of a causal diagram will be structured from left to right to reflect the sequence of causality as it unfolds in reality. This orientation is crucial because causal diagrams must inherently be acyclic and because causality itself is inherently temporal.\nSecond, we will enhance the representation of the event sequence within our diagrams by systematically indexing our nodes according to the relative timing of events. If an event represented by X_0 precedes another event represented by X_1, the indexing will indicate this chronological order.\n\n\nRepresenting uncertainty in timing explicitly\nIn settings in which the sequence of events is ambiguous or cannot be definitively known, particularly in the context of cross-sectional data where all measurements are taken at a single point in time, we adopt a specific convention to express causality under uncertainty: X_{\\phi t}. This notation allows us to propose a temporal order without clear, time-specific measurements, acknowledging our speculation.\nFor instance, when the timing between events is unclear, we denote an event that is presumed to occur first as X_{\\phi 0} and a subsequent event as X_{\\phi 1}, indicating a tentative ordering where X_{\\phi 0} is thought to precede X_{\\phi 1}. However, it is essential to underscore that this notation signals our uncertainty regarding the actual timing of events; our measurements do not give us the confidence to assert this sequence definitively.\n\n\nArrows\nAs indicated in Figure 2, black arrows denote causality, red arrows reveal an open backdoor path, dashed black arrows denote attenuation, and red dashed arrows denote bias in a true causal association between A and Y. Finally, a blue arrow with a circle point denotes effect-measure modification, also known as “effect modification.” We might be interested in treatment effect heterogeneity without evaluating the causality in the sources of this heterogeneity. For example, we cannot typically imagine any intervention in which people could be randomised into cultures. However, we may be interested in whether the effects of an intervention that might be manipulable, such as marriage, differ by culture. To clarify this interest, we require a non-causal arrow.\n\\mathcal{R}\\to A denotes a random treatment assignment.\n\n\nBoxes\nWe use a black box to denote conditioning that reduces confounding or that is inert.\nWe use a red box to describe settings in which conditioning on a variable introduces confounding bias.\nOccasionally we will use a dashed circle do denote a latent variable, that is, a variable that is either not measured or not conditioned upon.\n\n\nTerminology for Conditional Independence\nThe bottom panel of Figure 2 shows some mathematical notation. Do not be alarmed, we are safe! Part 1 of the course will not require more complicated math than this notation. And we shall see that the notation is a compact way to describe intuitions that can be expressed less compactly in words:\n\nStatistical Independence (\\coprod): in the context of causal inference, statistical independence between the treatment and potential outcomes, denoted as A \\coprod Y(a), means the treatment assignment is independent of the potential outcomes. This assumption is critical for estimating causal effects without bias.\nStatistical Dependence (\\cancel\\coprod): conversely, \\cancel\\coprod denotes statistical dependence, indicating that the distribution of one variable is influenced by the other. For example, A \\cancel\\coprod Y(a) implies that the treatment assignment is related to the potential outcomes, potentially introducing bias into causal estimates.\nConditioning (|): conditioning, denoted by the vertical line |, allows for specifying contexts or conditions under which independence or dependence holds.\n\nConditional Independence (A \\coprod Y(a)|L): This means that once we account for a set of variables L, the treatment and potential outcomes are independent. This condition is often the basis for strategies aiming to control for confounding.\nConditional Dependence (A \\cancel\\coprod Y(a)|L): States that potential outcomes and treatments are not independent after conditioning on L, indicating a need for careful consideration in the analysis to avoid biased causal inferences."
  },
  {
    "objectID": "content/01-background.html#the-five-elementary-structures-of-causality",
    "href": "content/01-background.html#the-five-elementary-structures-of-causality",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "The Five Elementary Structures of Causality",
    "text": "The Five Elementary Structures of Causality\nJudea Pearl proved that all elementary structures of causality can be represented graphically (Pearl 2009). Figure 3 presents this five elementary structures.\n\n\n\n\n\n\nFigure 3: Five elementary structures. This figure is adapted from (Bulbulia 2024).\n\n\n\nThe structures are as follows:\n\nTwo Variables:\n\nCausality Absent: There is no causal effect between variables A and B. They do not influence each other, denoted as A \\coprod B, indicating they are statistically independent.\nCausality: Variable A causally affects variable B. This relationship suggests an association between them, denoted as A \\cancel\\coprod B, indicating they are statistically dependent.\n\nThree Variables:\n\nFork: Variable A causally affects both B and C. Variables B and C are conditionally independent given A, denoted as B \\coprod C | A. This structure implies that knowing A removes any association between B and C due to their common cause.\nChain: A causal chain exists where C is affected by B, which in turn is affected by A. Variables A and C are conditionally independent given B, denoted as A \\coprod C | B. This indicates that B mediates the effect of A on C, and knowing B breaks the association between A and C.\nCollider: Variable C is affected by both A and B, which are independent. However, conditioning on C induces an association between A and B, denoted as A \\cancel\\coprod B | C. This structure is unique because it suggests that A and B, while initially independent, become associated when we account for their common effect C.\n\n\nOnce we understand the basic relationships between two variables, we can build upon these to create more complex relationships. These structures help us see how statistical independences and dependencies emerge from the data, allowing us to clarify the causal relationships we presume exist. Such clarity is crucial for ensuring that confounders are balanced across treatment groups, given all measured confounders, so that Y(a) \\coprod A | L.\nYou might wonder, “If not from the data, where do our assumptions about causality come from?” This question will come up repeatedly throughout the course. The short answer is that our assumptions are based on existing knowledge. This reliance on current knowledge might seem counterintuitive for buiding scientific knowledge-— shouldn’t we use data to build knowledge, not the other way around? Yes, but it is not that straightforward. Data often hold the answers we’re looking for but can be ambiguous. When the causal structure is unclear, it is important to sketch out different causal diagrams, explore their implications, and, if necessary, conduct separate analyses based on these diagrams.\nOtto Neurath, an Austrian philosopher and a member of the Vienna Circle, famously used the metaphor of a ship that must be rebuilt at sea to describe the process of scientific theory and knowledge development.\n\nDuhem has shown … that every statement about any happening is saturated with hypotheses of all sorts and that these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. (Neurath 1973, 199)\n\nThis quotation emphasises the iterative process that accumulates scientific knowledge; new insights are cast from the foundation of existing knowledge. Causal diagrams are at home in Neurath’s boat. The tradition of science that believes that knowledge develops from the results of statistical tests applied to data should be resisted. The data alone typically do not contain the answers we seek."
  },
  {
    "objectID": "content/01-background.html#our-conventions",
    "href": "content/01-background.html#our-conventions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Our Conventions",
    "text": "Our Conventions\nFigure 4 presents our conventions:\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/S1-graphical-key.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 4: Graphical conventions and symbol key"
  },
  {
    "objectID": "content/01-background.html#the-four-rules-of-confounding-control",
    "href": "content/01-background.html#the-four-rules-of-confounding-control",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "The Four Rules of Confounding Control",
    "text": "The Four Rules of Confounding Control\nFigure 4 describe the four elementary rules of confounding control:\n\n\n\n\n\n\nFigure 4: Four rules of confounding control\n\n\n\n\nCondition on Common Cause or its Proxy: this rule applies to settings in which the treatment (A) and the outcome (Y) share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps tp isolate the specific effect of A on Y. (We do not draw a path from $ A Y$ because we do not assume this path.)\nDo Not Condition on a Mediator: this rule applies to settings in which the variable L is a mediator of A \\to Y. Here, conditioning on a mediator will bias the total causal effect estimate. Later in the course, we will discuss the assumptions required for causal mediation. For now, if we are interested in total effect estimates, we must not condition on a mediator. Here we draw the path from A \\to Y to ensure that if such a path exists, it will not become biased from our conditioning strategy.\nDo Not Condition on a Collider: this rule applies to settings in which we L is a common effect of A and Y. Conditioning on a collider may invoke a spurious association. Last week we considered an example in which marriage caused wealth and happiness caused wealth. Conditioning on wealth in this setting will induce an association between happiness and marriage. Why? If we know the outcome, wealth, then we know there are at least two ways of wealth. Among those wealthy but low on happiness, we can predict that they are more likely to be married, for how else would they be wealthy? Similarly, among those who are wealthy and are not married, we can predict that they are happy, for how else would they be wealthy if not through marriage? These relationships are predictable entirely without a causal association between marriage and happiness!\nProxy Rule: Conditioning on a Descendent Is Akin to Conditioning on Its Parent: this rule applies to settings in which we L’ is an effect from another variable L. The graph considers when L’ is downstream of a collider. For example, suppose we condition on home ownership, which is an effect of wealth. Such conditioning will open up a non-causal path without causation because home ownership is a proxy for wealth. Consider, if someone owns a house but is not married, they are more likely to be happy, for how else could they accumulate the wealth required for home ownership? Likewise, if someone is unhappy and owns a house, we can infer that they are more likely to be married because how else would they be wealthy? Conditioning on a proxy for a collider here is akin to conditioning on the collider itself.\n\nHowever, we can also use the proxy rule to reduce bias. Return to the earlier example in which there is an unmeasured common cause of marriage and happiness, which we called “cultural upbringing” Suppose we have not measured this variable but have measured proxies for this variable, such as country of birth, childhood religion, number of languages one speaks, and others. By controlling for baseline values of these proxies, we can exert more control over unmeasured confounding. Even if bias is not eliminated, we should reduce bias wherever possible, which includes not introducing new biases, such as mediator bias, along the way. Later in the course, we will teach you how to perform sensitivity analyses to verify the robustness of your results to unmeasured confounding. Sensitivity analysis is critical because where the data are observational, we cannot entirely rule out unmeasured confounding."
  },
  {
    "objectID": "content/01-background.html#how-time-series-data-can-spare-effort",
    "href": "content/01-background.html#how-time-series-data-can-spare-effort",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "How Time Series Data Can Spare Effort",
    "text": "How Time Series Data Can Spare Effort\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/3-how-time-series-address-confounding-bias.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 5: How time-series data spare us thinking about confounding biases (Bulbulia 2024)."
  },
  {
    "objectID": "content/01-background.html#why-time-series-data-are-insufficient",
    "href": "content/01-background.html#why-time-series-data-are-insufficient",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Why Time-Series Data Are Insufficient",
    "text": "Why Time-Series Data Are Insufficient\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/3-how-time-series-address-confounding-bias.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 6: How time-series data spare us thinking about confounding biases (Bulbulia 2024)."
  },
  {
    "objectID": "content/01-background.html#why-time-series-data-are-insufficient-1",
    "href": "content/01-background.html#why-time-series-data-are-insufficient-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Why Time-Series Data Are Insufficient",
    "text": "Why Time-Series Data Are Insufficient\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/5-dags-show-time-series-not-resolved.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 7: How time-series data spare us thinking about confounding biases (Bulbulia 2024)."
  },
  {
    "objectID": "content/01-background.html#effect-modification",
    "href": "content/01-background.html#effect-modification",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Effect Modification",
    "text": "Effect Modification\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/6-effectmodification.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 8: Effect-modification is not Moderation (Bulbulia 2024)."
  },
  {
    "objectID": "content/01-background.html#structural-representation-of-measurement-error-bias",
    "href": "content/01-background.html#structural-representation-of-measurement-error-bias",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Structural Representation of Measurement Error Bias",
    "text": "Structural Representation of Measurement Error Bias\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/8-structural-representation-measurement-error-bias.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 9: Causal DAGs can be useful for examiningn some types of measurement error bias(Bulbulia 2024)."
  },
  {
    "objectID": "content/01-background.html#structural-representation-of-external-validity-as-measurement-error-bias",
    "href": "content/01-background.html#structural-representation-of-external-validity-as-measurement-error-bias",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Structural Representation of External Validity as Measurement Error Bias",
    "text": "Structural Representation of External Validity as Measurement Error Bias\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/9-external-validity-as-measurement-error.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 10: External validity as measurement error bias(Bulbulia 2024)."
  },
  {
    "objectID": "content/01-background.html#connfounding-and-selection-bias-in-experiments",
    "href": "content/01-background.html#connfounding-and-selection-bias-in-experiments",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Connfounding and Selection Bias in Experiments",
    "text": "Connfounding and Selection Bias in Experiments\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/10-confounding-selection-bias-experiments.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 11: How experiments fail."
  },
  {
    "objectID": "content/01-background.html#mediator-bias",
    "href": "content/01-background.html#mediator-bias",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Mediator Bias",
    "text": "Mediator Bias\n\n\n\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"/pdfs/bulbulia-hand-outs/S9-mediator-bias.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;\n\n\n\nFigure 12: How experiments fail."
  },
  {
    "objectID": "content/01-background.html#footnotes",
    "href": "content/01-background.html#footnotes",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“What if?” questions implicitly invoke the idea of intervening on the world. “If we did this, then what would happen to that…?” Our preferred terminology reflects our interest in the effects of interventions.↩︎"
  },
  {
    "objectID": "content/00-background.html",
    "href": "content/00-background.html",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "",
    "text": "Note\n\n\n\nSuggested Readings - (Hernan and Robins 2020b) Chapters 1-3 link\n\n(Neal 2020) Chapter 1-2 link"
  },
  {
    "objectID": "content/00-background.html#objectives",
    "href": "content/00-background.html#objectives",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Objectives",
    "text": "Objectives\n\nYou will understand why causation is never directly observed.\nYou will understand how experiments address this “causal gap.”\nYou will understand how applying three principles from experimental research allows human scientists to close this “causal gap” when making inferences about a population as a whole — that is, inferences about “marginal effects.”"
  },
  {
    "objectID": "content/00-background.html#s-observational-studies-indicated-30-all-cause-mortality-reduction-from-estrogen-therapies",
    "href": "content/00-background.html#s-observational-studies-indicated-30-all-cause-mortality-reduction-from-estrogen-therapies",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "1990s observational studies indicated 30% all-cause mortality reduction from estrogen therapies?",
    "text": "1990s observational studies indicated 30% all-cause mortality reduction from estrogen therapies?\n\n\n\n\n\n\nNoteIn the 1980s and 1990s estrogen treatments appeared to benefit postmenopausal women\n\n\n\nHazard ratio for all-cause mortality: 0.68 current users vs. never users (Grodstein, Manson, and Stampfer 2006)"
  },
  {
    "objectID": "content/00-background.html#standard-medical-advice",
    "href": "content/00-background.html#standard-medical-advice",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Standard Medical Advice",
    "text": "Standard Medical Advice\n\n1992 American College of Obstetricians and Gynecologists {“Probable beneficial effect of estrogens on heart disease.”}\n1992 American College of Physicians “Women who have coronary heart disease or who are at increased risk of coronary heart disease are likely to benefit from hormone therapy.”}\n1993 National Cholesterol Education Program {“Epidemiologic evidence for the benefit of estrogen replacement therapy is especially strong for secondary prevention in women with prior CHD.”}\n1996 American Heart Association {“ERT does look promising as a long-term protection against heart attack.”}"
  },
  {
    "objectID": "content/00-background.html#womens-health-initiative-evaluate-estrogens-experimentally",
    "href": "content/00-background.html#womens-health-initiative-evaluate-estrogens-experimentally",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Women’s Health Initiative: Evaluate Estrogens Experimentally",
    "text": "Women’s Health Initiative: Evaluate Estrogens Experimentally\n\nMassive randomised, double-blind, placebo-controlled trial\n16,000 U.S. women aged 50-79 years\nAssigned to Estrogen plus Progestin therapy\nWomen followed approximately every year for up to 8 years."
  },
  {
    "objectID": "content/00-background.html#findings-clear-discrepancy",
    "href": "content/00-background.html#findings-clear-discrepancy",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Findings: Clear Discrepancy",
    "text": "Findings: Clear Discrepancy\n\n\n\n\n\n\nWarningExperimental Findings: opposite of observational findings\n\n\n\nAll-cause mortality Hazard Ratio: 1.23 for initiators vs non-initiators. (Manson et al. 2003)"
  },
  {
    "objectID": "content/00-background.html#medical-community-response-reject-all-observational-studies",
    "href": "content/00-background.html#medical-community-response-reject-all-observational-studies",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Medical community response: Reject all observational studies",
    "text": "Medical community response: Reject all observational studies\n\nCan observational studies ever be trusted?\nShould observational studies ever be funded again?\n{What went wrong?}"
  },
  {
    "objectID": "content/00-background.html#opening",
    "href": "content/00-background.html#opening",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Opening",
    "text": "Opening\n\nRobert Frost writes:\n\nTwo roads diverged in a yellow wood,\nAnd sorry I could not travel both\nAnd be one traveler, long I stood\nAnd looked down one as far as I could\nTo where it bent in the undergrowth;\nThen took the other, as just as fair,\nAnd having perhaps the better claim,\nBecause it was grassy and wanted wear;\nThough as for that the passing there\nHad worn them really about the same,\nAnd both that morning equally lay\nIn leaves no step had trodden black.\nOh, I kept the first for another day!\nYet knowing how way leads on to way,\nI doubted if I should ever come back.\nI shall be telling this with a sigh\nSomewhere ages and ages hence:\nTwo roads diverged in a wood, and I—\nI took the one less traveled by,\nAnd that has made all the difference.\n– The Road Not Taken"
  },
  {
    "objectID": "content/00-background.html#introduction-motivating-example",
    "href": "content/00-background.html#introduction-motivating-example",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Introduction: Motivating Example",
    "text": "Introduction: Motivating Example\nConsider the following question:\n\nAlice attends religious service regularly. Does this increase her volunteering?\n\nThere is evidence that people who attend religious volunteer more, but would Alice volunteer anyway?\n“And sorry I could not travel both. And be one traveler \\dots”"
  },
  {
    "objectID": "content/00-background.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "href": "content/00-background.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem",
    "text": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem\nThe fundamental problem of causal inference is that causality is never directly observed.\nLet Y an’a’d A denote random variables.\nWe formulate a causal question by asking whether experiencing a exposure A, when this exposure is set to level A = a, would lead to a difference in Y, compared to what would have occurred had the exposure been set to a different level, say A=a' will lead to a difference in outcome Y. For simplicity, we imagine binary exposure such that A = 1 denotes receiving the “religious service” exposure and A = 0 denotes receiving the “no religious service” exposure. Assume these are the only two exposures of interest:\nLet:\n\nY_i(a = 1) denotes Alice’s volunteering if Alice attended religious service (potential outcome when A_i = 1).\nY_i(a = 0) denotes Alice’s volunteering if Alice did not attend religious service (potential outcome when A_i = 0).\n\nWhat does it mean to quantify a causal effect.\nWe may define the individual-level causal effect of religious service on volunteering for a Alice (i) as the difference between two states of the world: one for which Alice experiences regular religious service and the other, not. We write this contrast by referring to the potential outcomes under different levels of exposure:\n\n\\text{Causal Effect}_i = Y_i(1) - Y_i(0).\n\nWe say there is a causal effect of religious service attendance if\n\nY_i(1) - Y_i(0) \\neq 0.\n\nNote, however – at most, only one of these two states can be observed.\nBecause each person experiences only one exposure condition in reality, we cannot directly compute this difference from any dataset — the missing observation is called the counterfactual:\n\nIf Y_i|A_i = 1 is observed, then Y_i(0)|A_i=1 is counterfactual.\nIf Y_i|A_i = 0 is observed, then Y_i(1)|A_i=1 is counterfactual.\n\nFrost put it this way:\n“And sorry I could not travel both / And be one traveler, long I stood \\dots”\nIn short, individuals cannot simultaneously experience both exposure conditions, so we cannot quantitatively estimate individual-level causal effects (generally) because one outcome is inevitably missing.\n\nHow can we make contrasts between counterfactual (potential) outcomes?\n\nFundamental Assumption 1: Causal Consistency\nCausal consistency means that the potential outcome corresponding to the exposure an individual actually receives is exactly what we observe. In other words, if individual i receives exposure a, then the potential outcome (or equivalently the counterfactual outcome under a given level of exposure A=a – that is Y_i(a) – is equivalent to the the observed outcome: Y_i \\mid A_i \\equiv a. Where the symbol \\equiv means “equivalent to”, when we assume that the causal consistency assumption is satisfied, we assume that:\n\n\\begin{aligned}\n\\underbrace{Y_i(1)}_{\\text{counterfactual}} &\\equiv \\underbrace{(Y_i \\mid A_i = 1)}_{\\text{observable}}, \\\\\n\\underbrace{Y_i(0)}_{\\text{counterfactual}} &\\equiv \\underbrace{(Y_i \\mid A_i = 0)}_{\\text{observable}}.\n\\end{aligned}\n\nNotice however that we cannot generally obtain individual causal effects because at any given time, each individual may only receive at most one leve of an exposure. Where the symbol \\implies means “implies,” at any given time, receiving one level of an exposure precludes receiving any other level of that exposure:\n\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\text{is counterfactual}\n Likewise:\n\nY_i|A_i = 0 \\implies Y_i(1)|A_i = 1~ \\text{is counterfactual}\n\nBecause of the laws of physics (above the atomic scale), an individual can experience only one exposure level at any moment. Consequently, we can observe only one of the two counterfactual outcomes needed to quantify a causal effect. This is the fundamental problem of causal inference. Counterfactual contrasts cannot be individually observed.\nHowever, because of the causal consistency assumption, we can nevertheless recover half of the missing counterfactual (or “potential”) outcomes needed to estimate average treatment effects. We may do this if two other assumptions are satisfied.\n\n\nFundamental Assumption 2: Exchangeability\nExchangeability justifies recovering unobserved counterfactuals from observed outcomes and averaging them. By accepting that Y_i(a) = Y_i if A_i = a, we can estimate population-level average potential outcomes. In an experiment where exposure groups are comparable, we define the Average Treatment Effect (ATE) as:\n\n\\begin{aligned}\n\\text{ATE} &= \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] \\\\\n           &= \\mathbb{E}(Y \\mid A=1) \\;-\\; \\mathbb{E}(Y \\mid A=0).\n\\end{aligned}\n Because randomisation (or more generally control over the probability of receiving treatment) ensures that missing counterfactuals are exchangeable with those observed, we can still estimate \\mathbb{E}[Y(a)]. For instance, assume:\n\n\\underbrace{\\mathbb{E}[Y(1)\\mid A=1]}_{\\text{counterfactual}}  = \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobservable}}}  =  \\underbrace{(Y_i \\mid A_i = 1)}_{\\text{observed}}\n\nwhich lets us infer the average outcome if everyone were treated. Likewise, if\n\n\\underbrace{\\mathbb{E}[Y(0)\\mid A=0]}_{\\text{counterfactual}}  = \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobservable}}}  =  \\underbrace{\\mathbb{E}(Y \\mid A_i = 0)}_{\\text{observed}}\n\nthen we can infer the average outcome if everyone were given the control. The difference between these two quantities gives the ATE:\n\n\\text{ATE} = \\Big[\n\\overbrace{\\mathbb{E}[Y(1)\\mid A=1]}^{\\substack{\\text{by consistency:}\\\\ \\equiv \\text{ observed } \\; \\mathbb{E}[Y\\mid A=1]}}\n\\;+\\;\n\\overbrace{\\textcolor{red}{\\mathbb{E}[Y(1)\\mid A=0]}}^{\\substack{\\text{by exchangeability:}\\\\ \\text{unobservable, yet } \\; \\equiv \\mathbb{E}[Y\\mid A=1]}}\n\\Big]\n-\\,\n\\Big[\n\\overbrace{\\mathbb{E}[Y(0)\\mid A=0]}^{\\substack{\\text{by consistency:}\\\\ \\equiv  \\text{observed } \\; \\mathbb{E}[Y\\mid A=0]}}\n\\;+\\;\n\\overbrace{\\textcolor{red}{\\mathbb{E}[Y(0)\\mid A=1]}}^{\\substack{\\text{by exchangeability:}\\\\ \\text{unobservable, yet } \\; \\equiv \\mathbb{E}[Y\\mid A=0]}}\n\\Big]\n\nWe have it that \\mathbb{E}[Y\\mid A=1] and \\mathbb{E}[Y\\mid A=0] and \\mathbb{E}[Y(1)\\mid A=0] are observed. If both consistency and exchangeability are satisifed then we may use these observed quantities to identify contrasts of counterfactual quanities.\nThus, although individual-level counterfactuals are missing, the consistency assumptions and the exchangeability assumptions allow us to identify the average effect of treatment using observed data. Randomised controlled experiments allow us to meet these assumptions. Randomisation warrents the exchangeability assumption. Control warrents the consistency asumption.\n\n\nFundamental Assumption 3: Positivity\nThere is one further assumption, called positivity. It states that treatment assignments cannot be deterministic. That is, for every covariate pattern L = l, each individual has a non-zero probability of receiving ever treatment level to be compared:\n\nP(A = a \\mid L = l) &gt; 0.\n\nRandomised experiments achieve positivity by design – at least for the sample that is selected into the study. In observational settings violations occur if some subgroups never receive a particular treatment. If treatments occur but are rare, we may have sufficient data from which to obtain convincing causal inferences.\nPositivity is the only assumption that can be verified with data.\n\n\n\nChallenges with Observational Data\n\n1. Satisfying Causal Consistency is Difficult in Observational Settings\nBelow are some ways in which real-world complexities can violate causal consistency in observational studies. For example, causal consistency requires there is no interference between units (also called “SUTVA” or “Stable Unit Treatment Value.” Causal consistency also requires that each treatment level is well-defined and applied uniformly. If these conditions fail, then Y(a) may not reflect a consistent exposures across individuals. We are then comparing apples with oranges. Consider some examples:\n\nCultural Dependence: one group’s “religious service” will differ qualitatively from another’s. Attending a !Kung! healing ritual and an Aztec human sacrifice may, plausible, produce different responses in people. In one, charity, in the other, terror.\nSocial Dependencies: Watts sees Alice gives, so Watts gives too. Here, effects under treatment differs depending on how others respond.\n\nIf the actual exposures differ across individuals, then consistency (Y_i(a) = Y_i \\mid A_i) may fail, because A=a is not the same phenomenon for everyone.\n\n\n2. Conditional Exchangeability (No Unmeasured Confounding) Is Difficult to Achieve\nIn theory, we can identify a causal effect from observational data if all confounders L are measured. Formally, we need the potential outcomes to be independent of treatment once we condition on L. One way to express this asssumption is: Y(a) \\coprod A \\mid L. If the potential outcomes are independent of treatment assignment, we can identify the Average Treatment Effect (ATE) as: \n\\text{ATE} \\;=\\; \\sum_{l}\n\\Bigl[\\mathbb{E}\\bigl(Y \\mid A=1, L=l\\bigr) \\;-\\; \\mathbb{E}\\bigl(Y \\mid A=0, L=l\\bigr)\\Bigr] \\;\\Pr(L=l).\n\nIn randomised experiments, conditioning is automatic because A is unrelated to potential outcomes by design. In observational studies, ensuring or approximating such conditional exchangeability is often difficult. For example, bilingualism research would need to consider:\n\nCultural histories: cultures that value language acquistion might also value knowledge acquistion. Associations might arise from Culture, not causation.\nPersonal values: families who place a high priority on bilingualism may also promote other developmental resources.\n\nIf important confounders go unmeasured or are poorly measured, these differences can bias causal effect estimates.\n\n\n\n3. The Positivity Assumption May Fail: Treatments Might Not Exist for All\nPositivity requires that each individual could, in principle, receive any exposure level. But in real-world observational settings, some groups have no access to bilingual education (or no reason to be monolingual), making certain treatment levels impossible for them. If a treatment level does not appear in the data for a given subgroup, any causal effect estimate for that subgroup is purely an extrapolation (Westreich and Cole 2010; Hernan and Robins 2020a).\n\n\nSummary\nWe introduced the fundamental problem of causal inference by distinguishing correlation (associations in the data) from causation (contrasts between potential outcomes, of which only one can be observed for each individual).\nRandomised experiments address this problem by balancing confounding variables across treatment levels. Although individual causal effects are unobservable, random assignment allows us to infer average causal effects — also called marginal effects.\nIn observational data, inferring average treatment effects demands that we satisfy three assumptions that are automatically satisfied in (well-conducted) experiments: causal consistency, exchangeability, and positivity. These assumptions ensure that we can compare like-with-like (that the population-level treatment effect is consistent across individuals), that there are no unmeasured common causes of the exposure and outcomes that may lead to associations in the absence of causality, and that every exposure level is a real possibility for each subgroup."
  },
  {
    "objectID": "content/00-background.html#how-can-we-make-contrasts-between-counterfactual-potential-outcomes",
    "href": "content/00-background.html#how-can-we-make-contrasts-between-counterfactual-potential-outcomes",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "How can we make contrasts between counterfactual (potential) outcomes?",
    "text": "How can we make contrasts between counterfactual (potential) outcomes?\n\nFundamental Assumption 1: Causal Consistency\nCausal consistency means that the potential outcome corresponding to the exposure an individual actually receives is exactly what we observe. In other words, if individual i receives exposure a, then the potential outcome (or equivalently the counterfactual outcome under a given level of exposure A=a – that is Y_i(a) – is equivalent to the the observed outcome: Y_i \\mid A_i \\equiv a. Where the symbol \\equiv means “equivalent to”, when we assume that the causal consistency assumption is satisfied, we assume that:\n\n\\begin{aligned}\n\\underbrace{Y_i(1)}_{\\text{counterfactual}} &\\equiv \\underbrace{(Y_i \\mid A_i = 1)}_{\\text{observable}}, \\\\\n\\underbrace{Y_i(0)}_{\\text{counterfactual}} &\\equiv \\underbrace{(Y_i \\mid A_i = 0)}_{\\text{observable}}.\n\\end{aligned}\n\nNotice however that we cannot generally obtain individual causal effects because at any given time, each individual may only receive at most one leve of an exposure. Where the symbol \\implies means “implies,” at any given time, receiving one level of an exposure precludes receiving any other level of that exposure:\n\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\text{is counterfactual}\n Likewise:\n\nY_i|A_i = 0 \\implies Y_i(1)|A_i = 1~ \\text{is counterfactual}\n\nBecause of the laws of physics (above the atomic scale), an individual can experience only one exposure level at any moment. Consequently, we can observe only one of the two counterfactual outcomes needed to quantify a causal effect. This is the fundamental problem of causal inference. Counterfactual contrasts cannot be individually observed.\nHowever, because of the causal consistency assumption, we can nevertheless recover half of the missing counterfactual (or “potential”) outcomes needed to estimate average treatment effects. We may do this if two other assumptions are satisfied.\n\n\nFundamental Assumption 2: Exchangeability\nExchangeability justifies recovering unobserved counterfactuals from observed outcomes and averaging them. By accepting that Y_i(a) = Y_i if A_i = a, we can estimate population-level average potential outcomes. In an experiment where exposure groups are comparable, we define the Average Treatment Effect (ATE) as:\n\n\\begin{aligned}\n\\text{ATE} &= \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] \\\\\n           &= \\mathbb{E}(Y \\mid A=1) \\;-\\; \\mathbb{E}(Y \\mid A=0).\n\\end{aligned}\n Because randomisation (or more generally control over the probability of receiving treatment) ensures that missing counterfactuals are exchangeable with those observed, we can still estimate \\mathbb{E}[Y(a)]. For instance, assume:\n\n\\underbrace{\\mathbb{E}[Y(1)\\mid A=1]}_{\\text{counterfactual}}  = \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobservable}}}  =  \\underbrace{(Y_i \\mid A_i = 1)}_{\\text{observed}}\n\nwhich lets us infer the average outcome if everyone were treated. Likewise, if\n\n\\underbrace{\\mathbb{E}[Y(0)\\mid A=0]}_{\\text{counterfactual}}  = \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobservable}}}  =  \\underbrace{\\mathbb{E}(Y \\mid A_i = 0)}_{\\text{observed}}\n\nthen we can infer the average outcome if everyone were given the control. The difference between these two quantities gives the ATE:\n\n\\text{ATE} = \\Big[\n\\overbrace{\\mathbb{E}[Y(1)\\mid A=1]}^{\\substack{\\text{by consistency:}\\\\ \\equiv \\text{ observed } \\; \\mathbb{E}[Y\\mid A=1]}}\n\\;+\\;\n\\overbrace{\\textcolor{red}{\\mathbb{E}[Y(1)\\mid A=0]}}^{\\substack{\\text{by exchangeability:}\\\\ \\text{unobservable, yet } \\; \\equiv \\mathbb{E}[Y\\mid A=1]}}\n\\Big]\n-\\,\n\\Big[\n\\overbrace{\\mathbb{E}[Y(0)\\mid A=0]}^{\\substack{\\text{by consistency:}\\\\ \\equiv  \\text{observed } \\; \\mathbb{E}[Y\\mid A=0]}}\n\\;+\\;\n\\overbrace{\\textcolor{red}{\\mathbb{E}[Y(0)\\mid A=1]}}^{\\substack{\\text{by exchangeability:}\\\\ \\text{unobservable, yet } \\; \\equiv \\mathbb{E}[Y\\mid A=0]}}\n\\Big]\n\nWe have it that \\mathbb{E}[Y\\mid A=1] and \\mathbb{E}[Y\\mid A=0] and \\mathbb{E}[Y(1)\\mid A=0] are observed. If both consistency and exchangeability are satisifed then we may use these observed quantities to identify contrasts of counterfactual quanities.\nThus, although individual-level counterfactuals are missing, the consistency assumptions and the exchangeability assumptions allow us to identify the average effect of treatment using observed data. Randomised controlled experiments allow us to meet these assumptions. Randomisation warrents the exchangeability assumption. Control warrents the consistency asumption.\n\n\nFundamental Assumption 3: Positivity\nThere is one further assumption, called positivity. It states that treatment assignments cannot be deterministic. That is, for every covariate pattern L = l, each individual has a non-zero probability of receiving ever treatment level to be compared:\n\nP(A = a \\mid L = l) &gt; 0.\n\nRandomised experiments achieve positivity by design – at least for the sample that is selected into the study. In observational settings violations occur if some subgroups never receive a particular treatment. If treatments occur but are rare, we may have sufficient data from which to obtain convincing causal inferences.\nPositivity is the only assumption that can be verified with data."
  },
  {
    "objectID": "content/00-background.html#challenges-with-observational-data",
    "href": "content/00-background.html#challenges-with-observational-data",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Challenges with Observational Data",
    "text": "Challenges with Observational Data\n\n1. Satisfying Causal Consistency is Difficult in Observational Settings\nBelow are some ways in which real-world complexities can violate causal consistency in observational studies. For example, causal consistency requires there is no interference between units (also called “SUTVA” or “Stable Unit Treatment Value.” Causal consistency also requires that each treatment level is well-defined and applied uniformly. If these conditions fail, then Y(a) may not reflect a consistent exposures across individuals. We are then comparing apples with oranges. Consider some examples:\n\nCultural Dependence: one group’s “religious service” will differ qualitatively from another’s. Attending a !Kung! healing ritual and an Aztec human sacrifice may, plausible, produce different responses in people. In one, charity, in the other, terror.\nSocial Dependencies: Watts sees Alice gives, so Watts gives too. Here effect under treatment differs depending on how others respond.\n\nIf the actual exposures differ across individuals, then consistency (Y_i(a) = Y_i \\mid A_i) may fail, because A=a is not the same phenomenon for everyone.\n\n\n2. Conditional Exchangeability (No Unmeasured Confounding) Is Difficult to Achieve\nIn theory, we can identify a causal effect from observational data if all confounders L are measured. Formally, we need the potential outcomes to be independent of treatment once we condition on L. One way to express this asssumption is: Y(a) \\coprod A \\mid L. If the potential outcomes are independent of treatment assignment, we can identify the Average Treatment Effect (ATE) as: \n\\text{ATE} \\;=\\; \\sum_{l}\n\\Bigl[\\mathbb{E}\\bigl(Y \\mid A=1, L=l\\bigr) \\;-\\; \\mathbb{E}\\bigl(Y \\mid A=0, L=l\\bigr)\\Bigr] \\;\\Pr(L=l).\n\nIn randomised experiments, conditioning is automatic because A is unrelated to potential outcomes by design. In observational studies, ensuring or approximating such conditional exchangeability is often difficult. For example, bilingualism research would need to consider:\n\nCultural histories: cultures that value language acquistion might also value knowledge acquistion. Associations might arise from Culture, not causation.\nPersonal values: families who place a high priority on bilingualism may also promote other developmental resources.\n\nIf important confounders go unmeasured or are poorly measured, these differences can bias causal effect estimates.\n\n\n3. The Positivity Assumption May Fail: Treatments Might Not Exist for All\nPositivity requires that each individual could, in principle, receive any exposure level. But in real-world observational settings, some groups have no access to bilingual education (or no reason to be monolingual), making certain treatment levels impossible for them. If a treatment level does not appear in the data for a given subgroup, any causal effect estimate for that subgroup is purely an extrapolation (Westreich and Cole 2010; Hernan and Robins 2020a)."
  },
  {
    "objectID": "content/00-background.html#summary",
    "href": "content/00-background.html#summary",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Summary",
    "text": "Summary\nWe introduced the fundamental problem of causal inference by distinguishing correlation (associations in the data) from causation (contrasts between potential outcomes, of which only one can be observed for each individual).\nRandomised experiments address this problem by balancing confounding variables across treatment levels. Although individual causal effects are unobservable, random assignment allows us to infer average causal effects — also called marginal effects.\nIn observational data, inferring average treatment effects demands that we satisfy three assumptions that are automatically satisfied in (well-conducted) experiments: causal consistency, exchangeability, and positivity. These assumptions ensure that we can compare like-with-like (that the population-level treatment effect is consistent across individuals), that there are no unmeasured common causes of the exposure and outcomes that may lead to associations in the absence of causality, and that every exposure level is a real possibility for each subgroup.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "content/00-background.html#what-is-causality",
    "href": "content/00-background.html#what-is-causality",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "What is Causality?",
    "text": "What is Causality?"
  },
  {
    "objectID": "content/00-background.html#david-humes-two-definitions-in-enquiries-1751",
    "href": "content/00-background.html#david-humes-two-definitions-in-enquiries-1751",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "David Hume’s Two Definitions in Enquiries (1751)",
    "text": "David Hume’s Two Definitions in Enquiries (1751)\n\nDefinition 1:\n\nWe may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second…\n\n\n\nDefinition 2:\n\nOr, in other words, where, if the first object had not been, the second never would have existed\n\nEnquiries Concerning Human Understanding, and Concerning the Principles of Morals 1751"
  },
  {
    "objectID": "content/00-background.html#our-lives-are-filled-with-what-if-questions",
    "href": "content/00-background.html#our-lives-are-filled-with-what-if-questions",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Our lives are filled with “What If?” questions",
    "text": "Our lives are filled with “What If?” questions"
  },
  {
    "objectID": "content/00-background.html#the-fundamental-problem-of-causal-inference",
    "href": "content/00-background.html#the-fundamental-problem-of-causal-inference",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nTo quantify a causal effect requires a counterfactual contrast:\n\\tau_{you} = \\Big[Y_{\\text{you}}(a =1) - Y_{\\text{you}}(a=0)\\Big]\nWhere, Y(a) denotes the potential outcome under an intervention A = a. Here, we assume a binary intervention. At any time, we may observe the outcome of only"
  },
  {
    "objectID": "content/00-background.html#however-we-only-observe-facts-not-counterfactuals",
    "href": "content/00-background.html#however-we-only-observe-facts-not-counterfactuals",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "However, we only observe facts not counterfactuals",
    "text": "However, we only observe facts not counterfactuals\n\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\textcolor{red}{\\text{is counterfactual}}\n\n\n“And sorry I could not travel both. And be one traveller, long I stood \\dots”"
  },
  {
    "objectID": "content/00-background.html#average-treatment-effect-in-randomised-controlled-experiments-work-from-assumptions",
    "href": "content/00-background.html#average-treatment-effect-in-randomised-controlled-experiments-work-from-assumptions",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Average Treatment Effect in randomised controlled experiments work from assumptions",
    "text": "Average Treatment Effect in randomised controlled experiments work from assumptions\n\n\\text{Average Treatment Effect} = \\left[ \\begin{aligned}\n&\\left( \\underbrace{\\mathbb{E}[Y(1)|A = 1]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(1)|A = 0]}_{\\text{unobserved}}} \\right) \\\\\n&- \\left( \\underbrace{\\mathbb{E}[Y(0)|A = 0]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(0)|A = 1]}_{\\text{unobserved}}} \\right)\n\\end{aligned} \\right]"
  },
  {
    "objectID": "content/00-background.html#under-identifying-assumptions-we-may-infer-causal-effects-from-associations",
    "href": "content/00-background.html#under-identifying-assumptions-we-may-infer-causal-effects-from-associations",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Under identifying assumptions, we may infer causal effects from associations",
    "text": "Under identifying assumptions, we may infer causal effects from associations\n\n\\text{ATE} = \\sum_{l} \\left( \\mathbb{E}[Y|A=1, \\textcolor{cyan}{L=l}] - \\mathbb{E}[Y|A=0, \\textcolor{cyan}{L=l}] \\right) \\times \\textcolor{cyan}{\\Pr(L=l)}\n\nWhere L is a set of measured covariates and A\\coprod Y(a)|L"
  },
  {
    "objectID": "content/00-background.html#paradigmatic-concern-confounding-by-common-cause",
    "href": "content/00-background.html#paradigmatic-concern-confounding-by-common-cause",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Paradigmatic Concern: Confounding by Common Cause",
    "text": "Paradigmatic Concern: Confounding by Common Cause\n\\commoncauseT"
  },
  {
    "objectID": "content/00-background.html#where-assumptions-justify-we-may-condition-on-measured-confounders-to-obtain-balance.",
    "href": "content/00-background.html#where-assumptions-justify-we-may-condition-on-measured-confounders-to-obtain-balance.",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Where assumptions justify, we may condition on measured confounders to obtain balance.",
    "text": "Where assumptions justify, we may condition on measured confounders to obtain balance.\n\n\n\nL \\to A; L \\to Y\n\n\n\n\\boxed{L}"
  },
  {
    "objectID": "content/00-background.html#what-error-1-mediator-bias",
    "href": "content/00-background.html#what-error-1-mediator-bias",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "“What Error #1”: Mediator Bias",
    "text": "“What Error #1”: Mediator Bias\nA \\to \\boxed{L} \\to Y"
  },
  {
    "objectID": "content/00-background.html#data-generating-process",
    "href": "content/00-background.html#data-generating-process",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nset.seed(123)                           # reproducibility\nn        &lt;- 1000\nservice  &lt;- rbinom(n, 1, 0.5)          # A\nwealth   &lt;- 2 * service + rnorm(n)     # L\ncharity  &lt;- 1.5 * wealth + rnorm(n, sd = 1.5)  # Y\n\nsim1 &lt;- tibble(service, wealth, charity)"
  },
  {
    "objectID": "content/00-background.html#model-comparison",
    "href": "content/00-background.html#model-comparison",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Model comparison",
    "text": "Model comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel A: Omit L\n\n\nModel B: Control for L\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nservice\n2.9\n2.6, 3.2\n&lt;0.001\n-0.27\n-0.53, -0.01\n0.043\n\n\nwealth\n\n\n\n\n\n\n1.6\n1.5, 1.7\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\nControlling for the mediator reverses the sign of the service coefficient."
  },
  {
    "objectID": "content/00-background.html#which-model-looks-better",
    "href": "content/00-background.html#which-model-looks-better",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Which model looks better?",
    "text": "Which model looks better?\n\n\nCode\nlibrary(performance)\n\ncompare_performance(fit_adjust, fit_omit, rank = TRUE)\n\n\n# Comparison of Model Performance Indices\n\nName       | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights\n--------------------------------------------------------------------\nfit_adjust |    lm | 0.682 |     0.682 | 1.473 | 1.475 |        1.00\nfit_omit   |    lm | 0.312 |     0.311 | 2.169 | 2.171 |   2.88e-168\n\nName       | AICc weights | BIC weights | Performance-Score\n-----------------------------------------------------------\nfit_adjust |         1.00 |        1.00 |           100.00%\nfit_omit   |    2.91e-168 |   3.35e-167 |             0.00%\n\n\nCode\n# BIC(fit_adjust) - BIC(fit_omit)  # negative → \"better\" fit"
  },
  {
    "objectID": "content/00-background.html#what-error-2-collider-bias",
    "href": "content/00-background.html#what-error-2-collider-bias",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "“What Error #2”: Collider Bias",
    "text": "“What Error #2”: Collider Bias\nA\\to \\boxed{L} \\leftarrow Y"
  },
  {
    "objectID": "content/00-background.html#data-generating-process-1",
    "href": "content/00-background.html#data-generating-process-1",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\n\nCode\nset.seed(2025)\n\nn          &lt;- 1000\nservice    &lt;- rbinom(n, 1, 0.5)                            # A\ndonations  &lt;- rnorm(n)                                     # Y\nwealth     &lt;- rnorm(n, mean = service + donations, sd = 1) # collider L\n\nsim2 &lt;- tibble(service, wealth, donations)"
  },
  {
    "objectID": "content/00-background.html#model-comparison-1",
    "href": "content/00-background.html#model-comparison-1",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel A: Omit L\n\n\nModel B: Control for L (collider)\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nservice\n0.00\n-0.12, 0.13\n&gt;0.9\n-0.53\n-0.63, -0.44\n&lt;0.001\n\n\nwealth\n\n\n\n\n\n\n0.51\n0.48, 0.54\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\nAdding the collider creates a spurious, significant effect of A."
  },
  {
    "objectID": "content/00-background.html#which-model-looks-better-1",
    "href": "content/00-background.html#which-model-looks-better-1",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Which model looks better?",
    "text": "Which model looks better?\n\n\nCode\ncompare_performance(fit_biased, fit_correct, rank = TRUE)\n\n\n# Comparison of Model Performance Indices\n\nName        | Model |        R2 |  R2 (adj.) |  RMSE | Sigma | AIC weights\n--------------------------------------------------------------------------\nfit_biased  |    lm |     0.526 |      0.525 | 0.707 | 0.708 |        1.00\nfit_correct |    lm | 3.722e-06 | -9.983e-04 | 1.028 | 1.029 |   1.42e-162\n\nName        | AICc weights | BIC weights | Performance-Score\n------------------------------------------------------------\nfit_biased  |         1.00 |        1.00 |           100.00%\nfit_correct |    1.43e-162 |   1.65e-161 |             0.00%\n\n\nCode\nBIC(fit_biased) - BIC(fit_correct)\n\n\n[1] -740.425"
  },
  {
    "objectID": "content/00-background.html#take-home",
    "href": "content/00-background.html#take-home",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Take-Home",
    "text": "Take-Home\n\n\n\n\n\n\nImportant\n\n\n\nRelying on model fit perpetuates the causality crisis in psychology (Bulbulia 2023).\nDraw the DAG first; decide what belongs in the model before looking at numbers."
  },
  {
    "objectID": "content/00-background.html#the-what-error-is-widespread-in-experimental-studies-in-the-social-sciences",
    "href": "content/00-background.html#the-what-error-is-widespread-in-experimental-studies-in-the-social-sciences",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "The What Error is widespread in experimental studies in the social sciences",
    "text": "The What Error is widespread in experimental studies in the social sciences\n\n“Overall, we find that 46.7% of the experimental studies published in APSR, AJPS, and JOP from 2012 to 2014 engaged in posttreatment conditioning (35 of 75 studies) …”\n“About 1 in 4 drop cases or subset the data based on post-treatment criteria, and nearly a third include post-treatment variables as covariates”\n“Most tellingly, nearly 1 in 8 articles directly conditions on variables that the authors themselves show as being an outcome of the experiment – an unambiguous indicator of a fundamental lack of understanding … that conditioning on posttreatment variables can invalidate results from randomized experiments.”\n“Empirically, then, the answer to the question of whether the discipline already understands posttreatment bias is clear: It does not.” (Montgomery, Nyhan, and Torres 2018)"
  },
  {
    "objectID": "content/00-background.html#mediator-bias-control-strategy-longitudinal-hygiene",
    "href": "content/00-background.html#mediator-bias-control-strategy-longitudinal-hygiene",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Mediator Bias control strategy: Longitudinal Hygiene",
    "text": "Mediator Bias control strategy: Longitudinal Hygiene\n\n\n\nA\\to L \\to Y\n\n\n\nA\\to Y\n\nDon’t include L"
  },
  {
    "objectID": "content/00-background.html#how-to-tame-the-what-error-hide-your-future",
    "href": "content/00-background.html#how-to-tame-the-what-error-hide-your-future",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "How to Tame The What Error? Hide your future",
    "text": "How to Tame The What Error? Hide your future\nUse Repeated Measures on the Same Individuals"
  },
  {
    "objectID": "content/00-background.html#collider-bias-control-strategy-hide-your-future",
    "href": "content/00-background.html#collider-bias-control-strategy-hide-your-future",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Collider bias control strategy: Hide your future",
    "text": "Collider bias control strategy: Hide your future"
  },
  {
    "objectID": "content/00-background.html#collider-bias-by-proxy-control-strategy-hide-your-future",
    "href": "content/00-background.html#collider-bias-by-proxy-control-strategy-hide-your-future",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Collider bias by proxy control strategy: Hide your future",
    "text": "Collider bias by proxy control strategy: Hide your future"
  },
  {
    "objectID": "content/00-background.html#post-exposure-collider-bias-control-strategy-hide-your-future",
    "href": "content/00-background.html#post-exposure-collider-bias-control-strategy-hide-your-future",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Post-exposure collider bias control strategy: Hide your future",
    "text": "Post-exposure collider bias control strategy: Hide your future"
  },
  {
    "objectID": "content/00-background.html#are-longitudinal-data-sensitivity-analysis-enough",
    "href": "content/00-background.html#are-longitudinal-data-sensitivity-analysis-enough",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Are longitudinal data + sensitivity analysis enough?",
    "text": "Are longitudinal data + sensitivity analysis enough?\n\n\nIf the data we collect were like this: Y_{\\text{time 0}} ~~...~~ A_{\\text{time 1}}\n\nWe should not be tempted to model this.\nY \\to A"
  },
  {
    "objectID": "content/00-background.html#are-longitudinal-data-sensitivity-analysis-enough-1",
    "href": "content/00-background.html#are-longitudinal-data-sensitivity-analysis-enough-1",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Are longitudinal data + sensitivity analysis enough?",
    "text": "Are longitudinal data + sensitivity analysis enough?\n{Too obvious} this is wrong?\nY \\to A"
  },
  {
    "objectID": "content/00-background.html#longitudinal-data-are-not-enough",
    "href": "content/00-background.html#longitudinal-data-are-not-enough",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Longitudinal data are not enough",
    "text": "Longitudinal data are not enough\nTemporal ordering was {precisely the problem} with the observational hormone studies in the 80s/90s that modelled .\nHow?"
  },
  {
    "objectID": "content/00-background.html#researchers-failed-to-emulate-an-experiment-in-their-data-target-trial",
    "href": "content/00-background.html#researchers-failed-to-emulate-an-experiment-in-their-data-target-trial",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Researchers failed to emulate an experiment in their data (Target Trial)",
    "text": "Researchers failed to emulate an experiment in their data (Target Trial)\n\nWomen’s Health Initiative: overall hazard ratio 1.23 (0.99, 1.53)\nWomen’s Health Initiative: when broken down by years to follow-up:\n\n0-2 years 1.51 (1.06, 2.14)\n2-5 years 1.31 (0.93, 1.83)\n5 or more years 0.67 (0.41, 1.09)\n\n\n\n\n\n\n\n\nNoteSurvivor Bias.\n\n\n\nThe observational results can be \n\n\n\n\n\n\n\n\nNoteEmulating a target trial with observational data recovers experimental effects.\n\n\n\nRe-modelling initiation into hormone therapy recovers experimental findings (Hernán et al. 2016)."
  },
  {
    "objectID": "content/00-background.html#visualising-the-when-error",
    "href": "content/00-background.html#visualising-the-when-error",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Visualising the When Error",
    "text": "Visualising the When Error\n\n\n\n\n\n\n\n\nFigure 1: Three ways to start the clock. Only one is right."
  },
  {
    "objectID": "content/00-background.html#target-trial-check-list",
    "href": "content/00-background.html#target-trial-check-list",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Target Trial Check List",
    "text": "Target Trial Check List\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n\n\n\n\n\nelement\ndefinition\n\n\n\n\nEligibility\nPost-menopausal women aged 50–79, no prior CHD\n\n\nTreatment\nInitiate oestrogen + progestin on day of Rx pickup\n\n\nComparison\nNo hormone-therapy initiation on that day\n\n\nOutcome\nAll-cause mortality\n\n\nFollow-up\n8 years or until death / loss to follow-up\n\n\n\n\n\n\nTable 1: Target-trial specification for the hormone-therapy example."
  },
  {
    "objectID": "content/05-measurement.html",
    "href": "content/05-measurement.html",
    "title": "Measurement And Selection Biases",
    "section": "",
    "text": "General Terminology for Causal Directed Acyclic Graphs\n\n\nTable 1"
  },
  {
    "objectID": "content/05-measurement.html#effect-modification-on-causal-directed-acyclic-graphs",
    "href": "content/05-measurement.html#effect-modification-on-causal-directed-acyclic-graphs",
    "title": "Measurement And Selection Biases",
    "section": "Effect-Modification on Causal Directed Acyclic Graphs",
    "text": "Effect-Modification on Causal Directed Acyclic Graphs\nThe primary function of a causal directed acyclic graph is to allow investigators to apply Pearl’s backdoor adjustment theorem to evaluate whether causal effects may be identified from data, as shown in Table 1. We have noted that modifying a causal effect within one or more strata of the target population opens the possibility for biased average treatment effect estimates when the distribution of these effect modifiers differs in the analytic sample population (Bulbulia 2024b).\n\n———. 2024b. “Methods in Causal Inference Part 2: Interaction, Mediation, and Time-Varying Treatments.” Evolutionary Human Sciences 6: e41. https://doi.org/10.1017/ehs.2024.32.\n\nBulbulia, J. A. 2024a. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35.\nWe do not generally represent non-linearities in causal directed acyclic graphs, which are tools for obtaining relationships of conditional and unconditional independence from assumed structural relationships encoded in a causal diagram that may lead to a non-causal treatment/outcome association (Bulbulia 2024a).\nTable 2 presents our convention for highlighting a relationship of effect modification in settings where (1) we assume no confounding of treatment and outcome and (2) there is effect modification such that the effect of A on Y differs in at least one stratum of the target population.\n\n\n\nEffect-Modification\n\n\nTable 2\n\n\n\nTo focus on effect modification, we do not draw a causal arrow from the direct effect modifier F to the outcome Y. This convention is specific to this article (refer to Hernan and Robins (2020), pp. 126–127, for a discussion of ‘non-causal’ arrows).\n\nHernan, M. A., and J. M. Robins. 2020. Causal Inference: What If? Chapman & Hall/CRC Monographs on Statistics & Applied Probab. Taylor & Francis. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/."
  },
  {
    "objectID": "content/05-measurement.html#part-1-how-measurement-error-bias-makes-your-causal-inferences-weird-id-sec-1",
    "href": "content/05-measurement.html#part-1-how-measurement-error-bias-makes-your-causal-inferences-weird-id-sec-1",
    "title": "Measurement And Selection Biases",
    "section": "Part 1: How Measurement Error Bias Makes Your Causal Inferences weird {#id-sec-1}",
    "text": "Part 1: How Measurement Error Bias Makes Your Causal Inferences weird {#id-sec-1}\n(wrongly estimated inferences due to inappropriate restriction and distortion)\nMeasurements record reality, but they are not always accurate. Whenever variables are measured with error, our results can be misleading. Every study must therefore consider how its measurements might mislead.\nCausal graphs can deepen understanding because—as implied by the concept of ‘record’—there are structural or causal properties that give rise to measurement error. Measurement error can take various forms, each with distinct implications for causal inference:\n\nIndependent (undirected) / uncorrelated: Errors in different variables do not influence each other.\n\nIndependent (undirected) and correlated: Errors in different variables are related through a shared cause.\n\nDependent (directed) and uncorrelated: Errors in one variable influence the measurement of another, but these influences are not related through a shared cause.\n\nDependent (directed) and correlated: Errors in one variable influence the measurement of another, and these influences are related through a shared cause (Hernán and Cole 2009; VanderWeele and Hernán 2012).\n\n\nHernán, Miguel A., and Stephen R. Cole. 2009. “Invited Commentary: Causal Diagrams and Measurement Bias.” American Journal of Epidemiology 170 (8): 959–62. https://doi.org/10.1093/aje/kwp293.\nThe six causal diagrams presented in Table 2 illustrate structural features of measurement error bias and clarify how these structural features compromise causal inferences.\n\n\n\nEffect-Modification Examples of measurement error bias\n\n\nTable 2\n\n\n\nUnderstanding these structural features will help explain why measurement error bias cannot typically be evaluated with statistical models, and will prepare us to link target-population restriction biases to measurement error.\n\nExample 1: Uncorrelated Non-Differential Errors under Sharp Null (No Treatment Effect)\nTable 2 \\mathcal{G}_1 illustrates uncorrelated non-differential measurement error under the ‘sharp null’, which arises when the error terms in the exposure and outcome are independent. In this setting, measurement error is not expected to bias estimates.\nExample: A study on whether beliefs in big Gods affect social complexity in ancient societies, where societies randomly omitted or inaccurately recorded such beliefs and complexity, with errors independent across variables. Under randomisation, uncorrelated undirected errors will generally not bias estimates under the sharp null, assuming all backdoor paths are closed. However, mismeasured confounders can open backdoor paths (Robins and Hernan 2008).\n\nRobins, James, and Miguel Hernan. 2008. “Estimation of the Causal Effects of Time-Varying Exposures.” Chapman & Hall/CRC Handbooks of Modern Statistical Methods, 553–99.\n\n\nExample 2: Uncorrelated Non-Differential Errors “Off the Null” (True Effect Present)\nTable 2 \\mathcal{G}_2 illustrates uncorrelated non-differential measurement error when there is a true treatment effect. This bias, also called information bias (Lash, Fox, and Fink 2009), often attenuates the effect toward the null—but not always (Anne M. Jurek et al. 2005; Anne M. Jurek et al. 2006; Anne M. Jurek, Greenland, and Maldonado 2008).\n\nLash, Timothy L., Matthew P. Fox, and Aliza K. Fink. 2009. Applying Quantitative Bias Analysis to Epidemiologic Data. Springer.\n\nJurek, Anne M, Sander Greenland, George Maldonado, and Timothy R Church. 2005. “Proper Interpretation of Non-Differential Misclassification Effects: Expectations Vs Observations.” International Journal of Epidemiology 34 (3): 680–87.\n\nJurek, Anne M., Gustavo Maldonado, Sander Greenland, and Timothy R. Church. 2006. “Exposure-Measurement Error Is Frequently Ignored When Interpreting Epidemiologic Study Results.” European Journal of Epidemiology 21 (12): 871–76. https://doi.org/10.1007/s10654-006-9083-0.\n\nJurek, Anne M, Sander Greenland, and George Maldonado. 2008. “Brief Report: How Far from Non-Differential Does Exposure or Disease Misclassification Have to Be to Bias Measures of Association Away from the Null?” International Journal of Epidemiology 37 (2): 382–85.\nExample: Same setting as above, but a real effect exists. Measurement error often underestimates it, but attenuation is not guaranteed. Mismeasured confounders may still open backdoor paths.\n\n\nExample 3: Correlated Non-Differential (Undirected) Measurement Errors\nTable 2 \\mathcal{G}_3 arises when the error terms of the treatment and outcome share a common cause.\nExample: Societies with advanced record-keeping produce more precise records of both big God beliefs and social complexity. This common cause creates a spurious association even in the absence of a true causal effect.\n\n\nExample 4: Uncorrelated Differential Measurement Error — Exposure → Error in Outcome\nTable 2 \\mathcal{G}_4 occurs when the exposure influences how the outcome is measured.\nExample: Big God beliefs lead to inflated historical records of social complexity, introducing bias even without a true causal effect.\n\n\nExample 5: Uncorrelated Differential Measurement Error — Outcome → Error in Exposure\nTable 2 \\mathcal{G}_5 occurs when the outcome influences the measurement of the exposure.\nExample: If social complexity shapes historical narratives, victors might record big God beliefs selectively to support political legitimacy.\n\n\nExample 6: Correlated Differential Measurement Error\nTable 2 \\mathcal{G}_6 occurs when the exposure influences already correlated error terms.\nExample: Social complexity fosters elites who glorify both political reach and big God beliefs, biasing both measures in a correlated fashion.\n\n\nSummary\nIn Part 1, we examined independent, correlated, dependent, and correlated–dependent forms of measurement error bias. These structural features clarify why such biases threaten causal inference and often cannot be resolved with statistical adjustment alone (VanderWeele and Hernán 2012).\n\nVanderWeele, Tyler J., and Miguel A. Hernán. 2012. “Results on Differential and Dependent Measurement Error of the Exposure and the Outcome Using Signed Directed Acyclic Graphs.” American Journal of Epidemiology 175 (12): 1303–10. https://doi.org/10.1093/aje/kwr458.\nWe return to measurement error in Part 4."
  },
  {
    "objectID": "content/05-measurement.html#part-2-target-population-restriction-bias-at-the-end-of-study-id-sec-2",
    "href": "content/05-measurement.html#part-2-target-population-restriction-bias-at-the-end-of-study-id-sec-2",
    "title": "Measurement And Selection Biases",
    "section": "Part 2: Target Population Restriction Bias at the End of Study {#id-sec-2}",
    "text": "Part 2: Target Population Restriction Bias at the End of Study {#id-sec-2}\nSuppose the analytic sample matches the target population at baseline. Attrition (right-censoring) may bias causal effect estimates by:\n1) opening biasing pathways (distortion), or\n2) restricting the analytic sample so it is no longer representative (restriction).\n\n\n\nSelection-bias over time Five examples of right-censoring bias.\n\n\nTable 4\n\n\n\n\nExample 1: Confounding by common cause of treatment and attrition\nTable 4 \\mathcal{G}_1 illustrates confounding by common cause of treatment and outcome in the censored such that the potential outcomes of the population at baseline Y(a) may differ from those of the censored population at the end of study Y'(a), so Y'(a) \\neq Y(a). Suppose investigators ask whether religious service attendance affects volunteering, and an unmeasured variable (loyalty) affects attendance, attrition, and volunteering—opening a backdoor path.\nWe have encountered this bias before: the structure matches correlated measurement errors (Table 3 \\mathcal{G}_3). Attrition may exacerbate measurement error bias by opening a path A \\;\\associationred\\; U \\;\\associationred\\; U_{\\Delta A} \\;\\associationred\\; Y'.\n\n\nExample 2: Treatment affects censoring\nTable 4 \\mathcal{G}_2 illustrates bias in which the treatment affects the censoring process. Here, the treatment causally affects the outcome reporter but not the outcome itself.\nExample: In a meditation trial with no true effect on well-being, Buddha-like detachment increases attrition and also changes how well-being is reported. This opens a path A \\;\\associationred\\; U_{\\Delta{A\\to Y}} \\;\\associationred\\; Y' (not confounding; no common cause of A and Y). Structurally, this is directed uncorrelated measurement error (Table 3 \\mathcal{G}_4). Results risk distortion via end-of-study restriction.\n\n\nExample 3: No treatment effect when outcome causes censoring\nTable 4 \\mathcal{G}_3 shows outcome-driven censoring under the sharp null. In theory, the ATE may remain unbiased, though the analytic sample is restricted. This corresponds to undirected uncorrelated measurement error (Table 3 \\mathcal{G}_1). In practice the sharp-null assumption is untestable and rarely known in advance.\n\n\nExample 4: Treatment effect when outcome causes censoring and a true effect exists\nTable 4 \\mathcal{G}_4 shows that if the outcome affects censoring in the presence of a true effect, bias arises (at least on one effect scale). This structure is equivalent to measurement error bias and can occur without confounding. See the worked example in Part 4.\n\n\nExample 5: Treatment effect and effect-modifiers differ in censored group (restriction bias without confounding)\nTable 4 \\mathcal{G}_5 represents a setting with a true treatment effect, but the distribution of effect modifiers differs at study end. If missingness is at random and models are correctly specified, inverse probability weighting or multiple imputation can recover valid estimates (Cole and Hernán 2008; Leyrat et al. 2021; Shiba and Kawahara 2021). If not (e.g., MNAR or model misspecification), causal estimation is compromised (Tchetgen Tchetgen and Wirth 2017; Malinsky, Shpitser, and Tchetgen Tchetgen 2022).\n\nCole, Stephen R, and Miguel A Hernán. 2008. “Constructing Inverse Probability Weights for Marginal Structural Models.” American Journal of Epidemiology 168 (6): 656–64.\n\nLeyrat, Clémence, James R Carpenter, Sébastien Bailly, and Elizabeth J Williamson. 2021. “Common Methods for Handling Missing Data in Marginal Structural Models: What Works and Why.” American Journal of Epidemiology 190 (4): 663–72.\n\nShiba, Koichiro, and Takuya Kawahara. 2021. “Using Propensity Scores for Causal Inference: Pitfalls and Tips.” Journal of Epidemiology 31 (8): 457–63.\n\nTchetgen Tchetgen, Eric J, and Kathleen E Wirth. 2017. “A General Instrumental Variable Framework for Regression Analysis with Outcome Missing Not at Random.” Biometrics 73 (4): 1123–31.\n\nMalinsky, Daniel, Ilya Shpitser, and Eric J Tchetgen Tchetgen. 2022. “Semiparametric Inference for Nonmonotone Missing-Not-at-Random Data: The No Self-Censoring Model.” Journal of the American Statistical Association 117 (539): 1415–23.\nNote that Table 4 \\mathcal{G}_5 resembles Table 3 \\mathcal{G}_2. Replacing unmeasured effect modifiers \\circledotted{F} and U_{\\Delta F} by \\circledotted{U_Y} shows the link to uncorrelated independent measurement error ‘off the null’.\nIn this setting there may be a common cause of A and Y, and, additionally, the end-of-study analytic sample is an undesirable restriction of the target population: marginal effects differ between the restricted sample and the target (see Supplement S4 for a simulation). Hence results can be weird due to inappropriate restriction.\n\n\nSummary\nRight-censoring can bias effect estimates by changing the distribution of effect modifiers between baseline and study end. Investigators should ensure the end-of-study potential outcomes distribution aligns with the target population. Methods such as inverse probability weighting and multiple imputation can mitigate this bias (Bulbulia 2024c), subject to their assumptions.\n\n———. 2024c. “A Practical Guide to Causal Inference in Three-Wave Panel Studies.” PsyArXiv Preprints, February. https://doi.org/10.31234/osf.io/uyg3d.\nThe take-home message: attrition is nearly inevitable; if unchecked, it yields weird results (wrongly estimated inferences due to inappropriate restriction and distortion). See Supplement S3 for a formal explanation and S4 for a simulation."
  },
  {
    "objectID": "content/05-measurement.html#id-sec-3",
    "href": "content/05-measurement.html#id-sec-3",
    "title": "Measurement And Selection Biases",
    "section": "Part 3: Target Population Restriction Bias at the Start of Study",
    "text": "Part 3: Target Population Restriction Bias at the Start of Study\nTarget‐restriction bias occurs when the analytic sample at baseline differs from the target population in the distribution of confounders and/or treatment‐effect modifiers. Misalignment may arise if the source population does not match the target, or if study selection alters distributions. Alignment cannot generally be verified from data (see Supplement S3).\n\nCollider‐Restriction Bias at Baseline\n\n\n\nCollider‐stratification bias at the start of a study Collider‐stratification bias at the start of a study (“M‐bias”)\n\n\nTable 5\n\n\n\nIn Table 5 \\mathcal{G}_1, unmeasured health awareness (U_1) influences both activity (A) and participation (S=1), and unmeasured SES (U_2) influences both heart health (Y) and S=1. Conditioning on S=1 opens paths: - U_1: Over‐representation of active individuals, overstating benefits. - U_2: Confounding path via SES, inflating effect estimates.\nAdjusting for U_1, U_2, or proxies can block these paths (Table 5 \\mathcal{G}_2).\n\n\nRestriction Bias Without Collider Stratification\n\n\n\nSelection bias at baseline Selection bias “off the null”\n\n\nTable 6\n\n\n\n\nExample 1: WEIRD Sample, Non‐WEIRD Target\nIf effect modifiers differ between a WEIRD sample and general population (Table 6 \\mathcal{G}_{1.1}), estimates may be biased without confounding . Structure matches Table 4 \\mathcal{G}_5 and Table 3 \\mathcal{G}_2. Known effect‐modifier distributions allow weighting , but mapping from restricted to target effects (f_W) is usually unknown .\n\n\nExample 2: Overly Broad Sample for Narrow Target\nIf the target is a restricted stratum (e.g., NZ men &gt; 40 without vasectomy) but sampling is broader (Table 6 \\mathcal{G}_{2.1}), bias mirrors right‐censoring with effect modifiers. Correct restriction (Table 6 \\mathcal{G}_{2.2}) aligns sample with target.\n\n\nExample 3: Correlated Covariate/Outcome Measurement Error Across Strata\nIn cross‐cultural studies, correlated measurement errors in L and Y (Table 6 \\mathcal{G}_{3.1}) can open biasing paths even if A is measured perfectly. Without local validation, pooling cultures risks contamination; restricting to cultures with reliable measures (Table 6 \\mathcal{G}_{3.2}) is safer.\n\n\nExample 4: Correlated Measurement Error in Effect Modifiers\nWith perfect A and Y, correlated errors in effect‐modifier measures (Table 6 \\mathcal{G}_{4.1}) still prevent valid heterogeneity estimation or use of target weights. Best practice: restrict to settings with reliable effect‐modifier measurement (Table 6 \\mathcal{G}_{4.2}) and report strata separately if errors differ."
  },
  {
    "objectID": "content/05-measurement.html#part-1-how-measurement-error-bias-makes-your-causal-inferences-weird",
    "href": "content/05-measurement.html#part-1-how-measurement-error-bias-makes-your-causal-inferences-weird",
    "title": "Measurement And Selection Biases",
    "section": "Part 1: How Measurement Error Bias Makes Your Causal Inferences weird",
    "text": "Part 1: How Measurement Error Bias Makes Your Causal Inferences weird\n(wrongly estimated inferences due to inappropriate restriction and distortion)\nMeasurements record reality, but they are not always accurate. Whenever variables are measured with error, our results can be misleading. Every study must therefore consider how its measurements might mislead.\nCausal graphs can deepen understanding because—as implied by the concept of ‘record’—there are structural or causal properties that give rise to measurement error. Measurement error can take various forms, each with distinct implications for causal inference:\n\nIndependent (undirected) / uncorrelated: Errors in different variables do not influence each other.\n\nIndependent (undirected) and correlated: Errors in different variables are related through a shared cause.\n\nDependent (directed) and uncorrelated: Errors in one variable influence the measurement of another, but these influences are not related through a shared cause.\n\nDependent (directed) and correlated: Errors in one variable influence the measurement of another, and these influences are related through a shared cause (Hernán and Cole 2009; VanderWeele and Hernán 2012).\n\n\nHernán, Miguel A., and Stephen R. Cole. 2009. “Invited Commentary: Causal Diagrams and Measurement Bias.” American Journal of Epidemiology 170 (8): 959–62. https://doi.org/10.1093/aje/kwp293.\nThe six causal diagrams presented in Table 3 illustrate structural features of measurement error bias and clarify how these structural features compromise causal inferences.\n\n\n\nMeasurement Error Bias Examples of measurement error bias\n\n\nTable 3\n\n\n\nUnderstanding these structural features will help explain why measurement error bias cannot typically be evaluated with statistical models, and will prepare us to link target-population restriction biases to measurement error.\n\nExample 1: Uncorrelated Non-Differential Errors under Sharp Null (No Treatment Effect)\nTable 3 \\mathcal{G}_1 illustrates uncorrelated non-differential measurement error under the ‘sharp null’, which arises when the error terms in the exposure and outcome are independent. In this setting, measurement error is not expected to bias estimates.\nExample: A study on whether beliefs in big Gods affect social complexity in ancient societies, where societies randomly omitted or inaccurately recorded such beliefs and complexity, with errors independent across variables. Under randomisation, uncorrelated undirected errors will generally not bias estimates under the sharp null, assuming all backdoor paths are closed. However, mismeasured confounders can open backdoor paths (Robins and Hernan 2008).\n\nRobins, James, and Miguel Hernan. 2008. “Estimation of the Causal Effects of Time-Varying Exposures.” Chapman & Hall/CRC Handbooks of Modern Statistical Methods, 553–99.\n\n\nExample 2: Uncorrelated Non-Differential Errors “Off the Null” (True Effect Present)\nTable 3 \\mathcal{G}_2 illustrates uncorrelated non-differential measurement error when there is a true treatment effect. This bias, also called information bias (Lash, Fox, and Fink 2009), often attenuates the effect toward the null—but not always (Anne M. Jurek et al. 2005; Anne M. Jurek et al. 2006; Anne M. Jurek, Greenland, and Maldonado 2008).\n\nLash, Timothy L., Matthew P. Fox, and Aliza K. Fink. 2009. Applying Quantitative Bias Analysis to Epidemiologic Data. Springer.\n\nJurek, Anne M, Sander Greenland, George Maldonado, and Timothy R Church. 2005. “Proper Interpretation of Non-Differential Misclassification Effects: Expectations Vs Observations.” International Journal of Epidemiology 34 (3): 680–87.\n\nJurek, Anne M., Gustavo Maldonado, Sander Greenland, and Timothy R. Church. 2006. “Exposure-Measurement Error Is Frequently Ignored When Interpreting Epidemiologic Study Results.” European Journal of Epidemiology 21 (12): 871–76. https://doi.org/10.1007/s10654-006-9083-0.\n\nJurek, Anne M, Sander Greenland, and George Maldonado. 2008. “Brief Report: How Far from Non-Differential Does Exposure or Disease Misclassification Have to Be to Bias Measures of Association Away from the Null?” International Journal of Epidemiology 37 (2): 382–85.\nExample: Same setting as above, but a real effect exists. Measurement error often underestimates it, but attenuation is not guaranteed. Mismeasured confounders may still open backdoor paths.\n\n\nExample 3: Correlated Non-Differential (Undirected) Measurement Errors\nTable 3 \\mathcal{G}_3 arises when the error terms of the treatment and outcome share a common cause.\nExample: Societies with advanced record-keeping produce more precise records of both big God beliefs and social complexity. This common cause creates a spurious association even in the absence of a true causal effect.\n\n\nExample 4: Uncorrelated Differential Measurement Error — Exposure → Error in Outcome\nTable 3 \\mathcal{G}_4 occurs when the exposure influences how the outcome is measured.\nExample: Big God beliefs lead to inflated historical records of social complexity, introducing bias even without a true causal effect.\n\n\nExample 5: Uncorrelated Differential Measurement Error — Outcome → Error in Exposure\nTable 3 \\mathcal{G}_5 occurs when the outcome influences the measurement of the exposure.\nExample: If social complexity shapes historical narratives, victors might record big God beliefs selectively to support political legitimacy.\n\n\nExample 6: Correlated Differential Measurement Error\nTable 3 \\mathcal{G}_6 occurs when the exposure influences already correlated error terms.\nExample: Social complexity fosters elites who glorify both political reach and big God beliefs, biasing both measures in a correlated fashion.\n\n\nSummary\nIn Part 1, we examined independent, correlated, dependent, and correlated–dependent forms of measurement error bias. These structural features clarify why such biases threaten causal inference and often cannot be resolved with statistical adjustment alone (VanderWeele and Hernán 2012).\n\nVanderWeele, Tyler J., and Miguel A. Hernán. 2012. “Results on Differential and Dependent Measurement Error of the Exposure and the Outcome Using Signed Directed Acyclic Graphs.” American Journal of Epidemiology 175 (12): 1303–10. https://doi.org/10.1093/aje/kwr458.\nWe return to measurement error in Part 4."
  },
  {
    "objectID": "content/05-measurement-2.html",
    "href": "content/05-measurement-2.html",
    "title": "Causal Diagrammes & Measurement",
    "section": "",
    "text": "DAGs represent causal structure; blue arrows highlight effect modification.\nSWIGs split intervention nodes into fixed/random components to display counterfactual states.\nSpecial node/edge styles indicate conditioning, unmeasured variables, and measurement processes."
  },
  {
    "objectID": "content/05-measurement-2.html#graphical-conventions",
    "href": "content/05-measurement-2.html#graphical-conventions",
    "title": "Causal Diagrammes & Measurement",
    "section": "",
    "text": "DAGs represent causal structure; blue arrows highlight effect modification.\nSWIGs split intervention nodes into fixed/random components to display counterfactual states.\nSpecial node/edge styles indicate conditioning, unmeasured variables, and measurement processes."
  },
  {
    "objectID": "content/05-measurement-2.html#part-1-measurement-error-bias",
    "href": "content/05-measurement-2.html#part-1-measurement-error-bias",
    "title": "Causal Diagrammes & Measurement",
    "section": "Part 1 — Measurement error bias",
    "text": "Part 1 — Measurement error bias\nMeasurement error structures: 1. Uncorrelated, non-differential: often attenuates effects under a true effect, but not guaranteed. 2. Correlated non-differential: common causes of errors can create spurious associations. 3. Uncorrelated differential: treatment or outcome affects error in the other. 4. Correlated differential: both directed and shared causes of error.\nKey insight: Many forms distort effect estimates “off the null”; even in randomised settings, mismeasured confounders can re-open biasing paths.\n?@tbl-terminologymeasurementerror illustrates six canonical structures."
  },
  {
    "objectID": "content/05-measurement-2.html#part-2-target-restriction-bias-at-study-end",
    "href": "content/05-measurement-2.html#part-2-target-restriction-bias-at-study-end",
    "title": "Causal Diagrammes & Measurement",
    "section": "Part 2 — Target restriction bias at study end",
    "text": "Part 2 — Target restriction bias at study end\nRight-censoring (attrition) can bias effects by: - Introducing new backdoor paths (distortion). - Changing distribution of effect modifiers between baseline and end-of-study (restriction).\nFive example DAGs in ?@tbl-terminologycensoring show: - Common cause of treatment and attrition. - Treatment-induced attrition. - Outcome-induced attrition (null and non-null cases). - Attrition changing effect modifier distribution.\nInverse probability weighting or multiple imputation can help if missingness is well-modelled."
  },
  {
    "objectID": "content/05-measurement-2.html#part-3-target-restriction-bias-at-baseline",
    "href": "content/05-measurement-2.html#part-3-target-restriction-bias-at-baseline",
    "title": "Causal Diagrammes & Measurement",
    "section": "Part 3 — Target restriction bias at baseline",
    "text": "Part 3 — Target restriction bias at baseline\nBias arises when the analytic sample differs from the target population in confounders or effect modifiers: - Collider restriction (M-bias). - Sampling only WEIRD groups for general population questions. - Overly broad or overly narrow eligibility criteria. - Cross-cultural designs with structured measurement error in covariates or effect modifiers.\n?@tbl-terminologyselectionrestrictionbaseline links these to equivalent measurement error structures (esp. uncorrelated error off the null)."
  },
  {
    "objectID": "content/05-measurement-2.html#part-4-swigs-and-unifying-biases",
    "href": "content/05-measurement-2.html#part-4-swigs-and-unifying-biases",
    "title": "Causal Diagrammes & Measurement",
    "section": "Part 4 — SWIGs and unifying biases",
    "text": "Part 4 — SWIGs and unifying biases\nSWIGs reveal: - Treatment measurement error: reporter is a post-treatment collider, diluting effects. - Outcome measurement error: error acts as an effect modifier; scale-dependent bias. - Target restriction biases at start/end are often equivalent to undirected, uncorrelated measurement error off the null.\n?@tbl-tblme shows SWIG templates and counterfactual worlds for key bias types."
  },
  {
    "objectID": "content/05-measurement-2.html#conclusions-and-workflow",
    "href": "content/05-measurement-2.html#conclusions-and-workflow",
    "title": "Causal Diagrammes & Measurement",
    "section": "Conclusions and workflow",
    "text": "Conclusions and workflow\nTo avoid weird inferences: 1. Define treatment and outcome precisely. 2. Clarify target population. 3. Ensure treatments satisfy consistency. 4. Verify exchangeability and positivity. 5. Evaluate measurement error structures. 6. Ensure end-of-study sample matches target population. 7. Document assumptions and reasoning transparently.\nComparative research demands extra caution: each site’s measurement validity must be assessed before pooling or comparing.\n\nFigures included: - ?@tbl-terminologygeneral (d-separation rules). - ?@tbl-effectmodification (effect modification on DAGs). - ?@tbl-terminologymeasurementerror (measurement error structures). - ?@tbl-terminologycensoring (censoring/restriction at end of study). - ?@tbl-terminologyselectionrestrictionclassic, ?@tbl-terminologyselectionrestrictionbaseline (restriction at baseline). - ?@tbl-tblme (SWIGs for measurement error). —"
  },
  {
    "objectID": "content/05-measurement.html#terminology-and-background",
    "href": "content/05-measurement.html#terminology-and-background",
    "title": "Measurement And Selection Biases",
    "section": "",
    "text": "General Terminology for Causal Directed Acyclic Graphs\n\n\nTable 1"
  },
  {
    "objectID": "jb-notes.html",
    "href": "jb-notes.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "jb-notes.html#appendix-b",
    "href": "jb-notes.html#appendix-b",
    "title": "",
    "section": "Appendix B: Causal Consistency in observational settings",
    "text": "Appendix B: Causal Consistency in observational settings\nIn observational research, there are typically multiple versions of the treatment. The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018]\nLet \\(\\coprod\\) denote independence. Where there are \\(K\\) different versions of treatment \\(A\\) and no confounding for \\(K\\)’s effect on \\(Y\\) given measured confounders \\(L\\) such that\n\\[\nY(k) \\coprod K | L\n\\]\nThen it can be proved that causal consistency follows. According to the theory of causal inference under multiple versions of treatment, the measured variable \\(A\\) functions as a “coarsened indicator” for estimating the causal effect of the multiple versions of treatment \\(K\\) on \\(Y(k)\\) [@vanderweele2009; @vanderweele2013; @vanderweele2018].\nIn the context of green spaces, let \\(A\\) represent the general action of moving closer to any green space and \\(K\\) represent the different versions of this treatment. For instance, \\(K\\) could denote moving closer to different green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.\nHere, the conditional independence implies that, given measured confounders \\(L\\) (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to (\\(K\\)) is independent of the outcomes \\(Y(k)\\) (e.g. mental well-being under the \\(K\\) conditions). In other words, the version of green space one chooses to live near does not affect the \\(K\\) potential outcomes, provided the confounders \\(L\\) are appropriately controlled for in our statistical models.\nPut simply, strategies for confounding control and consistently estimating causal effects when multiple treatment versions converge. However, the quantities we estimate under multiple treatment versions might need clearer interpretations. For example, we cannot readily determine which of the many treatment versions is most causally efficacious and which lack any causal effect or are harmful.\n\n\nAppendix C Simulation of Cross-Sectional Data to Compute the Average Treatment Effect When Conditioning on a Mediator\nThis appendix outlines a simulation designed to demonstrate the potential pitfalls of conditioning on a mediator in cross-sectional analyses. The simulation examines the scenario where the effect of access to green space (\\(A\\)) on happiness (\\(Y\\)) is fully mediated by exercise (\\(L\\)). This setup aims to illustrate how incorrect assumptions about the role of a variable (mediator vs. confounder) can lead to misleading estimates of the Average Treatment Effect (ATE)."
  },
  {
    "objectID": "content/talking-points.html",
    "href": "content/talking-points.html",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "NoteIn the 1980s and 1990s estrogen treatments appeared to benefit postmenopausal women\n\n\n\nHazard ratio for all-cause mortality: 0.68 current users vs. never users [@grodstein2006hormone]\n\n\n\n\n\n\n\n1992 American College of Obstetricians and Gynecologists \n1992 American College of Physicians \n1993 National Cholesterol Education Program \n1996 American Heart Association \n\n\n\n\n\n\n\nMassive randomised, double-blind, placebo-controlled trial\n16,000 U.S. women aged 50-79 years\nAssigned to Estrogen plus Progestin therapy\nWomen followed approximately every year for up to 8 years.\n\n\n\n\n\n\n\n\n\n\n\nWarningExperimental Findings: opposite of observational findings\n\n\n\nAll-cause mortality Hazard Ratio: 1.23 for initiators vs non-initiators. [@manson2003estrogen]\n\n\n\n\n\n\n\nCan observational studies ever be trusted?\nShould observational studies ever be funded again?\n\n\n\n\n\n\n\n\n\n\n\n\nWe may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second…\n\n\n\n\n\nOr, in other words, where, \n\nEnquiries Concerning Human Understanding, and Concerning the Principles of Morals 1751\n\n\n\n\n\n\n\n\n\n\n\n\nTo quantify a causal effect requires a counterfactual contrast:\n\\[\\tau_{you} = \\Big[Y_{\\text{you}}(a =1) - Y_{\\text{you}}(a=0)\\Big]\\]\nWhere, \\(Y(a)\\) denotes the potential outcome under an intervention \\(A = a\\). Here, we assume a binary intervention. At any time, we may observe the outcome of only \n\n\n\n\\[\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\textcolor{red}{\\text{is counterfactual}}\n\\]\n\n“And sorry I could not travel both. And be one traveller, long I stood \\(\\dots\\)”\n\n\n\n\n\\[\n\\text{Average Treatment Effect} = \\left[ \\begin{aligned}\n&\\left( \\underbrace{\\mathbb{E}[Y(1)|A = 1]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(1)|A = 0]}_{\\text{unobserved}}} \\right) \\\\\n&- \\left( \\underbrace{\\mathbb{E}[Y(0)|A = 0]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(0)|A = 1]}_{\\text{unobserved}}} \\right)\n\\end{aligned} \\right]\n\\]\n\n\n\n\\[\n\\text{ATE} = \\sum_{l} \\left( \\mathbb{E}[Y|A=1, \\textcolor{cyan}{L=l}] - \\mathbb{E}[Y|A=0, \\textcolor{cyan}{L=l}] \\right) \\times \\textcolor{cyan}{\\Pr(L=l)}\n\\]\nWhere \\(L\\) is a set of measured covariates and \\(A\\coprod Y(a)|L\\)"
  },
  {
    "objectID": "content/talking-points.html#s-observational-studies-indicated-30-all-cause-mortality-reduction-from-estrogen-therapies",
    "href": "content/talking-points.html#s-observational-studies-indicated-30-all-cause-mortality-reduction-from-estrogen-therapies",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "NoteIn the 1980s and 1990s estrogen treatments appeared to benefit postmenopausal women\n\n\n\nHazard ratio for all-cause mortality: 0.68 current users vs. never users [@grodstein2006hormone]"
  },
  {
    "objectID": "content/talking-points.html#standard-medical-advice",
    "href": "content/talking-points.html#standard-medical-advice",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "1992 American College of Obstetricians and Gynecologists \n1992 American College of Physicians \n1993 National Cholesterol Education Program \n1996 American Heart Association"
  },
  {
    "objectID": "content/talking-points.html#womens-health-initiative-evaluate-estrogens-experimentally",
    "href": "content/talking-points.html#womens-health-initiative-evaluate-estrogens-experimentally",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "Massive randomised, double-blind, placebo-controlled trial\n16,000 U.S. women aged 50-79 years\nAssigned to Estrogen plus Progestin therapy\nWomen followed approximately every year for up to 8 years."
  },
  {
    "objectID": "content/talking-points.html#findings-clear-discrepancy",
    "href": "content/talking-points.html#findings-clear-discrepancy",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "WarningExperimental Findings: opposite of observational findings\n\n\n\nAll-cause mortality Hazard Ratio: 1.23 for initiators vs non-initiators. [@manson2003estrogen]"
  },
  {
    "objectID": "content/talking-points.html#medical-community-response-reject-all-observational-studies",
    "href": "content/talking-points.html#medical-community-response-reject-all-observational-studies",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "Can observational studies ever be trusted?\nShould observational studies ever be funded again?"
  },
  {
    "objectID": "content/talking-points.html#david-humes-two-definitions-in-enquiries-1751",
    "href": "content/talking-points.html#david-humes-two-definitions-in-enquiries-1751",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "We may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second…\n\n\n\n\n\nOr, in other words, where, \n\nEnquiries Concerning Human Understanding, and Concerning the Principles of Morals 1751"
  },
  {
    "objectID": "content/talking-points.html#the-fundamental-problem-of-causal-inference",
    "href": "content/talking-points.html#the-fundamental-problem-of-causal-inference",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "To quantify a causal effect requires a counterfactual contrast:\n\\[\\tau_{you} = \\Big[Y_{\\text{you}}(a =1) - Y_{\\text{you}}(a=0)\\Big]\\]\nWhere, \\(Y(a)\\) denotes the potential outcome under an intervention \\(A = a\\). Here, we assume a binary intervention. At any time, we may observe the outcome of only"
  },
  {
    "objectID": "content/talking-points.html#however-we-only-observe-facts-not-counterfactuals",
    "href": "content/talking-points.html#however-we-only-observe-facts-not-counterfactuals",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "\\[\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\textcolor{red}{\\text{is counterfactual}}\n\\]\n\n“And sorry I could not travel both. And be one traveller, long I stood \\(\\dots\\)”"
  },
  {
    "objectID": "content/talking-points.html#average-treatment-effect-in-randomised-controlled-experiments-work-from-assumptions",
    "href": "content/talking-points.html#average-treatment-effect-in-randomised-controlled-experiments-work-from-assumptions",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "\\[\n\\text{Average Treatment Effect} = \\left[ \\begin{aligned}\n&\\left( \\underbrace{\\mathbb{E}[Y(1)|A = 1]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(1)|A = 0]}_{\\text{unobserved}}} \\right) \\\\\n&- \\left( \\underbrace{\\mathbb{E}[Y(0)|A = 0]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(0)|A = 1]}_{\\text{unobserved}}} \\right)\n\\end{aligned} \\right]\n\\]"
  },
  {
    "objectID": "content/talking-points.html#under-identifying-assumptions-we-may-infer-causal-effects-from-associations",
    "href": "content/talking-points.html#under-identifying-assumptions-we-may-infer-causal-effects-from-associations",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "",
    "text": "\\[\n\\text{ATE} = \\sum_{l} \\left( \\mathbb{E}[Y|A=1, \\textcolor{cyan}{L=l}] - \\mathbb{E}[Y|A=0, \\textcolor{cyan}{L=l}] \\right) \\times \\textcolor{cyan}{\\Pr(L=l)}\n\\]\nWhere \\(L\\) is a set of measured covariates and \\(A\\coprod Y(a)|L\\)"
  },
  {
    "objectID": "content/talking-points.html#paradigmatic-concern-confounding-by-common-cause",
    "href": "content/talking-points.html#paradigmatic-concern-confounding-by-common-cause",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Paradigmatic Concern: Confounding by Common Cause",
    "text": "Paradigmatic Concern: Confounding by Common Cause\n\\[\\commoncauseT\\]"
  },
  {
    "objectID": "content/talking-points.html#where-assumptions-justify-we-may-condition-on-measured-confounders-to-obtain-balance.",
    "href": "content/talking-points.html#where-assumptions-justify-we-may-condition-on-measured-confounders-to-obtain-balance.",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Where assumptions justify, we may condition on measured confounders to obtain balance.",
    "text": "Where assumptions justify, we may condition on measured confounders to obtain balance.\n\n\n\\[\n\\commoncauseT\n\\]\n\n\\[\n\\commoncausesolvedT\n\\]"
  },
  {
    "objectID": "content/talking-points.html#what-error-1-mediator-bias",
    "href": "content/talking-points.html#what-error-1-mediator-bias",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "“What Error #1”: Mediator Bias",
    "text": "“What Error #1”: Mediator Bias\n\\[\\mediatorT\\]"
  },
  {
    "objectID": "content/talking-points.html#data-generating-process",
    "href": "content/talking-points.html#data-generating-process",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(123)                           # reproducibility\nn        &lt;- 1000\nservice  &lt;- rbinom(n, 1, 0.5)          # A\nwealth   &lt;- 2 * service + rnorm(n)     # L\ncharity  &lt;- 1.5 * wealth + rnorm(n, sd = 1.5)  # Y\n\nsim1 &lt;- tibble(service, wealth, charity)"
  },
  {
    "objectID": "content/talking-points.html#model-comparison",
    "href": "content/talking-points.html#model-comparison",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Model comparison",
    "text": "Model comparison\n\n\n\n\n\n\nControlling for the mediator reverses the sign of the service coefficient.\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel A: Omit L\n\n\nModel B: Control for L\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nservice\n2.9\n2.6, 3.2\n&lt;0.001\n-0.27\n-0.53, -0.01\n0.043\n\n\nwealth\n\n\n\n\n\n\n1.6\n1.5, 1.7\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "content/talking-points.html#which-model-looks-better",
    "href": "content/talking-points.html#which-model-looks-better",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Which model looks better?",
    "text": "Which model looks better?\n\n\nCode\nlibrary(performance)\n\ncompare_performance(fit_adjust, fit_omit, rank = TRUE)\n\n\n# Comparison of Model Performance Indices\n\nName       | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights\n--------------------------------------------------------------------\nfit_adjust |    lm | 0.682 |     0.682 | 1.473 | 1.475 |        1.00\nfit_omit   |    lm | 0.312 |     0.311 | 2.169 | 2.171 |   2.88e-168\n\nName       | AICc weights | BIC weights | Performance-Score\n-----------------------------------------------------------\nfit_adjust |         1.00 |        1.00 |           100.00%\nfit_omit   |    2.91e-168 |   3.35e-167 |             0.00%\n\n\nCode\n# BIC(fit_adjust) - BIC(fit_omit)  # negative → \"better\" fit"
  },
  {
    "objectID": "content/talking-points.html#what-error-2-collider-bias",
    "href": "content/talking-points.html#what-error-2-collider-bias",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "“What Error #2”: Collider Bias",
    "text": "“What Error #2”: Collider Bias\n\\[\\colliderT\\]"
  },
  {
    "objectID": "content/talking-points.html#data-generating-process-1",
    "href": "content/talking-points.html#data-generating-process-1",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\n\nCode\nset.seed(2025)\n\nn          &lt;- 1000\nservice    &lt;- rbinom(n, 1, 0.5)                            # A\ndonations  &lt;- rnorm(n)                                     # Y\nwealth     &lt;- rnorm(n, mean = service + donations, sd = 1) # collider L\n\nsim2 &lt;- tibble(service, wealth, donations)"
  },
  {
    "objectID": "content/talking-points.html#model-comparison-1",
    "href": "content/talking-points.html#model-comparison-1",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\n\n\n\n\nAdding the collider creates a spurious, significant effect of A.\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel A: Omit L\n\n\nModel B: Control for L (collider)\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nservice\n0.00\n-0.12, 0.13\n&gt;0.9\n-0.53\n-0.63, -0.44\n&lt;0.001\n\n\nwealth\n\n\n\n\n\n\n0.51\n0.48, 0.54\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "content/talking-points.html#which-model-looks-better-1",
    "href": "content/talking-points.html#which-model-looks-better-1",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Which model looks better?",
    "text": "Which model looks better?\n\n\nCode\ncompare_performance(fit_biased, fit_correct, rank = TRUE)\n\n\n# Comparison of Model Performance Indices\n\nName        | Model |        R2 |  R2 (adj.) |  RMSE | Sigma | AIC weights\n--------------------------------------------------------------------------\nfit_biased  |    lm |     0.526 |      0.525 | 0.707 | 0.708 |        1.00\nfit_correct |    lm | 3.722e-06 | -9.983e-04 | 1.028 | 1.029 |   1.42e-162\n\nName        | AICc weights | BIC weights | Performance-Score\n------------------------------------------------------------\nfit_biased  |         1.00 |        1.00 |           100.00%\nfit_correct |    1.43e-162 |   1.65e-161 |             0.00%\n\n\nCode\nBIC(fit_biased) - BIC(fit_correct)\n\n\n[1] -740.425"
  },
  {
    "objectID": "content/talking-points.html#take-home",
    "href": "content/talking-points.html#take-home",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Take-Home",
    "text": "Take-Home\n\n\n\n\n\n\nImportant\n\n\n\nRelying on model fit perpetuates the causality crisis in psychology [@bulbulia2022].\nDraw the DAG first; decide what belongs in the model before looking at numbers."
  },
  {
    "objectID": "content/talking-points.html#the-what-error-is-widespread-in-experimental-studies-in-the-social-sciences",
    "href": "content/talking-points.html#the-what-error-is-widespread-in-experimental-studies-in-the-social-sciences",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "The What Error is widespread in experimental studies in the social sciences",
    "text": "The What Error is widespread in experimental studies in the social sciences\n\n“Overall, we find that 46.7% of the experimental studies published in APSR, AJPS, and JOP from 2012 to 2014 engaged in posttreatment conditioning (35 of 75 studies) …”\n“About 1 in 4 drop cases or subset the data based on post-treatment criteria, and nearly a third include post-treatment variables as covariates”\n“Most tellingly, nearly 1 in 8 articles directly conditions on variables that the authors themselves show as being an outcome of the experiment – an unambiguous indicator of a fundamental lack of understanding … that conditioning on posttreatment variables can invalidate results from randomized experiments.”\n“Empirically, then, the answer to the question of whether the discipline already understands posttreatment bias is clear: It does not.” [@montgomery2018]"
  },
  {
    "objectID": "content/talking-points.html#mediator-bias-control-strategy-longitudinal-hygiene",
    "href": "content/talking-points.html#mediator-bias-control-strategy-longitudinal-hygiene",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Mediator Bias control strategy: Longitudinal Hygiene",
    "text": "Mediator Bias control strategy: Longitudinal Hygiene\n\n\n\\[\n\\mediatorT\n\\]\n\n\\[\n\\commoncausesolvedT\n\\]"
  },
  {
    "objectID": "content/talking-points.html#how-to-tame-the-what-error-hide-your-future",
    "href": "content/talking-points.html#how-to-tame-the-what-error-hide-your-future",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "How to Tame The What Error? Hide your future",
    "text": "How to Tame The What Error? Hide your future\nUse Repeated Measures on the Same Individuals"
  },
  {
    "objectID": "content/talking-points.html#collider-bias-control-strategy-hide-your-future",
    "href": "content/talking-points.html#collider-bias-control-strategy-hide-your-future",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Collider bias control strategy: Hide your future",
    "text": "Collider bias control strategy: Hide your future\n\n\n\\[\\colliderT\\]\n\n\\[\\commoncausesolvedT\\]"
  },
  {
    "objectID": "content/talking-points.html#collider-bias-by-proxy-control-strategy-hide-your-future",
    "href": "content/talking-points.html#collider-bias-by-proxy-control-strategy-hide-your-future",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Collider bias by proxy control strategy: Hide your future",
    "text": "Collider bias by proxy control strategy: Hide your future\n\n\n\\[\\commoncausechildT\\]\n\n\\[\\commoncausesolvedchildT\\]"
  },
  {
    "objectID": "content/talking-points.html#post-exposure-collider-bias-control-strategy-hide-your-future",
    "href": "content/talking-points.html#post-exposure-collider-bias-control-strategy-hide-your-future",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Post-exposure collider bias control strategy: Hide your future",
    "text": "Post-exposure collider bias control strategy: Hide your future\n\n\n\\[\\mediatorcolliderT\\]\n\n\\[\\commoncausesolvedT\\]"
  },
  {
    "objectID": "content/talking-points.html#finally-our-biggest-worry-unmeasured-common-causes",
    "href": "content/talking-points.html#finally-our-biggest-worry-unmeasured-common-causes",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Finally, our biggest worry: Unmeasured Common Causes",
    "text": "Finally, our biggest worry: Unmeasured Common Causes\n\\[\\downstreamT\\]"
  },
  {
    "objectID": "content/talking-points.html#unmeasured-common-cause-strategy-hide-your-future",
    "href": "content/talking-points.html#unmeasured-common-cause-strategy-hide-your-future",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Unmeasured common cause strategy: Hide your future",
    "text": "Unmeasured common cause strategy: Hide your future\n\n\n\\[\\downstreamT\\]\n\n\\[\\commoncausesolvedT\\]\n\n\nAnd because , we should consistently report sensitivity analyses."
  },
  {
    "objectID": "content/talking-points.html#are-longitudinal-data-sensitivity-analysis-enough",
    "href": "content/talking-points.html#are-longitudinal-data-sensitivity-analysis-enough",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Are longitudinal data + sensitivity analysis enough?",
    "text": "Are longitudinal data + sensitivity analysis enough?\n\n\nIf the data we collect were like this: \\[Y_{\\text{time 0}} ~~...~~ A_{\\text{time 1}}\\]\n\nWe should not be tempted to model this.\n\\[\\ytoacrazyT\\]"
  },
  {
    "objectID": "content/talking-points.html#are-longitudinal-data-sensitivity-analysis-enough-1",
    "href": "content/talking-points.html#are-longitudinal-data-sensitivity-analysis-enough-1",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Are longitudinal data + sensitivity analysis enough?",
    "text": "Are longitudinal data + sensitivity analysis enough?\n this is wrong?\n\\[\\ytoacrazyT\\]"
  },
  {
    "objectID": "content/talking-points.html#longitudinal-data-are-not-enough",
    "href": "content/talking-points.html#longitudinal-data-are-not-enough",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Longitudinal data are not enough",
    "text": "Longitudinal data are not enough\nTemporal ordering was with the observational hormone studies in the 80s/90s that modelled .\nHow?"
  },
  {
    "objectID": "content/talking-points.html#researchers-failed-to-emulate-an-experiment-in-their-data-target-trial",
    "href": "content/talking-points.html#researchers-failed-to-emulate-an-experiment-in-their-data-target-trial",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Researchers failed to emulate an experiment in their data (Target Trial)",
    "text": "Researchers failed to emulate an experiment in their data (Target Trial)\n\nWomen’s Health Initiative: overall hazard ratio 1.23 (0.99, 1.53)\nWomen’s Health Initiative: when broken down by years to follow-up:\n\n0-2 years 1.51 (1.06, 2.14)\n2-5 years 1.31 (0.93, 1.83)\n5 or more years 0.67 (0.41, 1.09)\n\n\n\n\n\n\n\n\nNoteSurvivor Bias.\n\n\n\nThe observational results can be \n\n\n\n\n\n\n\n\nNoteEmulating a target trial with observational data recovers experimental effects.\n\n\n\nRe-modelling recovers experimental findings [@hernan2016]."
  },
  {
    "objectID": "content/talking-points.html#visualising-the-when-error",
    "href": "content/talking-points.html#visualising-the-when-error",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Visualising the When Error",
    "text": "Visualising the When Error\n\n\n\n\n\n\n\n\nFigure 1: Three ways to start the clock. Only one is right."
  },
  {
    "objectID": "content/talking-points.html#target-trial-check-list",
    "href": "content/talking-points.html#target-trial-check-list",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Target Trial Check List",
    "text": "Target Trial Check List\n\n\n\n\nTable 1: Target-trial specification for the hormone-therapy example.\n\n\n\n\n\n\nelement\ndefinition\n\n\n\n\nEligibility\nPost-menopausal women aged 50–79, no prior CHD\n\n\nTreatment\nInitiate oestrogen + progestin on day of Rx pickup\n\n\nComparison\nNo hormone-therapy initiation on that day\n\n\nOutcome\nAll-cause mortality\n\n\nFollow-up\n8 years or until death / loss to follow-up"
  },
  {
    "objectID": "content/talking-points.html#illustration",
    "href": "content/talking-points.html#illustration",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "content/talking-points.html#the-new-zealand-attitudes-and-values-study",
    "href": "content/talking-points.html#the-new-zealand-attitudes-and-values-study",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "The New Zealand Attitudes and Values Study:",
    "text": "The New Zealand Attitudes and Values Study:\n\nPlanned 20-year longitudinal study, currently in its 14th year, &gt; 77k measured\nSample frame is drawn randomly from the NZ Electoral Roll."
  },
  {
    "objectID": "content/talking-points.html#causal-questions",
    "href": "content/talking-points.html#causal-questions",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Causal Questions",
    "text": "Causal Questions\n\nAverage Treatment Effect: Difference in averages of social outcomes (a) if everyone in New Zealand were to attend religious service at least 1x per month (b) if everyone were not to attend religious service at all.\nHeterogeneous Treatment Effects: Machine learning of groups who differ in such effects. Focus here on Qini Curve at 20% and 30% of full “budget”.\nUse Qini results to select for Policy Trees: Clarify who responds strongly/weakly.\n\nNote: all results use “honest” learning – sample is split into training/test sets (here 70/30)."
  },
  {
    "objectID": "content/talking-points.html#outcomes",
    "href": "content/talking-points.html#outcomes",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Outcomes",
    "text": "Outcomes\n\nSense of Belonging\nSocial Support\nCharitable Donations (annual)\nVolunteering (weekly)\n\nAll outcomes measured one year after the intervention"
  },
  {
    "objectID": "content/talking-points.html#three-wave-panel-design-control-for-baseline-exposure-and-outcome-among-confounder-set.",
    "href": "content/talking-points.html#three-wave-panel-design-control-for-baseline-exposure-and-outcome-among-confounder-set.",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Three-wave Panel Design: control for baseline exposure and outcome among confounder set.",
    "text": "Three-wave Panel Design: control for baseline exposure and outcome among confounder set.\n\\[\n\\threeLONG\n\\]"
  },
  {
    "objectID": "content/talking-points.html#target-population-residents-of-new-zealand-in-2018-had-they-not-been-censored-through-2021.",
    "href": "content/talking-points.html#target-population-residents-of-new-zealand-in-2018-had-they-not-been-censored-through-2021.",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Target Population: Residents of New Zealand in 2018 had they not been censored through 2021.",
    "text": "Target Population: Residents of New Zealand in 2018 had they not been censored through 2021.\n\nCensus Weights (Age X Gender X Ethnicity)\n\n\n\n(IPCW Stabilised & Trimmed)"
  },
  {
    "objectID": "content/talking-points.html#religious-service-intervention",
    "href": "content/talking-points.html#religious-service-intervention",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Religious Service Intervention",
    "text": "Religious Service Intervention\n\\[f(A) = \\begin{cases} 1 & \\text{if } a \\ge 0 \\text{ At least 1 x per month Religious Attendance} \\\\ 0 & \\text{if } a = 0 \\text{ Monthly Religious Service} \\end{cases}\\]"
  },
  {
    "objectID": "content/talking-points.html#histogram-showing-split",
    "href": "content/talking-points.html#histogram-showing-split",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Histogram Showing Split",
    "text": "Histogram Showing Split"
  },
  {
    "objectID": "content/talking-points.html#love-plot-of-propensity-scores",
    "href": "content/talking-points.html#love-plot-of-propensity-scores",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Love Plot of Propensity Scores",
    "text": "Love Plot of Propensity Scores"
  },
  {
    "objectID": "content/talking-points.html#average-treatment-effect-results",
    "href": "content/talking-points.html#average-treatment-effect-results",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Average Treatment Effect Results:",
    "text": "Average Treatment Effect Results:"
  },
  {
    "objectID": "content/talking-points.html#workflow",
    "href": "content/talking-points.html#workflow",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Workflow",
    "text": "Workflow\n\nTarget population: New Zealand Population by 2018 census weights\nOutcomes: all NZAVS outcomes relating to pro-sociality (outcome-wide study)\nMissing data: inverse-probability of censoring weights non-parametrically estimated/ causal forests permit missing data at baseline\nEligibility: participated in NZAVS 2018 (wave-10) and 2019 (wave 11, exposure wave) may have been lost to follow-up in wave 12, 46,377.\nBonferroni correct for multiple comparisons / E-value threshold = 1.2"
  },
  {
    "objectID": "content/talking-points.html#transition-table-religious-attendance",
    "href": "content/talking-points.html#transition-table-religious-attendance",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Transition table: Religious Attendance",
    "text": "Transition table: Religious Attendance\n\n\n\nFrom / To\nState 0\nState 1\nTotal\n\n\n\n\nState 0\n28545\n681\n29226\n\n\nState 1\n887\n3593\n4480"
  },
  {
    "objectID": "content/talking-points.html#average-treatment-effect",
    "href": "content/talking-points.html#average-treatment-effect",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nThe following outcomes showed reliable causal evidence (E‑value lower bound &gt; 1.2): - Volunteering (weekly, log): 0.154(0.056,0.252); on the original scale, 11.264 minutes(3.943,19.158). E‑value bound = 1.287 - log Charity Donate: 0.13(0.045,0.215); on the original scale, 39.935 (12.247,74.883). E‑value bound = 1.25"
  },
  {
    "objectID": "content/talking-points.html#how-much-better-if-we-treated-by-cate",
    "href": "content/talking-points.html#how-much-better-if-we-treated-by-cate",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "How Much Better if We Treated by CATE?",
    "text": "How Much Better if We Treated by CATE?"
  },
  {
    "objectID": "content/talking-points.html#who-should-we-treat-first-belonging",
    "href": "content/talking-points.html#who-should-we-treat-first-belonging",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Who Should We Treat First? Belonging",
    "text": "Who Should We Treat First? Belonging"
  },
  {
    "objectID": "content/talking-points.html#who-should-we-treat-first-charitable-donations",
    "href": "content/talking-points.html#who-should-we-treat-first-charitable-donations",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Who Should We Treat First? Charitable Donations",
    "text": "Who Should We Treat First? Charitable Donations"
  },
  {
    "objectID": "content/talking-points.html#closing",
    "href": "content/talking-points.html#closing",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Closing",
    "text": "Closing\n\n\n\n\n\n\n\n\nError\nNeed\nRemedy\n\n\n\n\nWhat\nControl the right variables\nDraw a DAG first\n\n\nWhen\nStart the clock on time\nEmulate a target trial\n\n\nWho\nTreat the right people\nWeight to target + investigate heterogeneity"
  },
  {
    "objectID": "content/talking-points.html#thanks",
    "href": "content/talking-points.html#thanks",
    "title": "Three Types of Self-Inflicted Injuries in Observational Research",
    "section": "Thanks",
    "text": "Thanks\n\n\nTRT Grant 0418 & Max Planck Institute for Evolutionary Anthropology for support.\nChris G. Sibley (NZAVS lead Investigator) for the heavy data lifting\n77,490 NZAVS participants for their time\n with questions or if you would like to become involved: \n\n\n\n\n\n\nNZAVS"
  },
  {
    "objectID": "content/discussion-points.html",
    "href": "content/discussion-points.html",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "NoteIn the 1980s and 1990s estrogen treatments appeared to benefit postmenopausal women\n\n\n\nHazard ratio for all-cause mortality: 0.68 current users vs. never users [@grodstein2006hormone]\n\n\n\n\n\n\n\n1992 American College of Obstetricians and Gynecologists \n1992 American College of Physicians \n1993 National Cholesterol Education Program \n1996 American Heart Association \n\n\n\n\n\n\n\nMassive randomised, double-blind, placebo-controlled trial\n16,000 U.S. women aged 50-79 years\nAssigned to Estrogen plus Progestin therapy\nWomen followed approximately every year for up to 8 years.\n\n\n\n\n\n\n\n\n\n\n\nWarningExperimental Findings: opposite of observational findings\n\n\n\nAll-cause mortality Hazard Ratio: 1.23 for initiators vs non-initiators. [@manson2003estrogen]\n\n\n\n\n\n\n\nCan observational studies ever be trusted?\nShould observational studies ever be funded again?\n\n\n\n\n\n\n\n\n\n\n\n\nWe may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second…\n\n\n\n\n\nOr, in other words, where, \n\nEnquiries Concerning Human Understanding, and Concerning the Principles of Morals 1751\n\n\n\n\n\n\n\n\n\n\n\n\nTo quantify a causal effect requires a counterfactual contrast:\n\\[\\tau_{you} = \\Big[Y_{\\text{you}}(a =1) - Y_{\\text{you}}(a=0)\\Big]\\]\nWhere, \\(Y(a)\\) denotes the potential outcome under an intervention \\(A = a\\). Here, we assume a binary intervention. At any time, we may observe the outcome of only \n\n\n\n\\[\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\textcolor{red}{\\text{is counterfactual}}\n\\]\n\n“And sorry I could not travel both. And be one traveller, long I stood \\(\\dots\\)”\n\n\n\n\n\\[\n\\text{Average Treatment Effect} = \\left[ \\begin{aligned}\n&\\left( \\underbrace{\\mathbb{E}[Y(1)|A = 1]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(1)|A = 0]}_{\\text{unobserved}}} \\right) \\\\\n&- \\left( \\underbrace{\\mathbb{E}[Y(0)|A = 0]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(0)|A = 1]}_{\\text{unobserved}}} \\right)\n\\end{aligned} \\right]\n\\]\n\n\n\n\\[\n\\text{ATE} = \\sum_{l} \\left( \\mathbb{E}[Y|A=1, \\textcolor{cyan}{L=l}] - \\mathbb{E}[Y|A=0, \\textcolor{cyan}{L=l}] \\right) \\times \\textcolor{cyan}{\\Pr(L=l)}\n\\]\nWhere \\(L\\) is a set of measured covariates and \\(A\\coprod Y(a)|L\\)"
  },
  {
    "objectID": "content/discussion-points.html#s-observational-studies-indicated-30-all-cause-mortality-reduction-from-estrogen-therapies",
    "href": "content/discussion-points.html#s-observational-studies-indicated-30-all-cause-mortality-reduction-from-estrogen-therapies",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "NoteIn the 1980s and 1990s estrogen treatments appeared to benefit postmenopausal women\n\n\n\nHazard ratio for all-cause mortality: 0.68 current users vs. never users [@grodstein2006hormone]"
  },
  {
    "objectID": "content/discussion-points.html#standard-medical-advice",
    "href": "content/discussion-points.html#standard-medical-advice",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "1992 American College of Obstetricians and Gynecologists \n1992 American College of Physicians \n1993 National Cholesterol Education Program \n1996 American Heart Association"
  },
  {
    "objectID": "content/discussion-points.html#womens-health-initiative-evaluate-estrogens-experimentally",
    "href": "content/discussion-points.html#womens-health-initiative-evaluate-estrogens-experimentally",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "Massive randomised, double-blind, placebo-controlled trial\n16,000 U.S. women aged 50-79 years\nAssigned to Estrogen plus Progestin therapy\nWomen followed approximately every year for up to 8 years."
  },
  {
    "objectID": "content/discussion-points.html#findings-clear-discrepancy",
    "href": "content/discussion-points.html#findings-clear-discrepancy",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "WarningExperimental Findings: opposite of observational findings\n\n\n\nAll-cause mortality Hazard Ratio: 1.23 for initiators vs non-initiators. [@manson2003estrogen]"
  },
  {
    "objectID": "content/discussion-points.html#medical-community-response-reject-all-observational-studies",
    "href": "content/discussion-points.html#medical-community-response-reject-all-observational-studies",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "Can observational studies ever be trusted?\nShould observational studies ever be funded again?"
  },
  {
    "objectID": "content/discussion-points.html#david-humes-two-definitions-in-enquiries-1751",
    "href": "content/discussion-points.html#david-humes-two-definitions-in-enquiries-1751",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "We may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second…\n\n\n\n\n\nOr, in other words, where, \n\nEnquiries Concerning Human Understanding, and Concerning the Principles of Morals 1751"
  },
  {
    "objectID": "content/discussion-points.html#the-fundamental-problem-of-causal-inference",
    "href": "content/discussion-points.html#the-fundamental-problem-of-causal-inference",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "To quantify a causal effect requires a counterfactual contrast:\n\\[\\tau_{you} = \\Big[Y_{\\text{you}}(a =1) - Y_{\\text{you}}(a=0)\\Big]\\]\nWhere, \\(Y(a)\\) denotes the potential outcome under an intervention \\(A = a\\). Here, we assume a binary intervention. At any time, we may observe the outcome of only"
  },
  {
    "objectID": "content/discussion-points.html#however-we-only-observe-facts-not-counterfactuals",
    "href": "content/discussion-points.html#however-we-only-observe-facts-not-counterfactuals",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "\\[\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\textcolor{red}{\\text{is counterfactual}}\n\\]\n\n“And sorry I could not travel both. And be one traveller, long I stood \\(\\dots\\)”"
  },
  {
    "objectID": "content/discussion-points.html#average-treatment-effect-in-randomised-controlled-experiments-work-from-assumptions",
    "href": "content/discussion-points.html#average-treatment-effect-in-randomised-controlled-experiments-work-from-assumptions",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "\\[\n\\text{Average Treatment Effect} = \\left[ \\begin{aligned}\n&\\left( \\underbrace{\\mathbb{E}[Y(1)|A = 1]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(1)|A = 0]}_{\\text{unobserved}}} \\right) \\\\\n&- \\left( \\underbrace{\\mathbb{E}[Y(0)|A = 0]}_{\\text{observed}} + \\textcolor{cyan}{\\underbrace{\\mathbb{E}[Y(0)|A = 1]}_{\\text{unobserved}}} \\right)\n\\end{aligned} \\right]\n\\]"
  },
  {
    "objectID": "content/discussion-points.html#under-identifying-assumptions-we-may-infer-causal-effects-from-associations",
    "href": "content/discussion-points.html#under-identifying-assumptions-we-may-infer-causal-effects-from-associations",
    "title": "Self-Inflicted Injuries",
    "section": "",
    "text": "\\[\n\\text{ATE} = \\sum_{l} \\left( \\mathbb{E}[Y|A=1, \\textcolor{cyan}{L=l}] - \\mathbb{E}[Y|A=0, \\textcolor{cyan}{L=l}] \\right) \\times \\textcolor{cyan}{\\Pr(L=l)}\n\\]\nWhere \\(L\\) is a set of measured covariates and \\(A\\coprod Y(a)|L\\)"
  },
  {
    "objectID": "content/discussion-points.html#paradigmatic-concern-confounding-by-common-cause",
    "href": "content/discussion-points.html#paradigmatic-concern-confounding-by-common-cause",
    "title": "Self-Inflicted Injuries",
    "section": "Paradigmatic Concern: Confounding by Common Cause",
    "text": "Paradigmatic Concern: Confounding by Common Cause\n\\[\\commoncauseT\\]"
  },
  {
    "objectID": "content/discussion-points.html#where-assumptions-justify-we-may-condition-on-measured-confounders-to-obtain-balance.",
    "href": "content/discussion-points.html#where-assumptions-justify-we-may-condition-on-measured-confounders-to-obtain-balance.",
    "title": "Self-Inflicted Injuries",
    "section": "Where assumptions justify, we may condition on measured confounders to obtain balance.",
    "text": "Where assumptions justify, we may condition on measured confounders to obtain balance.\n\n\n\\[\n\\commoncauseT\n\\]\n\n\\[\n\\commoncausesolvedT\n\\]"
  },
  {
    "objectID": "content/discussion-points.html#what-error-1-mediator-bias",
    "href": "content/discussion-points.html#what-error-1-mediator-bias",
    "title": "Self-Inflicted Injuries",
    "section": "“What Error #1”: Mediator Bias",
    "text": "“What Error #1”: Mediator Bias\n\\[\\mediatorT\\]"
  },
  {
    "objectID": "content/discussion-points.html#data-generating-process",
    "href": "content/discussion-points.html#data-generating-process",
    "title": "Self-Inflicted Injuries",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(123)                           # reproducibility\nn        &lt;- 1000\nservice  &lt;- rbinom(n, 1, 0.5)          # A\nwealth   &lt;- 2 * service + rnorm(n)     # L\ncharity  &lt;- 1.5 * wealth + rnorm(n, sd = 1.5)  # Y\n\nsim1 &lt;- tibble(service, wealth, charity)"
  },
  {
    "objectID": "content/discussion-points.html#model-comparison",
    "href": "content/discussion-points.html#model-comparison",
    "title": "Self-Inflicted Injuries",
    "section": "Model comparison",
    "text": "Model comparison\n\n\n\n\n\n\nControlling for the mediator reverses the sign of the service coefficient.\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel A: Omit L\n\n\nModel B: Control for L\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nservice\n2.9\n2.6, 3.2\n&lt;0.001\n-0.27\n-0.53, -0.01\n0.043\n\n\nwealth\n\n\n\n\n\n\n1.6\n1.5, 1.7\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "content/discussion-points.html#which-model-looks-better",
    "href": "content/discussion-points.html#which-model-looks-better",
    "title": "Self-Inflicted Injuries",
    "section": "Which model looks better?",
    "text": "Which model looks better?\n\n\nCode\nlibrary(performance)\n\ncompare_performance(fit_adjust, fit_omit, rank = TRUE)\n\n\n# Comparison of Model Performance Indices\n\nName       | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights\n--------------------------------------------------------------------\nfit_adjust |    lm | 0.682 |     0.682 | 1.473 | 1.475 |        1.00\nfit_omit   |    lm | 0.312 |     0.311 | 2.169 | 2.171 |   2.88e-168\n\nName       | AICc weights | BIC weights | Performance-Score\n-----------------------------------------------------------\nfit_adjust |         1.00 |        1.00 |           100.00%\nfit_omit   |    2.91e-168 |   3.35e-167 |             0.00%\n\n\nCode\n# BIC(fit_adjust) - BIC(fit_omit)  # negative → \"better\" fit"
  },
  {
    "objectID": "content/discussion-points.html#what-error-2-collider-bias",
    "href": "content/discussion-points.html#what-error-2-collider-bias",
    "title": "Self-Inflicted Injuries",
    "section": "“What Error #2”: Collider Bias",
    "text": "“What Error #2”: Collider Bias\n\\[\\colliderT\\]"
  },
  {
    "objectID": "content/discussion-points.html#data-generating-process-1",
    "href": "content/discussion-points.html#data-generating-process-1",
    "title": "Self-Inflicted Injuries",
    "section": "Data Generating Process",
    "text": "Data Generating Process\n\n\nCode\nset.seed(2025)\n\nn          &lt;- 1000\nservice    &lt;- rbinom(n, 1, 0.5)                            # A\ndonations  &lt;- rnorm(n)                                     # Y\nwealth     &lt;- rnorm(n, mean = service + donations, sd = 1) # collider L\n\nsim2 &lt;- tibble(service, wealth, donations)"
  },
  {
    "objectID": "content/discussion-points.html#model-comparison-1",
    "href": "content/discussion-points.html#model-comparison-1",
    "title": "Self-Inflicted Injuries",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\n\n\n\n\nAdding the collider creates a spurious, significant effect of A.\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel A: Omit L\n\n\nModel B: Control for L (collider)\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nservice\n0.00\n-0.12, 0.13\n&gt;0.9\n-0.53\n-0.63, -0.44\n&lt;0.001\n\n\nwealth\n\n\n\n\n\n\n0.51\n0.48, 0.54\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval"
  },
  {
    "objectID": "content/discussion-points.html#which-model-looks-better-1",
    "href": "content/discussion-points.html#which-model-looks-better-1",
    "title": "Self-Inflicted Injuries",
    "section": "Which model looks better?",
    "text": "Which model looks better?\n\n\nCode\ncompare_performance(fit_biased, fit_correct, rank = TRUE)\n\n\n# Comparison of Model Performance Indices\n\nName        | Model |        R2 |  R2 (adj.) |  RMSE | Sigma | AIC weights\n--------------------------------------------------------------------------\nfit_biased  |    lm |     0.526 |      0.525 | 0.707 | 0.708 |        1.00\nfit_correct |    lm | 3.722e-06 | -9.983e-04 | 1.028 | 1.029 |   1.42e-162\n\nName        | AICc weights | BIC weights | Performance-Score\n------------------------------------------------------------\nfit_biased  |         1.00 |        1.00 |           100.00%\nfit_correct |    1.43e-162 |   1.65e-161 |             0.00%\n\n\nCode\nBIC(fit_biased) - BIC(fit_correct)\n\n\n[1] -740.425"
  },
  {
    "objectID": "content/discussion-points.html#take-home",
    "href": "content/discussion-points.html#take-home",
    "title": "Self-Inflicted Injuries",
    "section": "Take-Home",
    "text": "Take-Home\n\n\n\n\n\n\nImportant\n\n\n\nRelying on model fit perpetuates the causality crisis in psychology [@bulbulia2022].\nDraw the DAG first; decide what belongs in the model before looking at numbers."
  },
  {
    "objectID": "content/discussion-points.html#the-what-error-is-widespread-in-experimental-studies-in-the-social-sciences",
    "href": "content/discussion-points.html#the-what-error-is-widespread-in-experimental-studies-in-the-social-sciences",
    "title": "Self-Inflicted Injuries",
    "section": "The What Error is widespread in experimental studies in the social sciences",
    "text": "The What Error is widespread in experimental studies in the social sciences\n\n“Overall, we find that 46.7% of the experimental studies published in APSR, AJPS, and JOP from 2012 to 2014 engaged in posttreatment conditioning (35 of 75 studies) …”\n“About 1 in 4 drop cases or subset the data based on post-treatment criteria, and nearly a third include post-treatment variables as covariates”\n“Most tellingly, nearly 1 in 8 articles directly conditions on variables that the authors themselves show as being an outcome of the experiment – an unambiguous indicator of a fundamental lack of understanding … that conditioning on posttreatment variables can invalidate results from randomized experiments.”\n“Empirically, then, the answer to the question of whether the discipline already understands posttreatment bias is clear: It does not.” [@montgomery2018]"
  },
  {
    "objectID": "content/discussion-points.html#mediator-bias-control-strategy-longitudinal-hygiene",
    "href": "content/discussion-points.html#mediator-bias-control-strategy-longitudinal-hygiene",
    "title": "Self-Inflicted Injuries",
    "section": "Mediator Bias control strategy: Longitudinal Hygiene",
    "text": "Mediator Bias control strategy: Longitudinal Hygiene\n\n\n\\[\n\\mediatorT\n\\]\n\n\\[\n\\commoncausesolvedT\n\\]"
  },
  {
    "objectID": "content/discussion-points.html#how-to-tame-the-what-error-hide-your-future",
    "href": "content/discussion-points.html#how-to-tame-the-what-error-hide-your-future",
    "title": "Self-Inflicted Injuries",
    "section": "How to Tame The What Error? Hide your future",
    "text": "How to Tame The What Error? Hide your future\nUse Repeated Measures on the Same Individuals"
  },
  {
    "objectID": "content/discussion-points.html#collider-bias-control-strategy-hide-your-future",
    "href": "content/discussion-points.html#collider-bias-control-strategy-hide-your-future",
    "title": "Self-Inflicted Injuries",
    "section": "Collider bias control strategy: Hide your future",
    "text": "Collider bias control strategy: Hide your future\n\n\n\\[\\colliderT\\]\n\n\\[\\commoncausesolvedT\\]"
  },
  {
    "objectID": "content/discussion-points.html#collider-bias-by-proxy-control-strategy-hide-your-future",
    "href": "content/discussion-points.html#collider-bias-by-proxy-control-strategy-hide-your-future",
    "title": "Self-Inflicted Injuries",
    "section": "Collider bias by proxy control strategy: Hide your future",
    "text": "Collider bias by proxy control strategy: Hide your future\n\n\n\\[\\commoncausechildT\\]\n\n\\[\\commoncausesolvedchildT\\]"
  },
  {
    "objectID": "content/discussion-points.html#post-exposure-collider-bias-control-strategy-hide-your-future",
    "href": "content/discussion-points.html#post-exposure-collider-bias-control-strategy-hide-your-future",
    "title": "Self-Inflicted Injuries",
    "section": "Post-exposure collider bias control strategy: Hide your future",
    "text": "Post-exposure collider bias control strategy: Hide your future\n\n\n\\[\\mediatorcolliderT\\]\n\n\\[\\commoncausesolvedT\\]"
  },
  {
    "objectID": "content/discussion-points.html#finally-our-biggest-worry-unmeasured-common-causes",
    "href": "content/discussion-points.html#finally-our-biggest-worry-unmeasured-common-causes",
    "title": "Self-Inflicted Injuries",
    "section": "Finally, our biggest worry: Unmeasured Common Causes",
    "text": "Finally, our biggest worry: Unmeasured Common Causes\n\\[\\downstreamT\\]"
  },
  {
    "objectID": "content/discussion-points.html#unmeasured-common-cause-strategy-hide-your-future",
    "href": "content/discussion-points.html#unmeasured-common-cause-strategy-hide-your-future",
    "title": "Self-Inflicted Injuries",
    "section": "Unmeasured common cause strategy: Hide your future",
    "text": "Unmeasured common cause strategy: Hide your future\n\n\n\\[\\downstreamT\\]\n\n\\[\\commoncausesolvedT\\]\n\n\nAnd because , we should consistently report sensitivity analyses."
  },
  {
    "objectID": "content/discussion-points.html#are-longitudinal-data-sensitivity-analysis-enough",
    "href": "content/discussion-points.html#are-longitudinal-data-sensitivity-analysis-enough",
    "title": "Self-Inflicted Injuries",
    "section": "Are longitudinal data + sensitivity analysis enough?",
    "text": "Are longitudinal data + sensitivity analysis enough?\n\n\nIf the data we collect were like this: \\[Y_{\\text{time 0}} ~~...~~ A_{\\text{time 1}}\\]\n\nWe should not be tempted to model this.\n\\[\\ytoacrazyT\\]"
  },
  {
    "objectID": "content/discussion-points.html#are-longitudinal-data-sensitivity-analysis-enough-1",
    "href": "content/discussion-points.html#are-longitudinal-data-sensitivity-analysis-enough-1",
    "title": "Self-Inflicted Injuries",
    "section": "Are longitudinal data + sensitivity analysis enough?",
    "text": "Are longitudinal data + sensitivity analysis enough?\n this is wrong?\n\\[\\ytoacrazyT\\]"
  },
  {
    "objectID": "content/discussion-points.html#longitudinal-data-are-not-enough",
    "href": "content/discussion-points.html#longitudinal-data-are-not-enough",
    "title": "Self-Inflicted Injuries",
    "section": "Longitudinal data are not enough",
    "text": "Longitudinal data are not enough\nTemporal ordering was with the observational hormone studies in the 80s/90s that modelled .\nHow?"
  },
  {
    "objectID": "content/discussion-points.html#researchers-failed-to-emulate-an-experiment-in-their-data-target-trial",
    "href": "content/discussion-points.html#researchers-failed-to-emulate-an-experiment-in-their-data-target-trial",
    "title": "Self-Inflicted Injuries",
    "section": "Researchers failed to emulate an experiment in their data (Target Trial)",
    "text": "Researchers failed to emulate an experiment in their data (Target Trial)\n\nWomen’s Health Initiative: overall hazard ratio 1.23 (0.99, 1.53)\nWomen’s Health Initiative: when broken down by years to follow-up:\n\n0-2 years 1.51 (1.06, 2.14)\n2-5 years 1.31 (0.93, 1.83)\n5 or more years 0.67 (0.41, 1.09)\n\n\n\n\n\n\n\n\nNoteSurvivor Bias.\n\n\n\nThe observational results can be \n\n\n\n\n\n\n\n\nNoteEmulating a target trial with observational data recovers experimental effects.\n\n\n\nRe-modelling recovers experimental findings [@hernan2016]."
  },
  {
    "objectID": "content/discussion-points.html#visualising-the-when-error",
    "href": "content/discussion-points.html#visualising-the-when-error",
    "title": "Self-Inflicted Injuries",
    "section": "Visualising the When Error",
    "text": "Visualising the When Error\n\n\n\n\n\n\n\n\nFigure 1: Three ways to start the clock. Only one is right."
  },
  {
    "objectID": "content/discussion-points.html#target-trial-check-list",
    "href": "content/discussion-points.html#target-trial-check-list",
    "title": "Self-Inflicted Injuries",
    "section": "Target Trial Check List",
    "text": "Target Trial Check List\n\n\n\n\nTable 1: Target-trial specification for the hormone-therapy example.\n\n\n\n\n\n\nelement\ndefinition\n\n\n\n\nEligibility\nPost-menopausal women aged 50–79, no prior CHD\n\n\nTreatment\nInitiate oestrogen + progestin on day of Rx pickup\n\n\nComparison\nNo hormone-therapy initiation on that day\n\n\nOutcome\nAll-cause mortality\n\n\nFollow-up\n8 years or until death / loss to follow-up"
  },
  {
    "objectID": "content/discussion-points.html#illustration",
    "href": "content/discussion-points.html#illustration",
    "title": "Self-Inflicted Injuries",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "content/discussion-points.html#the-new-zealand-attitudes-and-values-study",
    "href": "content/discussion-points.html#the-new-zealand-attitudes-and-values-study",
    "title": "Self-Inflicted Injuries",
    "section": "The New Zealand Attitudes and Values Study:",
    "text": "The New Zealand Attitudes and Values Study:\n\nPlanned 20-year longitudinal study, currently in its 14th year, &gt; 77k measured\nSample frame is drawn randomly from the NZ Electoral Roll."
  },
  {
    "objectID": "content/discussion-points.html#causal-questions",
    "href": "content/discussion-points.html#causal-questions",
    "title": "Self-Inflicted Injuries",
    "section": "Causal Questions",
    "text": "Causal Questions\n\nAverage Treatment Effect: Difference in averages of social outcomes (a) if everyone in New Zealand were to attend religious service at least 1x per month (b) if everyone were not to attend religious service at all.\nHeterogeneous Treatment Effects: Machine learning of groups who differ in such effects. Focus here on Qini Curve at 20% and 30% of full “budget”.\nUse Qini results to select for Policy Trees: Clarify who responds strongly/weakly.\n\nNote: all results use “honest” learning – sample is split into training/test sets (here 70/30)."
  },
  {
    "objectID": "content/discussion-points.html#outcomes",
    "href": "content/discussion-points.html#outcomes",
    "title": "Self-Inflicted Injuries",
    "section": "Outcomes",
    "text": "Outcomes\n\nSense of Belonging\nSocial Support\nCharitable Donations (annual)\nVolunteering (weekly)\n\nAll outcomes measured one year after the intervention"
  },
  {
    "objectID": "content/discussion-points.html#three-wave-panel-design-control-for-baseline-exposure-and-outcome-among-confounder-set.",
    "href": "content/discussion-points.html#three-wave-panel-design-control-for-baseline-exposure-and-outcome-among-confounder-set.",
    "title": "Self-Inflicted Injuries",
    "section": "Three-wave Panel Design: control for baseline exposure and outcome among confounder set.",
    "text": "Three-wave Panel Design: control for baseline exposure and outcome among confounder set.\n\\[\n\\threeLONG\n\\]"
  },
  {
    "objectID": "content/discussion-points.html#target-population-residents-of-new-zealand-in-2018-had-they-not-been-censored-through-2021.",
    "href": "content/discussion-points.html#target-population-residents-of-new-zealand-in-2018-had-they-not-been-censored-through-2021.",
    "title": "Self-Inflicted Injuries",
    "section": "Target Population: Residents of New Zealand in 2018 had they not been censored through 2021.",
    "text": "Target Population: Residents of New Zealand in 2018 had they not been censored through 2021.\n\nCensus Weights (Age X Gender X Ethnicity)\n\n\n\n\n\n\n\n\nFigure 2: Census weights for the NZAVS analytic sample.\n\n\n\n\n\n\n\n(IPCW Stabilised & Trimmed)\n\n\n\n\n\n\n\n\nFigure 3: Stabilised inverse probability of censoring weights."
  },
  {
    "objectID": "content/discussion-points.html#religious-service-intervention",
    "href": "content/discussion-points.html#religious-service-intervention",
    "title": "Self-Inflicted Injuries",
    "section": "Religious Service Intervention",
    "text": "Religious Service Intervention\n\\[f(A) = \\begin{cases} 1 & \\text{if } a \\ge 0 \\text{ At least 1 x per month Religious Attendance} \\\\ 0 & \\text{if } a = 0 \\text{ Monthly Religious Service} \\end{cases}\\]"
  },
  {
    "objectID": "content/discussion-points.html#histogram-showing-split",
    "href": "content/discussion-points.html#histogram-showing-split",
    "title": "Self-Inflicted Injuries",
    "section": "Histogram Showing Split",
    "text": "Histogram Showing Split\n\n\n\n\n\nAdd images/2025_church_graph_cut.png after running the eligibility workflow (laboratory/workshop-example/01-init-L10.R).\n\n\n\nFigure 4: Distribution of monthly religious attendance (cut at ≥1)."
  },
  {
    "objectID": "content/discussion-points.html#love-plot-of-propensity-scores",
    "href": "content/discussion-points.html#love-plot-of-propensity-scores",
    "title": "Self-Inflicted Injuries",
    "section": "Love Plot of Propensity Scores",
    "text": "Love Plot of Propensity Scores\n\n\n\n\n\nPropensity diagnostics appear once policy-tree scripts export images/2025_church_propensity.png.\n\n\n\nFigure 5: Propensity score diagnostic."
  },
  {
    "objectID": "content/discussion-points.html#average-treatment-effect-results",
    "href": "content/discussion-points.html#average-treatment-effect-results",
    "title": "Self-Inflicted Injuries",
    "section": "Average Treatment Effect Results:",
    "text": "Average Treatment Effect Results:"
  },
  {
    "objectID": "content/discussion-points.html#workflow",
    "href": "content/discussion-points.html#workflow",
    "title": "Self-Inflicted Injuries",
    "section": "Workflow",
    "text": "Workflow\n\nTarget population: New Zealand Population by 2018 census weights\nOutcomes: all NZAVS outcomes relating to pro-sociality (outcome-wide study)\nMissing data: inverse-probability of censoring weights non-parametrically estimated/ causal forests permit missing data at baseline\nEligibility: participated in NZAVS 2018 (wave-10) and 2019 (wave 11, exposure wave) may have been lost to follow-up in wave 12, 46,377.\nBonferroni correct for multiple comparisons / E-value threshold = 1.2"
  },
  {
    "objectID": "content/discussion-points.html#transition-table-religious-attendance",
    "href": "content/discussion-points.html#transition-table-religious-attendance",
    "title": "Self-Inflicted Injuries",
    "section": "Transition table: Religious Attendance",
    "text": "Transition table: Religious Attendance\n\n\n\nFrom / To\nState 0\nState 1\nTotal\n\n\n\n\nState 0\n28545\n681\n29226\n\n\nState 1\n887\n3593\n4480"
  },
  {
    "objectID": "content/discussion-points.html#average-treatment-effect",
    "href": "content/discussion-points.html#average-treatment-effect",
    "title": "Self-Inflicted Injuries",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nThe following outcomes showed reliable causal evidence (E‑value lower bound &gt; 1.2): - Volunteering (weekly, log): 0.154(0.056,0.252); on the original scale, 11.264 minutes(3.943,19.158). E‑value bound = 1.287 - log Charity Donate: 0.13(0.045,0.215); on the original scale, 39.935 (12.247,74.883). E‑value bound = 1.25"
  },
  {
    "objectID": "content/discussion-points.html#how-much-better-if-we-treated-by-cate",
    "href": "content/discussion-points.html#how-much-better-if-we-treated-by-cate",
    "title": "Self-Inflicted Injuries",
    "section": "How Much Better if We Treated by CATE?",
    "text": "How Much Better if We Treated by CATE?\n\n\n\n\n\nQini curves load from images/2025_church_combined_qini.png after the heterogeneous treatment analysis scripts run.\n\n\n\nFigure 7: Qini curve comparisons for policy targeting."
  },
  {
    "objectID": "content/discussion-points.html#who-should-we-treat-first-belonging",
    "href": "content/discussion-points.html#who-should-we-treat-first-belonging",
    "title": "Self-Inflicted Injuries",
    "section": "Who Should We Treat First? Belonging",
    "text": "Who Should We Treat First? Belonging\n\n\n\n\n\nPolicy tree plot (belonging) will appear once images/2025_2L_church_on_belonging.png is generated.\n\n\n\nFigure 8: Policy tree: belonging."
  },
  {
    "objectID": "content/discussion-points.html#who-should-we-treat-first-charitable-donations",
    "href": "content/discussion-points.html#who-should-we-treat-first-charitable-donations",
    "title": "Self-Inflicted Injuries",
    "section": "Who Should We Treat First? Charitable Donations",
    "text": "Who Should We Treat First? Charitable Donations\n\n\n\n\n\nPolicy tree plot (charitable donations) loads from images/2025_2L_church_on_charity_donations.png when available.\n\n\n\nFigure 9: Policy tree: charitable donations."
  },
  {
    "objectID": "content/discussion-points.html#closing",
    "href": "content/discussion-points.html#closing",
    "title": "Self-Inflicted Injuries",
    "section": "Closing",
    "text": "Closing\n\n\n\n\n\n\n\n\nError\nNeed\nRemedy\n\n\n\n\nWhat\nControl the right variables\nDraw a DAG first\n\n\nWhen\nStart the clock on time\nEmulate a target trial\n\n\nWho\nTreat the right people\nWeight to target + investigate heterogeneity"
  },
  {
    "objectID": "content/discussion-points.html#thanks",
    "href": "content/discussion-points.html#thanks",
    "title": "Self-Inflicted Injuries",
    "section": "Thanks",
    "text": "Thanks\n\n\nTRT Grant 0418 & Max Planck Institute for Evolutionary Anthropology for support.\nChris G. Sibley (NZAVS lead Investigator) for the heavy data lifting\n77,490 NZAVS participants for their time\n with questions or if you would like to become involved: \n\n\n\n\n\n\nNZAVS"
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#introduction",
    "href": "workshop-scripts/06-interpretation-2.html#introduction",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "Introduction",
    "text": "Introduction\nThe following briefly walks through the results of ATE and CATE estimation and reporting."
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#method---example-report",
    "href": "workshop-scripts/06-interpretation-2.html#method---example-report",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "Method - Example Report",
    "text": "Method - Example Report\n\n\nCode\ncat(boilerplate_generate_text(\n  category = \"method\",\n  global_vars =  global_vars,\n  sections = c(\"statistical_models.grf_short_explanation\"),\n  # global_vars = list(\n  #   name_outcomes_lower = name_outcomes_lower,\n  #   name_exposure_lower = name_exposure_lower,\n  #   name_exposure_capfirst = name_exposure_variable\n  # ),\n  db = unified_db\n))\n\n\nStatistical Estimation\nWe estimate heterogeneous treatment effects with Generalized Random Forests (GRF) (Tibshirani et al. 2024). GRF extends random forests for causal inference by focusing on conditional average treatment effects (CATE). It handles complex interactions and non-linearities without explicit model specification, and it provides ‘honest’ estimates by splitting data between model-fitting and inference. GRF is doubly robust because it remains consistent if either the outcome model or the propensity model is correct. We evaluate policies with the policytree package (Sverdrup et al. 2024; Athey and Wager 2021a) and visualise results with margot (Bulbulia 2024). (Refer to S4 for a detailed explanation of our approach.)\n\nBulbulia, J. A. 2024. Margot: MARGinal Observational Treatment-Effects. https://doi.org/10.5281/zenodo.10907724."
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#results",
    "href": "workshop-scripts/06-interpretation-2.html#results",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "Results",
    "text": "Results\n\nAverage Treatment Effects\n\n\n\nCode\nate_results$plot\n\n\n\n\n\n\n\n\nFigure 1: Average Treatment Effects on Multi-dimensional Wellbeing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nATE\n2.5 %\n97.5 %\nE-Value\nE-Value bound\n\n\n\n\nVolunteering Hours\n0.299\n0.27\n0.328\n1.953\n1.876\n\n\nCharitable Giving\n0.182\n0.163\n0.201\n1.641\n1.589\n\n\n\n\n\n\nTable 1: Average Treatment Effects on Multi-dimensional Wellbeing\n\n\n\n\n\nConfidence intervals were adjusted for multiple comparisons using Bonferroni correction (\\alpha = 0.05). E‑values were also adjusted using Bonferroni correction (\\alpha = 0.05).\nThe following outcomes present reliable causal evidence for average treatment effects (E‑value lower bound &gt; 1.2):\n\nVolunteering Hours: 0.299(0.270,0.328); on the original scale, 0.388(0.350,0.426). E‑value bound = 1.88\nCharitable Giving: 0.182(0.163,0.201); on the original scale, 0.228(0.204,0.252). E‑value bound = 1.59\n\nInterpretation is per the stated policy contrast; estimates reflect \\mathbf{E}[Y(1)] - \\mathbf{E}[Y(0)] on standardized outcome units. For continuous outcomes, E-values use an OLS-to-RR approximation with exposure contrast \\delta = 1 and outcome SD = 1.\n\n\n\n\nHeterogeneous Treatment Effects\nWe begin by examining the distribution of individual treatment effects ($) across our sample. Figure 2 presents the estimated treatment effects for each individual, revealing substantial variability in how people respond to {name_exposure_lower}.\n\n\n\nCode\n# create tau plots showing individual treatment effect distributions\ntau_plots &lt;- margot_plot_tau(\n  models_for_analysis, \n  label_mapping = labels_for_analysis\n)\n\n# display the plot\ntau_plots\n\n\n\n\n\n\n\n\nFigure 2: Distribution of Individual Treatment Effects (τᵢ) Across Outcomes\n\n\n\n\n\n\nThe histograms above show considerable heterogeneity in treatment effects across individuals in the charitable giving condition. To determine whether this variability is systematic (i.e., predictable based on individual characteristics) rather than random noise, we employ two complementary approaches: Qini curves to assess the reliability of heterogeneous effects, and policy trees to identify subgroups with differential treatment responses.\n\nQini Curves\nThe Qini curve shows the cumulative gain as we expand a targeting rule down the CATE ranking.\n\nBeneficial exposure: we add individuals from the top positive CATEs downward; the baseline is ‘expose everyone.’\nDetrimental exposure: we first flip outcome direction (so higher values represent more harm; see ), then add the exposure starting with individuals whose CATEs show the greated harm, gradually including those predicted to be more resistant to harm; the baseline is ‘expose everyone.’ The curve therefore quantifies the harm by when those most suceptible to harm are exposed.\n\nIf the Qini curve stays above its baseline, a targeted policy increases the outcome more than a one-size-fits-all alternative.\n\n\nRATE AUTOC RESULTS\n\nCode\n# only show if there are autoc results\nif (length(model_groups$autoc) &gt; 0) {\n  cat(rate_interp$autoc_results)\n} else {\n  cat(\"No significant RATE AUTOC results were found.\")\n}\n\n\n\n\nEvidence for heterogeneous treatment effects (policy = treat best responders) using AUTOC\nAUTOC uses logarithmic weighting to focus treatment on top responders.\nPositive RATE estimates for: Charitable Giving.\nEstimates (Charitable Giving: 0.259 (95% CI 0.245, 0.273)) show robust heterogeneity.\nNegative RATE estimates for: Volunteering Hours.\nEstimates (Volunteering Hours: -0.156 (95% CI -0.178, -0.134)) caution against CATE prioritisation.\n\n\n\nCode\n# display autoc plots if they exist\nif (length(autoc_plots) &gt; 0) {\n  # create blank plot for spacing\n  blank_plot &lt;- plot_spacer()\n  \n  # determine grid layout (2 columns preferred)\n  n_plots &lt;- length(autoc_plots)\n  n_cols &lt;- 2\n  n_rows &lt;- ceiling(n_plots / n_cols)\n  \n  # create list of plots including blank spacers for even grid\n  plot_list &lt;- autoc_plots\n  n_blanks_needed &lt;- (n_rows * n_cols) - n_plots\n  \n  # add blank plots to fill the grid\n  if (n_blanks_needed &gt; 0) {\n    for (i in 1:n_blanks_needed) {\n      plot_list &lt;- append(plot_list, list(blank_plot))\n    }\n  }\n  \n  # combine plots in a grid\n  combined_autoc &lt;- wrap_plots(\n    plot_list,\n    ncol = n_cols,\n    nrow = n_rows\n  ) +\n    plot_layout(guides = \"collect\") +\n    plot_annotation(\n      title    = \"RATE AUTOC Curves for Heterogeneous Effects\",\n      subtitle = paste(\"Outcomes with significant autocorrelation (n =\", \n                       n_plots, \")\")\n    ) &\n    theme(\n      legend.position   = \"bottom\",\n      plot.title        = element_text(hjust = 0.5),\n      plot.subtitle     = element_text(hjust = 0.5)\n    )\n  \n  print(combined_autoc)\n} else {\n  message(\"no autoc plots to display\")\n}\n\n\n\n\n\n\n\n\nFigure 3: RATE AUTOC Curves\n\n\n\n\n\n\n\nCode\n# only use if you have reliable qini results\nif (length(reliable_ids) &gt; 0) {\n  cat(qini_gain$qini_explanation)\n} else {\n  cat(\"No significant heterogeneous treatment effects were detected using Qini curve analysis.\")\n}\n\nThe QINI curve compares targeted treatment allocation (based on individual treatment effects) versus uniform allocation (based on average treatment effect). Small differences in the expected values of the treatment after the entire population is treated are expected due to out of sample cross-validation (all estimates are tested on data the model has not seen). We computed expected policy effects from prioritising individuals by CATE at 10% and 40% spend levels.\nCharitable Giving At 10% spend: CATE prioritisation is beneficial (diff: 0.05 [95% CI 0.05, 0.06]). At 40% spend: CATE prioritisation is beneficial (diff: 0.12 [95% CI 0.10, 0.13]).\nVolunteering Hours At 10% spend: CATE prioritisation worsens outcomes compared to ATE. At 40% spend: No reliable benefits from CATE prioritisation.\n\n\nCode\n# only use if you have multiple qini results\nif (length(reliable_ids) &gt; 0) {\n  knitr::kable(\n    qini_gain$summary_table |&gt; \n      mutate(across(where(is.numeric), ~ round(., 2))),\n    format = \"markdown\",\n    caption = \"Qini Curve Results\"\n  )\n} else {\n  cat(\"*Note: Qini curve table only displayed when multiple significant results are found.*\")\n}\n\n\n\n\n\n\nModel\nSpend 10%\nSpend 40%\n\n\n\n\nCharitable Giving\n0.05 [0.05, 0.06]\n0.12 [0.10, 0.13]\n\n\nVolunteering Hours\n-0.01 [-0.02, -0.00]\n-0.01 [-0.03, 0.00]\n\n\n\nQini Curve Results\n\n\n\n\n\n\n\n\n\n\nFigure 4: Qini Curves for Heterogeneous Treatment Effects\n\n\n\n\n\n\n\n\nDecision Rules (Who is Most Sensitive to Treatment?)\n\nCode\ncat(\n  boilerplate::boilerplate_generate_text(\n    category     = \"results\",\n    sections     = c(\"grf.interpretation_policy_tree\"),\n    global_vars  = global_vars,\n    db           = unified_db\n  )\n)\n\n\nPolicy Trees\nWe used policy trees (Sverdrup et al. 2024; Athey and Wager 2021b, 2021a) to find straightforward ‘if-then’ rules for who benefits most from treatment, based on participant characteristics. Because we flipped some measures, a higher predicted effect always means greater improvement. Policy trees can uncover small but important subgroups whose treatment responses stand out, even when the overall differences might be modest.\nThe following pages present policy trees for each outcome with reliable heterogeneous effects. Each tree shows: (1) the decision rules for treatment assignment, (2) the distribution of treatment effects across subgroups, and (3) visual representation of how covariates split the population into groups with differential treatment responses.\n\n\n\nCode\n# display policy trees if they exist - one per page\nif (length(policy_plots) &gt; 0) {\n  # iterate through each policy tree\n  for (i in seq_along(policy_plots)) {\n    # add page break before each plot except the first\n    if (i &gt; 1) {\n      cat(\"\\n\\n{{&lt; pagebreak &gt;}}\\n\\n\")\n    }\n    \n    # create individual caption for each tree\n    cat(paste0(\"\\n\\n#### Policy Tree \", i, \": \", qini_names[[i]], \"\\n\\n\"))\n    \n    # print the policy tree\n    print(policy_plots[[i]])\n    \n    # add some space after\n    cat(\"\\n\\n\")\n  }\n} else {\n  message(\"no policy trees to display\")\n}\n\n\n\n\n#### Policy Tree 1: Charitable Giving\n\n\n\n\n\n\n\n\nFigure 5: Policy Trees for Treatment Assignment\n\n\n\n\n\n\n\nCode\n# use this text below your decision tree graphs\nif (length(reliable_ids) &gt; 0) {\n  cat(policy_text, \"\\n\")\n}\n\n\n\n\nPolicy Tree Interpretations (depth 1)\n\nFindings for Charitable Giving at the end of study\nThe policy-tree analysis divides cases on baseline Age. Those who score ≤ -0.726 (original: -0.726) are advised Control. Those above -0.726 (original: -0.726) are advised Treated.\n\n\nTreatment-effect heterogeneity\nThe policy tree produces two terminal leaves. Conditional average treatment effects (CATEs) are estimated within each leaf:\n\nLeaf 1—baseline age ≤ -0.726 (n = 2,357; 23.6% of the test set): 68.1% were recommended control. The mean outcome under control was -0.101, the mean under treatment was 0.090, yielding a CATE of 0.191.\nLeaf 2—baseline age &gt; -0.726 (n = 7,643; 76.4% of the test set): 97.1% were recommended treated. The mean outcome under control was -0.108, the mean under treatment was 0.088, yielding a CATE of 0.196.\n\n\n\nOverall policy performance\nAcross the full test set (N = 10,000), the policy prescribes control for 1,826 participants (18.3%) and treated for 8,174 participants (81.7%).\n\n\n\nWhat happens when you estimate a policy tree when there is no heterogeneity?\n\n\n\nCode\n policy_results_nL &lt;- margot_policy(\n    models_for_analysis,\n    decision_tree_args = decision_tree_defaults,\n    policy_tree_args   = policy_tree_defaults,\n    model_names        = names(models_for_analysis$full_models),\n    max_depth          = 1L,\n    original_df        = original_df,\n    label_mapping      = labels_for_analysis,\n    output_objects     = c(\"combined_plot\")\n  )\n \n policy_results_nL[[2]]\n\n\n$combined_plot\n\n\n\n\n\n\n\n\nFigure 6: Policy Tree: No Variability in Effect: 1L Trees\n\n\n\n\n\n\n\n\nExamples of 2L Policy Trees\n\nTreatment Variation: 2L Policy Tree\n\n\n\nCode\n policy_results_2LL &lt;- margot_policy(\n    models_for_analysis,\n    decision_tree_args = decision_tree_defaults,\n    policy_tree_args   = policy_tree_defaults,\n    model_names        = names(models_for_analysis$full_models),\n    max_depth          = 2L,\n    original_df        = original_df,\n    label_mapping      = labels_for_analysis,\n    output_objects     = c(\"combined_plot\")\n  )\n \n policy_results_2LL[[1]]\n\n\n$combined_plot\n\n\n\n\n\n\n\n\nFigure 7: Policy Tree: Variability in Effect: 2L Tree\n\n\n\n\n\n\n\n\nNo Treatment Variation: 2L Policy Tree\nAs indicated in Figure 8 there is no genuine treatment variation. Prediction are at the margins of the population, reflecting noise in the data generating process.\n\n\n\nCode\n policy_results_2LL[[2]]\n\n\n$combined_plot\n\n\n\n\n\n\n\n\nFigure 8: Policy Tree: No Variability in Effect: 2L Trees"
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#discussion",
    "href": "workshop-scripts/06-interpretation-2.html#discussion",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#appendix-explain-grf",
    "href": "workshop-scripts/06-interpretation-2.html#appendix-explain-grf",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "S1: Estimating and Interpreting Heterogeneous Treatment Effects with GRF",
    "text": "S1: Estimating and Interpreting Heterogeneous Treatment Effects with GRF\n\nQini Curves\nThe Qini curve shows the cumulative gain as we expand a targeting rule down the CATE ranking.\n\nBeneficial exposure: we add individuals from the top positive CATEs downward; the baseline is ‘expose everyone.’\nDetrimental exposure: we first flip outcome direction (so higher values represent more harm; see ), then add the exposure starting with individuals whose CATEs show the greated harm, gradually including those predicted to be more resistant to harm; the baseline is ‘expose everyone.’ The curve therefore quantifies the harm by when those most suceptible to harm are exposed."
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#appendix-strengths",
    "href": "workshop-scripts/06-interpretation-2.html#appendix-strengths",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "S2: Strengths and Limitations of Causal Forests",
    "text": "S2: Strengths and Limitations of Causal Forests\n\nCode\ncat(\n  boilerplate::boilerplate_generate_text(\n    category     = \"discussion\",\n    sections     = c(\"strengths.strengths_grf_short\"),\n    global_vars  = global_vars,\n    db           = unified_db\n  )\n)\n\nWe used causal forests to uncover how the treatment effect changes across people who differ in age, gender, baseline scores, and other measured characteristics (Tibshirani et al. 2024). This flexible, non-parametric method avoids the rigid functional‐form assumptions of linear or logistic regression and can capture complex, higher-order interactions that would otherwise be missed.\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\nAthey, Susan, and Stefan Wager. 2021b. “Policy Learning With Observational Data.” Econometrica 89 (1): 133–61. https://doi.org/10.3982/ECTA15732.\n\n———. 2021a. “Policy Learning with Observational Data.” Econometrica 89 (1): 133–61. https://doi.org/https://doi.org/10.3982/ECTA15732.\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42. https://doi.org/10.1080/01621459.2017.1319839.\nTo guard against over-fitting we split the data: one portion trained the forest, and the other evaluated its predictions train/test ratio: 50/50. On the evaluation set we computed three complementary metrics: (i) rate statistics: the area under the treatment–outcome curve (AUTOC) which summarise how well the model ranks individuals from ‘most likely to benefit’ to ‘least likely to benefit’ based on their baseline caracteristics; (ii) Qini curves which show the cumulative gain achieved by treating successively larger fractions of the population – which is relevant to understanding gains from different spend levels. policy trees, which convert the forest’s complex predictions into a short set of human-readable if–then rules that can guide targeting in practice (Sverdrup et al. 2024; Athey and Wager 2021b, 2021a). Collectively, these tools help to clarify whether heterogeneous effects exist, and also how much extra benefit a data-driven targeting policy might yield over random allocation (Wager and Athey 2018).\nAlthough causal forests improve on traditional parametric methods, every observational approach carries risks.\nFirst, causal inference in observational settings inevitably relies on untestable ignorability assumptions (treatments are ‘as good as random’ conditional on measured covariates). Whether we have measured all factors that may jointly influence treatment assignment and the outcomes cannot be evaluated by statistical tests. If strong common causes of both the exposure and outcome are unobserved or poorly measured, our estimates will be biased. Interpreting subgroup findings can also be challenging: statistically significant differences are not always large enough to matter in real life. More basic, perhaps, methods for detecting real differences rely on measures that are, in survey research, inherently noisy, and noise often, but no always, attenuates affects. The twin dangers of mistaking noise for signal, or of obscuring singles from noise, abide. Such limitations must be kept firmly in mind when interpreting results."
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html#references",
    "href": "workshop-scripts/06-interpretation-2.html#references",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/03-interaction.html",
    "href": "content/03-interaction.html",
    "title": "Heterogeneity",
    "section": "",
    "text": "Note\n\n\n\nRequired Reading\n\n(Hernan and Robins 2020) Chapters 4-5 link\n\nOptional Reading\n\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link"
  },
  {
    "objectID": "content/03-interaction.html#hte-cate-and-estimated-hattaux",
    "href": "content/03-interaction.html#hte-cate-and-estimated-hattaux",
    "title": "Heterogeneity",
    "section": "HTE, CATE, and estimated \\hat{\\tau}(x)",
    "text": "HTE, CATE, and estimated \\hat{\\tau}(x)\n\nHTE: The fact that \\tau(x) varies with x.\nCATE, \\tau(x): The true subgroup-average effect.\nEstimated CATE, \\hat{\\tau}(x): The model-based prediction for a subgroup with features x.\n\nFor unit i with profile X_i = x:\n\nUnobservable individual effect: Y_i(1) - Y_i(0).\n\\hat{\\tau}(x): Our estimate of \\tau(x), the average effect for all units with profile x.\n\nHigh-dimensional X makes manual interaction modeling impractical. Modern ML methods such as causal forests (grf in R) (Tibshirani et al. 2024) are designed to estimate \\hat{\\tau}(x) flexibly.\n\n\n\n\n\n\n\n\n\n\nConcept\nNotation\nDefinition\nScope\nRequirements\n\n\n\n\nInteraction\n–\nJoint effect of multiple interventions compared with the sum of their separate effects\nMultiple interventions (A, B)\nAdjust for all confounders of A \\to Y and B \\to Y (L \\coprod Q)\n\n\nEffect modification\n\\tau(x) varies with x\nEffect of a single intervention (A) differs by subgroup defined by X = x\nSingle intervention\nAdjust for confounders of A \\to Y within each subgroup\n\n\nEstimated CATE\n\\hat{\\tau}(x)\nModel-based estimate of \\tau(x) for X = x\nPrediction task\nFlexible estimation methods (e.g., causal forests)"
  },
  {
    "objectID": "content/03-interaction.html#appendix-identification-of-interaction-effects",
    "href": "content/03-interaction.html#appendix-identification-of-interaction-effects",
    "title": "Heterogeneity",
    "section": "Appendix: Identification of Interaction Effects",
    "text": "Appendix: Identification of Interaction Effects\n\n\n\n\n\n\n\n\nFigure 1: Diagram illustrating causal interaction. Assessing the joint effect of two interventions, A (e.g., teaching method) and B (e.g., tutoring), on outcome Y (e.g., test score). L represents confounders of the A-Y relationship, and Q represents confounders of the B-Y relationship. Red arrows indicate biasing backdoor paths requiring adjustment. Assumes A and B are decided independently here.\n\n\n\n\n\nFigure 2 shows we need to condition on (adjust for) both L_0 and Q_0.\n\n\n\n\n\n\n\n\nFigure 2: Identification of causal interaction requires adjusting for all confounders of A-Y (L) and B-Y (Q). Boxes around L and Q indicate conditioning, closing backdoor paths.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: How shall we investigate effect modification of A on Y by G? Can you see the problem?\n\n\n\n\n\nThus ,it is essential to understand that when we control for confounding along the the A \\to Y path, we do not identify the causal effects of effect-modifiers. Rather, we should consider effect-modifiers prognostic indicators. Moreover, we’re going to need to develop methods for clarifying prognostic indicators in multi-dimensional settings where"
  },
  {
    "objectID": "content/02-causal-workflow.html",
    "href": "content/02-causal-workflow.html",
    "title": "The Causal Workflow",
    "section": "",
    "text": "NoteReadings for Workshop\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html"
  },
  {
    "objectID": "content/02-causal-workflow.html#projects-with-applied-interests",
    "href": "content/02-causal-workflow.html#projects-with-applied-interests",
    "title": "Reporting Guide",
    "section": "Projects with Applied Interests",
    "text": "Projects with Applied Interests\n\nTranslate effects into absolute, population-level impact.\n\nReport absolute risk differences for the target population. Always show the baseline so deltas are interpretable.\n\nShow heterogeneity and targetability.\n\nWhere possible, identify who is affected, who is unaffected, and who may be harmed. If appropriate, provide a simple, auditable policy rule for targeting (and a plain-language rationale).\n\nExpress uncertainty in decision terms.\n\nGo beyond confidence intervals: report probabilities that an option is optimal, expected net benefit, expected regret, and when helpful the value of additional information. Use simulation to make uncertainty tangible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackages\n\n\nCode\nreport::cite_packages()\n\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "simulations/01-simulation-population.html",
    "href": "simulations/01-simulation-population.html",
    "title": "Simulation: Population Estimate",
    "section": "",
    "text": "S2 and S3 are from the supplement to Bulbulia (2024b). S2 is a simulation. It illustrates how, if the distribution of effect-modifiers in one’s sample differs from that of the target population, the marginal effect estimate (ATE) will be biased. S3 offers a mathmatical explanation for this result. These simulations are followed by others that clarify how inference may fail when the true timing of events in one’s measures differs from what is assumed."
  },
  {
    "objectID": "simulations/01-simulation-population.html#simulation-of-population-ate",
    "href": "simulations/01-simulation-population.html#simulation-of-population-ate",
    "title": "Simulation: Population Estimate",
    "section": "",
    "text": "Data Generation: we simulate a dataset for 1,000 individuals, where e.g. degree of religious belief (A) influences wealth (L), which in turn affects charitable donations (Y$). The simulation is based on predefined parameters that establish L as a mediator between A and Y.\nParameter Definitions:\n\nThe probability of access to green space (A) is set at 0.5.\nThe effect of A on L (exercise) is given by \\beta = 2.\nThe effect of L on Y (happiness) is given by \\delta = 1.5.\nStandard deviations for L and Y are set at 1 and 1.5, respectively.\n\nModel 1 (Correct Assumption): fits a linear regression model assuming L as a mediator, including both A and L as regressors on Y. This model aligns with the data-generating process, and, by the rules of d-separation, induces mediator bias for the A\\to Y path.\nModel 2 (Incorrect Assumption): fits a linear regression model including only A as a regressor on Y, omitting the mediator L. This model assesses the direct effect of A on Y without accounting for mediation.\nAnalysis: We compares the estimated effects of A on Y under each model specification.\n\n# load libraries\n!require(kableExtra)){install.packages(\"kableExtra\")} # tables\nif(!require(gtsummary)){install.packages(\"gtsummary\")} # tables\n\n# simulation seed\nset.seed(123) #  reproducibility\n\n# define the parameters \nn = 1000 # Number of observations\np = 0.5  # Probability of A = 1 (exposure)\nalpha = 0 # Intercept for L (mediator)\nbeta = 2  # Effect of A on L \ngamma = 1 # Intercept for Y \ndelta = 1.5 # Effect of L on Y\nsigma_L = 1 # Standard deviation of L\nsigma_Y = 1.5 # Standard deviation of Y\n\n# simulate the data: fully mediated effect by L\nA = rbinom(n, 1, p) # binary exposure variable\nL = alpha + beta*A + rnorm(n, 0, sigma_L) # mediator L affect by A\nY = gamma + delta*L + rnorm(n, 0, sigma_Y) # Y affected only by L,\n\n# make the data frame\ndata = data.frame(A = A, L = L, Y = Y)\n\n# fit regression in which we control for L, a mediator\n# (cross-sectional data is consistent with this model)\nfit_1 &lt;- lm( Y ~ A + L, data = data)\n\n# fit regression in which L is assumed to be a mediator, not a confounder.\n# (cross-sectional data is also consistent with this model)\nfit_2 &lt;- lm( Y ~ A, data = data)\n\n# create gtsummary tables for each regression model\ntable1 &lt;- gtsummary::tbl_regression(fit_1)\ntable2 &lt;- gtsummary::tbl_regression(fit_2)\n\n# merge the tables for comparison\ntable_comparison &lt;- gtsummary::tbl_merge(\n  list(table1, table2),\n  tab_spanner = c(\"Model: Exercise assumed confounder\", \n                  \"Model: Exercise assumed to be a mediator\")\n)\n# make html table (for publication)\nmarkdown_table_0 &lt;- as_kable_extra(table_comparison, \n                                   format = \"html\")\n# print table (note, you might prefer \"markdown\" or another format)                                \nmarkdown_table_0\n\nThe following code is designed to estimate the Average Treatment Effect (ATE) using the clarify package in R, which is referenced here as (Greifer et al. 2023). The procedure involves two steps: simulating coefficient distributions for regression models and then calculating the ATE based on these simulations. This process is applied to two distinct models to demonstrate the effects of including versus excluding a mediator variable in the analysis.\n\n\n\n\nLoad the clarify Package: this package provides functions to simulate regression coefficients and compute average marginal effects (AME), robustly facilitating the estimation of ATE.\nSet seed: set.seed(123) ensures that the results of the simulations are reproducible, allowing for consistent outcomes across different code runs.\nSimulate the data distribution:\nsim_coefs_fit_1 and sim_coefs_fit_2 are generated using the sim function from the clarify package, applied to two fitted models (fit_1 and fit_2). These functions simulate the distribution of coefficients based on the specified models, capturing the uncertainty around the estimated parameters.\nCalculate ATE:\n\nFor both models, the sim_ame function calculates the ATE as the marginal risk difference (RD) when the treatment variable (A) is present (A == 1). This function uses the simulated coefficients to estimate the treatment effect across the simulated distributions, providing a comprehensive view of the ATE under each model.\nTo streamline the output, the function is set to verbose mode off (verbose = FALSE).\n\nResults:\n\nSummaries of these estimates (summary_sim_est_fit_1 and summary_sim_est_fit_2) are obtained, providing detailed statistics including the estimated ATE and its 95% confidence intervals (CI).\n\nPresentation: report ATE and CIs:\n\nUsing the glue package, the ATE and its 95% CIs for both models are formatted into a string for easy reporting. This step transforms the statistical output into a more interpretable form, highlighting the estimated treatment effect and its precision.\n\n# use `clarify` package to obtain ATE\nif(!require(clarify)){install.packages(\"clarify\")} # clarify package\n# simulate fit 1 ATE\nset.seed(2025)\nsim_coefs_fit_1 &lt;- sim(fit_1)\nsim_coefs_fit_2 &lt;- sim(fit_2)\n\n# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)\nsim_est_fit_1 &lt;-\n  sim_ame(\n    sim_coefs_fit_1,\n    var = \"A\",\n    subset = A == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)\nsim_est_fit_2 &lt;-\n  sim_ame(\n    sim_coefs_fit_2,\n    var = \"A\",\n    subset = A == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# obtain summaries\nsummary_sim_est_fit_1 &lt;- summary(sim_est_fit_1, null = c(`RD` = 0))\nsummary_sim_est_fit_2 &lt;- summary(sim_est_fit_2, null = c(`RD` = 0))\n\n# reporting \n# ate for fit 1, with 95% CI\nATE_fit_1 &lt;- glue::glue(\n  \"ATE =\n                        {round(summary_sim_est_fit_1[3, 1], 2)},\n                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},\n                        {round(summary_sim_est_fit_1[3, 3], 2)}]\"\n)\n# ate for fit 2, with 95% CI\nATE_fit_2 &lt;-\n  glue::glue(\n    \"ATE = {round(summary_sim_est_fit_2[3, 1], 2)},\n                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},\n                        {round(summary_sim_est_fit_2[3, 3], 2)}]\"\n  )\n\n\n\n\n\nModel 1 (L as a Confounder): this analysis assumes that L is a confounder in the relationship between the treatment (A) and the outcome (Y), and thus, it includes L in the model. The ATE estimated here reflects the effect of A while controlling for L.\nModel 2 (L as a Mediator): in contrast, this analysis considers L to be a mediator, and the model either includes L explicitly in its estimation process or excludes it to examine the direct effect of A on Y. The approach to mediation analysis here is crucial as it influences the interpretation of the ATE.\n\nBy comparing the ATEs from both models, researchers can understand the effect of mediation (or the lack thereof) on the estimated treatment effect. This comparison sheds light on how assumptions about variable roles (confounder vs. mediator) can significantly alter causal inferences drawn from cross-sectional data.\nWherever it is uncertain whether a variable is a confounder or a mediator, we suggest creating two causal diagrams and reporting both analyses."
  },
  {
    "objectID": "simulations/01-simulation-population.html#appendix-d",
    "href": "simulations/01-simulation-population.html#appendix-d",
    "title": "Simulation: Population Estimate",
    "section": "Appendix D: Simulation of Different Confounding Control Strategies",
    "text": "Appendix D: Simulation of Different Confounding Control Strategies\nThis appendix outlines the methodology and results of a data simulation designed to compare different strategies for controlling confounding in the context of environmental psychology research. Specifically, the simulation examines the effect of access to open green spaces (treatment, A_1) on happiness (outcome, Y_2) while addressing the challenge of unmeasured confounding. The simulation incorporates baseline measures of exposure and outcome (A_0, Y_0), baseline confounders (L_0), and an unmeasured confounder (U) to evaluate the effectiveness of different analytical approaches.\n\nMethodology\n1.Load Libraries kableExtra, gtsummary, and grf.\n\nTarget: we simulate data for 10,000 individuals, including baseline exposure to green spaces (A_0), baseline happiness (Y_0), baseline confounders (L_0), and an unmeasured confounder (U). The simulation uses a logistic model for treatment assignment and a linear model for the continuous outcome, incorporating interactions to assess how baseline characteristics modify the treatment effect.\nSet seed and simulate the data distribution:\n\nTreatment assignment coefficients: \\beta_{A0} = 0.25, \\beta_{Y0} = 0.3, \\beta_{L0} = 0.2, and \\beta_{U} = 0.1. Outcome model coefficients: \\delta_{A1} = 0.3, \\delta_{Y0} = 0.9, \\delta_{A0} = 0.1, \\delta_{L0} = 0.3, with an interaction effect (\\theta_{A0Y0L0} = 0.5) indicating the combined influence of baseline exposure, outcome, and confounders on the follow-up outcome.\n\nModel comparison:\n\nNo control model: estimates the effect of A_1 on Y_2 without controlling for any confounders.\nStandard covariate control model: controls for baseline confounders (L_0) alongside treatment (A_1).\nBaseline exposure and outcome model: extends the standard model by including baseline treatment and outcome (A_0, Y_0) and their interaction with L_0.\n\nResults: each model’s effectiveness in estimating the true treatment effect is assessed by comparing regression outputs. The simulation evaluates how well each model addresses the bias introduced by unmeasured confounding and the role of baseline characteristics in modifying treatment effects.\nPresentation: the results are synthesised in a comparative table, formatted using the kableExtra {Zhu (2021)] and gtsummary packages (Sjoberg et al. 2021), highlighting the estimated treatment effects and their statistical significance across models.\n\nOverall, we use the simulation to illustrate the importance of incorporating baseline characteristics and their interactions to mitigate the influence of unmeasured confounding.\nHere is the simulation/model code:\n\nlibrary(kableExtra)\nif(!require(kableExtra)){install.packages(\"kableExtra\")} # causal forest\nif(!require(gtsummary)){install.packages(\"gtsummary\")} # causal forest\n\nLoading required package: gtsummary\n\nif(!require(grf)){install.packages(\"grf\")} # causal forest\n\nLoading required package: grf\n\n\n\nAttaching package: 'grf'\n\n\nThe following object is masked from 'package:parameters':\n\n    get_scores\n\n# r_texmf()eproducibility\nset.seed(123) \n\n# set number of observations\nn &lt;- 10000 \n\n# baseline covariates\nU &lt;- rnorm(n) # Unmeasured confounder\nA_0 &lt;- rbinom(n, 1, prob = plogis(U)) # Baseline exposure\nY_0 &lt;- rnorm(n, mean = U, sd = 1) # Baseline outcome\nL_0 &lt;- rnorm(n, mean = U, sd = 1) # Baseline confounders\n\n# coefficients for treatment assignment\nbeta_A0 = 0.25\nbeta_Y0 = 0.3\nbeta_L0 = 0.2\nbeta_U = 0.1\n\n# simulate treatment assignment\nA_1 &lt;- rbinom(n, 1, prob = plogis(-0.5 + \n                                    beta_A0 * A_0 +\n                                    beta_Y0 * Y_0 + \n                                    beta_L0 * L_0 + \n                                    beta_U * U))\n# coefficients for continuous outcome\ndelta_A1 = 0.3\ndelta_Y0 = 0.9\ndelta_A0 = 0.1\ndelta_L0 = 0.3\ntheta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0\ndelta_U = 0.05\n# simulate continuous outcome including interaction\nY_2 &lt;- rnorm(n,\n             mean = 0 +\n               delta_A1 * A_1 + \n               delta_Y0 * Y_0 + \n               delta_A0 * A_0 + \n               delta_L0 * L_0 + \n               theta_A0Y0L0 * Y_0 * \n               A_0 * L_0 + \n               delta_U * U,\n             sd = .5)\n# assemble data frame\ndata &lt;- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)\n\n# model: no control\nfit_no_control &lt;- lm(Y_2 ~ A_1, data = data)\n\n# model: standard covariate control\nfit_standard &lt;- lm(Y_2 ~ A_1 + L_0, data = data)\n\n# model: interaction with baseline confounders, and baseline outcome and exposure\nfit_interaction  &lt;- lm(Y_2 ~ A_1 * (L_0 + A_0 + Y_0), data = data)\n\n# create gtsummary tables for each regression model\ntbl_fit_no_control&lt;- tbl_regression(fit_no_control)  \ntbl_fit_standard &lt;- tbl_regression(fit_standard)\ntbl_fit_interaction &lt;- tbl_regression(fit_interaction)\n\n# get only the treatment variable\ntbl_list_modified &lt;- lapply(list(\n  tbl_fit_no_control,\n  tbl_fit_standard,\n  tbl_fit_interaction),\nfunction(tbl) {\n  tbl %&gt;%\n    modify_table_body(~ .x %&gt;% dplyr::filter(variable == \"A_1\"))\n})\n# merge tables\ntable_comparison &lt;- tbl_merge(\n  tbls = tbl_list_modified,\n  tab_spanner = c(\n    \"No Control\",\n    \"Standard\",\n    \"Interaction\")\n) |&gt;\n  modify_table_styling(\n    column = c(p.value_1, p.value_2, p.value_3),\n    hide = TRUE\n  )\n# markdown table for publication\nmarkdown_table &lt;-\n  as_kable_extra(table_comparison, format = \"markdown\")\nprint(markdown_table)\n\n&lt;table style=\"NAborder-bottom: 0;\"&gt;\n &lt;thead&gt;\n&lt;tr&gt;\n&lt;th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"&gt;&lt;/th&gt;\n&lt;th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"&gt;&lt;div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \"&gt;No Control&lt;/div&gt;&lt;/th&gt;\n&lt;th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"&gt;&lt;div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \"&gt;Standard&lt;/div&gt;&lt;/th&gt;\n&lt;th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"&gt;&lt;div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \"&gt;Interaction&lt;/div&gt;&lt;/th&gt;\n&lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt; Characteristic &lt;/th&gt;\n   &lt;th style=\"text-align:center;\"&gt; Beta &lt;/th&gt;\n   &lt;th style=\"text-align:center;\"&gt; 95% CI &lt;/th&gt;\n   &lt;th style=\"text-align:center;\"&gt; Beta &lt;/th&gt;\n   &lt;th style=\"text-align:center;\"&gt; 95% CI &lt;/th&gt;\n   &lt;th style=\"text-align:center;\"&gt; Beta &lt;/th&gt;\n   &lt;th style=\"text-align:center;\"&gt; 95% CI &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;\"&gt; A_1 &lt;/td&gt;\n   &lt;td style=\"text-align:center;\"&gt; 1.5 &lt;/td&gt;\n   &lt;td style=\"text-align:center;\"&gt; 1.5, 1.6 &lt;/td&gt;\n   &lt;td style=\"text-align:center;\"&gt; 0.86 &lt;/td&gt;\n   &lt;td style=\"text-align:center;\"&gt; 0.80, 0.93 &lt;/td&gt;\n   &lt;td style=\"text-align:center;\"&gt; 0.25 &lt;/td&gt;\n   &lt;td style=\"text-align:center;\"&gt; 0.20, 0.31 &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;tfoot&gt;&lt;tr&gt;&lt;td style=\"padding: 0; \" colspan=\"100%\"&gt;\n&lt;sup&gt;&lt;/sup&gt; Abbreviation: CI = Confidence Interval&lt;/td&gt;&lt;/tr&gt;&lt;/tfoot&gt;\n&lt;/table&gt;\n\n\nNext, in the following code, we calculate the Average Treatment Effect (ATE) using simulation-based approaches for two distinct models: one with standard covariate control and another incorporating interaction. This approach leverages the clarify package in R, which facilitates the simulation and interpretation of estimated coefficients from linear models to derive ATEs under different modelling assumptions (Greifer et al. 2023).\nFirst, we use the sim function from the clarify package to generate simulated coefficient distributions for the standard model (fit_standard) and the interaction model (fit_interaction). This step is crucial for capturing the uncertainty in our estimates arising from sampling variability.\nNext, we employ each model’s sim_ame function to compute the average marginal effects (AME), focusing on the treatment variable (A_1). The calculation is done under the assumption that all individuals are treated (i.e., A_1 == 1), and we specify the contrast type as “RD” (Risk Difference) to directly obtain the ATE (Average Treatment Effect). The sim_ame function simulates the treatment effect across the distribution of simulated coefficients, providing a robust estimate of the ATE and its variability.\nThe summaries of these simulations (summary_sim_est_fit_std and summary_sim_est_fit_int) are then extracted to provide concise estimates of the ATE along with 95% confidence intervals (CIs) for both the standard and interaction models. This step is essential for understanding the magnitude and precision of the treatment effects estimated by the models.\nFinally, we use the glue package to format these estimates into a human-readable form, presenting the ATE and its corresponding 95% CIs for each model. This presentation facilitates clear communication of the estimated treatment effects, allowing for direct comparison between the models and highlighting the effect of including baseline characteristics and their interactions on estimating the ATE (Hester and Bryan 2022).\nThis simulation-based approach to estimating the ATE underscores the importance of considering model complexity and the roles of confounders and mediators in causal inference analyses. By comparing the ATE estimates from different models, we can assess the sensitivity of our causal conclusions to various assumptions and modelling strategies.\n\n# use `clarify` package to obtain ATE\nif(!require(clarify)){install.packages(\"clarify\")} # clarify package\n\nLoading required package: clarify\n\n# simulate fit 1 ATE\nset.seed(123)\nsim_coefs_fit_no_control&lt;- sim(fit_no_control)  \nsim_coefs_fit_std &lt;- sim(fit_standard)\nsim_coefs_fit_int &lt;- sim(fit_interaction)\n\n# marginal risk difference ATE, no controls\nsim_est_fit_no_control &lt;-\n  sim_ame(\n    sim_coefs_fit_no_control,\n    var = \"A_1\",\n    subset = A_1 == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)\nsim_est_fit_std &lt;-\n  sim_ame(\n    sim_coefs_fit_std,\n    var = \"A_1\",\n    subset = A_1 == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)\nsim_est_fit_int &lt;-\n  sim_ame(\n    sim_coefs_fit_int,\n    var = \"A_1\",\n    subset = A_1 == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# obtain summaries\nsummary_sim_coefs_fit_no_control &lt;-\n  summary(sim_est_fit_no_control, null = c(`RD` = 0))\nsummary_sim_est_fit_std &lt;-\n  summary(sim_est_fit_std, null = c(`RD` = 0))\nsummary_sim_est_fit_int &lt;-\n  summary(sim_est_fit_int, null = c(`RD` = 0))\n\n# get coefficients for reporting\n# ate for fit 1, with 95% CI\nATE_fit_no_control  &lt;- glue::glue(\n  \"ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, \n  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},\n  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]\"\n)\n# ate for fit 2, with 95% CI\nATE_fit_std &lt;- glue::glue(\n  \"ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, \n  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},\n  {round(summary_sim_est_fit_std[3, 3], 2)}]\"\n)\n# ate for fit 3, with 95% CI\nATE_fit_int &lt;-\n  glue::glue(\n    \"ATE = {round(summary_sim_est_fit_int[3, 1], 2)},\n    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},\n    {round(summary_sim_est_fit_int[3, 3], 2)}]\"\n  )\n# coefs they used in the manuscript\n\nUsing the clarify package, we infer the ATE for the standard model is ATE = 0.86, CI = [0.8, 0.92].\nUsing the clarify package, we infer the ATE for the model that conditions on the baseline exposure and baseline outcome to be: ATE = 0.43, CI = [0.39, 0.47], which is close to the values supplied to the data-generating mechanism.\nTake-home message:\nThe baseline exposure and baseline outcome are often the most important variables to include for confounding control. The baseline exposure also allows us to estimate an incident-exposure effect. For this reason, we should endeavour to obtain at least three waves of data such that these variables and other baseline confounders are included at time 0, the exposure is included at time 1, and the outcome is included at time 2."
  },
  {
    "objectID": "simulations/01-simulation-population.html#appendix-causal-forests",
    "href": "simulations/01-simulation-population.html#appendix-causal-forests",
    "title": "Simulation: Population Estimate",
    "section": "Appendix E: Non-parametric Estimation of Average Treatment Effects Using Causal Forests",
    "text": "Appendix E: Non-parametric Estimation of Average Treatment Effects Using Causal Forests\nThis appendix provides a practical example of estimating average treatment effects (ATE) using a non-parametric approach, specifically applying causal forests. Unlike traditional regression models, causal forests allow for estimating treatment effects without imposing strict assumptions about the form of the relationship between treatment, covariates, and outcomes. This flexibility makes them particularly useful for analysing complex datasets where the treatment effect may vary across observations.\n\nCausal Forest Model Implementation\n\nLibraries: the implementation begins with loading the necessary R libraries: grf for estimating conditional and average treatment effects using causal forests and glue for formatting the results for reporting.\n\nData generation: the code assumes the presence of a data frame data generated from the previous code snippet containing the variables:\n\n\nA_1: Treatment indicator.\nL_0: A covariate.\nY_2: Outcome of interest.\nA_0 and Y_0: Baseline exposure and outcome, respectively.\n\nTreatment (W) and outcome (Y) vectors are extracted from data alongside a matrix X that includes covariates and baseline characteristics.\nCausal Forest model: a causal forest model is fitted using the causal_forest function from the grf package (Tibshirani et al. 2024). This function takes the covariate matrix X, the outcome vector Y, and the treatment vector W as inputs, and it returns a model object that can be used for further analysis.\nAverage Treatment Effect estimation: the average_treatment_effect function computes the ATE from the fitted causal forest model. This step is crucial as it quantifies the overall effect of the treatment across the population, adjusting for covariates included in the model.\nReporting: The estimated ATE and its standard error (se) are extracted and formatted for reporting using the glue package (Hester and Bryan 2022). This facilitates clear communication of the results, showing the estimated effect size and its uncertainty.\n\n\n\nKey Takeaways\nFirst, causal forests offer a robust way to estimate treatment effects without making parametric solid assumptions. This approach is particularly advantageous in settings where the treatment effect may vary with covariates or across different subpopulations.\nSecond, the model estimates the ATE as the difference in expected outcomes between treated and untreated units, averaged across the population. This estimate reflects the overall effect of the treatment, accounting for the distribution of covariates in the sample.\nThird, we find that the estimated ATE by the causal forest model converges to the actual value used in the data-generating process (assumed to be 0.3). This demonstrates the effectiveness of causal forests in uncovering the true treatment effect from complex data.\nThis example underscores the utility of semi-parametric and non-parametric methods, such as causal forests, in causal inference analyses.\n\n# load causal forest library \nlibrary(grf) # estimate conditional and average treatment effects\nlibrary(glue) # reporting \n\n#  'data' is our data frame with columns 'A_1' for treatment, 'L_0' for a covariate, and 'Y_2' for the outcome\n#  we also have the baseline exposure 'A_0' and 'Y_0'\n#  ensure W (treatment) and Y (outcome) are vectors\nW &lt;- as.matrix(data$A_1)  # Treatment\nY &lt;- as.matrix(data$Y_2)  # Outcome\nX &lt;- as.matrix(data[, c(\"L_0\", \"A_0\", \"Y_0\")])\n\n# fit causal forest model \nfit_causal_forest &lt;- causal_forest(X, Y, W)\n\n# estimate the average treatment effect (ATE)\nate &lt;- average_treatment_effect(fit_causal_forest)\n\n# make data frame for reporting using \"glue' \nate&lt;- data.frame(ate)\n\n# obtain ate for report\nATE_fit_causal_forest &lt;-\n  glue::glue(\n    \"ATE = {round(ate[1, 1], 2)}, se = {round(ate[2, 1], 2)}\"\n  )\n\nCausal forest estimates the average treatment effect as ATE = 0.3, se = 0.01. This approach converges to the true value supplied to the generating mechanism of 0.3"
  },
  {
    "objectID": "glossary-and-dags.html",
    "href": "glossary-and-dags.html",
    "title": "Glossary and Causal DAGs",
    "section": "",
    "text": "Comprehensive collection of terminology and causal diagrams for workshop participants"
  },
  {
    "objectID": "glossary-and-dags.html#causal-inference-glossary",
    "href": "glossary-and-dags.html#causal-inference-glossary",
    "title": "Glossary and Causal DAGs",
    "section": "Causal Inference Glossary",
    "text": "Causal Inference Glossary\n\n📖 Complete Terminology Guide\nAccess the comprehensive glossary of causal inference terms and definitions used throughout the workshop.\nDownload Glossary (PDF)\nThis glossary contains essential definitions for terms including: - Average Treatment Effect (ATE), Conditional Average Treatment Effect (CATE) - Backdoor paths, confounders, and colliders - Instrumental variables and propensity scores - Time-varying confounding and measurement error - Causal estimands, estimators, and identification strategies\nEssential reference for understanding causal inference methodology and terminology."
  },
  {
    "objectID": "glossary-and-dags.html#causal-dags-reference-collection",
    "href": "glossary-and-dags.html#causal-dags-reference-collection",
    "title": "Glossary and Causal DAGs",
    "section": "Causal DAGs Reference Collection",
    "text": "Causal DAGs Reference Collection\n\nCore Terminology & Foundations\n\n1. Foundational Concepts\n1a. Local Conventions\nEssential local conventions for causal diagram construction and interpretation\n1b. Directed Graph Terminology\nCore terminology specific to directed acyclic graphs (DAGs)\nS1. Graphical Key\nVisual reference guide for interpreting causal diagram symbols and notation\n\n\n\nCommon Questions & Applications\n\n2. Practical Applications\n2. Common Causal Questions\nFrequently encountered causal questions and how to approach them\n6. Effect Modification\nUnderstanding when and how treatment effects vary across subgroups\n9. External Validity\nApproaches to generalising causal findings across populations and contexts\n\n\n\nTime Series & Confounding\n\n3. Keeping Time on Your Side\n3. Time Series Approaches\nHow longitudinal data help address confounding bias\n4. Three-Wave Panel Methods\nUsing three-wave panel data for causal inference\n5. Time Series Limitations\nWhen time series approaches may not resolve confounding\nS3. Time-Resolved Confounding\nAdvanced approaches to time-varying confounding\n\n\n\nAdvanced Topics\n\n4. Complex Methodological Issues\n7. Selection Bias Focus\nDetailed examination of selection bias in longitudinal studies\n8. Measurement Error\nStructural approaches to representing and addressing measurement error\n10. Experimental Design\nHow experiments address confounding and selection bias challenges\n\n\n\nSupplementary Materials\n\n5. Additional Resources\nS5. Timing Examples\nPractical examples of confounding and timing issues\nS6. Detailed Panel Examples\nWhat can go wrong in a three-wave panel.\nS7. Cross-Sectional Approaches When to report multiple DAGs in cross-sectional studies\nS8. Bias Correction\nQuantitative approaches to bias correction\nS9. Mediator Bias\nUnderstanding confounding bias in mediation analysis\nS10. Misclassification Bias\nExamples of misclassification bias and bias towards the nulls"
  },
  {
    "objectID": "pdf_content_analysis.html",
    "href": "pdf_content_analysis.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "pdf_content_analysis.html#current-problematic-mappings",
    "href": "pdf_content_analysis.html#current-problematic-mappings",
    "title": "",
    "section": "Current Problematic Mappings",
    "text": "Current Problematic Mappings\n\n✅ #fig-conventions (Line 134) - CORRECT\n\nContent: “conventions that describe the meanings of our symbols”\nDescription: “Our variable naming conventions”\n\nCurrent PDF: 1a-terminologylocalconventions.pdf ✅ CORRECT\nVariables described: X, A, Y, Y(a), L, U, M, X̄, R\n\n\n\n❌ #fig-general (Line 176) - WRONG\n\nContent: “conventions that describe components of our causal graphs”\nDescription: “Nodes, Edges, Conditioning Conventions”\nShould contain: Arrow types (black=causality, red=backdoor, dashed=attenuation), boxes (conditioning)\nCurrent PDF: 1a-terminologylocalconventions.pdf ❌ WRONG\nShould use: S1-graphical-key.pdf (based on filename and content description)\n\n\n\n❌ #fig-symbol-key (Line 285) - WRONG\n\nContent: “our conventions” (seems to be about confounding rules)\nDescription: “Four rules of confounding control”\nCurrent PDF: 1a-terminologylocalconventions.pdf ❌ WRONG\n\nShould use: Need to identify correct PDF for confounding control rules\n\n\n\n❌ #fig-terminologyconfounders (Line 302) - WRONG\n\nContent: “four elementary rules of confounding control”\nDescription: “Four rules of confounding control”\nCurrent PDF: 1a-terminologylocalconventions.pdf ❌ WRONG\nShould use: Same as above - likely S2-glossary.pdf or related confounding PDF"
  },
  {
    "objectID": "pdf_content_analysis.html#available-pdfs-analysis",
    "href": "pdf_content_analysis.html#available-pdfs-analysis",
    "title": "",
    "section": "Available PDFs Analysis",
    "text": "Available PDFs Analysis\nLooking at available PDFs: - 1a-terminologylocalconventions.pdf - Variable naming (correctly used for #fig-conventions) - 1b-terminologydirectedgraph.pdf - Five elementary structures (correctly used for #fig-directedgraph) - S1-graphical-key.pdf - Likely contains graphical symbols and conventions - S2-glossary.pdf - Likely contains terminology and rules - Other PDFs focused on specific topics (confounding, measurement error, etc.)"
  },
  {
    "objectID": "pdf_content_analysis.html#proposed-correct-mappings",
    "href": "pdf_content_analysis.html#proposed-correct-mappings",
    "title": "",
    "section": "Proposed Correct Mappings",
    "text": "Proposed Correct Mappings\n\n#fig-conventions → 1a-terminologylocalconventions.pdf ✅ (already correct)\n#fig-general → S1-graphical-key.pdf (nodes, edges, conditioning conventions)\n#fig-symbol-key → S1-graphical-key.pdf or S2-glossary.pdf (confounding control conventions)\n#fig-terminologyconfounders → S2-glossary.pdf (confounding control rules)"
  },
  {
    "objectID": "content/04-worked-example.html",
    "href": "content/04-worked-example.html",
    "title": "Worked Example: Religion → Cooperation",
    "section": "",
    "text": "This worked example demonstrates a complete causal inference pipeline using the margot package and generalised random forests (GRF). We investigate whether religious service attendance causally affects cooperative behaviour using synthetic data from the New Zealand Attitudes and Values Study.\n\n\n\n\n\n\nImportantResearch Question\n\n\n\nDoes weekly religious service attendance increase cooperative behaviour among New Zealand adults?"
  },
  {
    "objectID": "content/04-worked-example.html#introduction",
    "href": "content/04-worked-example.html#introduction",
    "title": "Worked Example: Religion → Cooperation",
    "section": "",
    "text": "This worked example demonstrates a complete causal inference pipeline using the margot package and generalised random forests (GRF). We investigate whether religious service attendance causally affects cooperative behaviour using synthetic data from the New Zealand Attitudes and Values Study.\n\n\n\n\n\n\nImportantResearch Question\n\n\n\nDoes weekly religious service attendance increase cooperative behaviour among New Zealand adults?"
  },
  {
    "objectID": "content/04-worked-example.html#causal-framework",
    "href": "content/04-worked-example.html#causal-framework",
    "title": "Worked Example: Religion → Cooperation",
    "section": "2 Causal Framework",
    "text": "2 Causal Framework\n\n2.1 Target Estimand\nWe estimate the Average Treatment Effect (ATE) of moving from no weekly religious service to weekly religious service on multiple cooperation outcomes.\n\n\n2.2 Identification Strategy\n\nNo unmeasured confounding - conditioning on baseline demographics, personality, and prior cooperation\nPositivity - sufficient overlap in religious service across covariate strata\nStable Unit Treatment Value Assumption (SUTVA) - no interference between individuals"
  },
  {
    "objectID": "content/04-worked-example.html#data-and-methods",
    "href": "content/04-worked-example.html#data-and-methods",
    "title": "Worked Example: Religion → Cooperation",
    "section": "3 Data and Methods",
    "text": "3 Data and Methods\n\n3.1 Synthetic NZAVS Data\n\nSample size: 40,000k at baseline\nDesign: Three-wave longitudinal panel (Oct 2018- Oct 2021)\nExposure: Weekly religious service attendance (binary)\nOutcomes: Four cooperation measures (charitable donations, volunteering, social support, belonging)\n\n\n\n3.2 Statistical Approach\nWe use causal forests implemented in the margot package to:\n\nEstimate average treatment effects with doubly robust machine learning\nEvaluate treatment effect heterogeneity using RATE and Qini metrics\nIdentify subgroups with policy trees for targeted interventions"
  },
  {
    "objectID": "content/04-worked-example.html#analysis-workflow",
    "href": "content/04-worked-example.html#analysis-workflow",
    "title": "Worked Example: Religion → Cooperation",
    "section": "4 Analysis Workflow",
    "text": "4 Analysis Workflow\n\n4.1 Step 1: Environment Setup\n\n\n\n\n\n\nNoteView Script: 00-setup.R\n\n\n\n\n\n\n\nCode\n# SPARCC Day 2 Workshop: Religion → Cooperation Analysis\n# Setup Script: Preparing the R Environment\n# This ensures reproducibility and consistent results across all participants\n\nrstudioapi::restartSession(clean = TRUE)\n\n# set seed for reproducibility\nset.seed(2025)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.230\") {\n  stop(\"please install margot &gt;= 1.0.230 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n\n\n# load packages ----------------------------------------------------------\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\nif (!requireNamespace(\"cli\", quietly = TRUE)) {\n  install.packages(\"cli\")\n}\nlibrary(\"cli\")\n\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir &lt;- here::here(\"data\")\n\ncli::cli_h1(\"created data folder ✔\")\n\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl &lt;- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file &lt;- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long &lt;- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\ncli::cli_h1(\"downloaded data to data folder for furture use ✔\")\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n\n\n\n\nKey actions: - Downloads synthetic NZAVS data (20,000 observations) - Sets up project structure and data directory - Ensures reproducibility with workshop seed (2025)\n\n\n4.2 Step 2: Data Import and Preparation\n\n\n\n\n\n\nNoteView Script: 01-data-import.R\n\n\n\n\n\n\n\nCode\n# SPARCC Day 2 Workshop: Data Import and Preparation\n# Script 1: Religion → Cooperation Analysis\n# This script imports synthetic NZAVS data and prepares variables for causal analysis\n\n# restart fresh session for clean workspace\nrstudioapi::restartSession(clean = TRUE)\n\n# set seed for workshop reproducibility  \nset.seed(2025)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n# min version of margot\nmin_version &lt;- \"1.0.230\"\nif (utils::packageVersion(\"margot\") &lt; min_version) {\n  stop(\n    \"please install margot &gt;= \", min_version, \":\\n\",\n    \"  devtools::install_github('go-bayes/margot')\"\n  )\n}\n\n# call library\nlibrary(\"margot\")\n\n# check version\npackageVersion(pkg = 'margot')\n\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,       # parallel processing\n  grf,             # causal forests\n  janitor,          # variables names\n  stringr,          # variable names\n  patchwork,        # graphs\n  table1,          # tables,\n  cli\n)\n\n\n# set up data directory structure\ndata_dir    &lt;- here::here('/Users/joseph/v-project Dropbox/Joseph Bulbulia/00-nazvs-data-backup/r/2015/')\npush_mods   &lt;- here::here(\"/Users/joseph/v-project Dropbox/data/25/grf-church-coop\") \n\n# load data -----------------------------------------------------------------\ndf_nz_long &lt;- margot::here_read_qs(\"nzavs_data\", data_dir)\n\n# initial data prep ---------------------------------------------------------\n# prepare intial data\n# define labels for rural classification\nrural_labels &lt;- c(\n  \"High Urban Accessibility\", \n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\", \n  \"Remote\", \n  \"Very Remote\"\n)\n\ndat_prep &lt;- df_nz_long |&gt;\n  arrange(\"id\", \"time_factor\") |&gt;\n  margot::remove_numeric_attributes() |&gt;\n  # mutate(\n  #   # cap extreme values\n  #   alcohol_intensity = pmin(alcohol_intensity, 15),\n  #   # flag heavy drinkers: freq ≥3 → 1, ≤2 → 0, else NA\n  #   heavy_drinker = case_when(\n  #     alcohol_frequency &gt;= 3 ~ 1,\n  #     alcohol_frequency &lt;= 2 ~ 0,\n  #     TRUE                  ~ NA_real_\n  #   ),\n  #   # map freq categories to weekly counts\n  #   alcohol_frequency_weekly = recode(\n  #     alcohol_frequency,\n  #     `0` = 0, `1` = 0.25,\n  #     `2` = 1, `3` = 2.5,\n  #     `4` = 4.5,\n  #     .default = NA_real_\n  #   ),\n  #   # relabel rural factor\n  #   rural_gch_2018_l = factor(\n  #     rural_gch_2018_l,\n  #     levels = 1:5,\n  #     labels = rural_labels,\n  #     ordered = TRUE\n  #   )\n  # ) |&gt;\n  droplevels()\n\n\n\n# view variable names -----------------------------------------------------\nprint(colnames(df_nz_long)) \n\n# get total participants\nn_total = length(unique(df_nz_long$id))\n\n# pretty number\nn_total = margot::pretty_number(n_total)\n\n# view\nn_total\n\n# save\nhere_save(n_total, \"n_total\")\n\n# define study variables ----------------------------------------------------\n# ** key decision 1: define your three study waves **\n# **  define your study waves **\nbaseline_wave      &lt;- \"Time 10\"        # baseline measurement\nexposure_waves     &lt;- \"Time 11\"    # when exposure is measured\noutcome_wave       &lt;- \"Time 12\"        # when outcomes are measured\nall_waves          &lt;- c(baseline_wave, exposure_waves, outcome_wave)\ntime_factor           &lt;- \"time_factor\" \n\ncli::cli_h1(\"set waves for three-wave study ✔\")\n\n# define exposure variable ----------------------------------------------------\n# ** key decision 2: define your exposure variable **\n# +--------------------------+\nname_exposure &lt;- \"religion_church\"\n\n# exposure variable labels\nvar_labels_exposure &lt;- list(\n  \"religion_church\" = \"Religious Service (weekly)\",\n  \"religion_church_binary\" = \"Religious Service (weekly)\"\n)\n\ncli::cli_h1(\"set variable name for exposure ✔\")\n\n# define outcome variables -----------------------------------------------\n# ** key decision 3: define outcome variables **\n# here, we are focussing on a subset of wellbeing outcomes\n# chose outcomes relevant to * your * study. Might be all/some/none/exactly \n# these:\noutcome_vars &lt;- c(\n  \"log_charity_donate\",\n  \"log_hours_charity\",\n  \"support\",\n  \"belong\"\n)\ncli::cli_h1(\"set variable name for outcomes ✔\")\n\n# read and sort outcome variables -----------------------------------------\n# we do this by domain: health, psych, present, life, social\nread_and_sort &lt;- function(key) {\n  raw  &lt;- here_read(key, push_mods)\n  vars &lt;- paste0(\"t2_\", raw, \"_z\")\n  sort(vars)\n}\nt2_outcome_z  &lt;- read_and_sort(\"outcome_vars\")\n\n# save for script 3\nhere_save(t2_outcome_z, \"t2_outcome_z\")\n\n\nlabel_mapping_all &lt;- list(\n  \"t2_belong_z\" = \"Sense of Belonging\",\n  \"t2_charity_donate_z\" = \"Charitable Donations (annual, log)\",\n  \"t2_log_hours_charity_z\" = \"Volunteering (weekly, log)\",\n  \"t2_support_z\" = \"Social Support\"\n)\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\ncli::cli_h1(\"created and saved label_mapping for use in graphs/tables ✔\")\n\n# define names for titles -------------------------------------------------\nnice_exposure_name &lt;- name_exposure %&gt;%\n  str_replace_all(\"_\", \" \") %&gt;%\n  stringr::str_to_title()\n\nnice_exposure_name =  \"Religious Service (binary)\" #stringr::str_to_sentence(name_exposure)\nnice_outcome_name = \"Multi-Dimensional Cooperation\"\ntitle = glue::glue(\"Effect of {nice_exposure_name} on {nice_outcome_name}\")\n# save for final rport\nhere_save(title, \"title\")\n\n\n# plot title --------------------------------------------------------------\ntitle_binary = \"Effects of {{name_exposure}} on {{name_outcomes}}\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# for manuscript later\nmargot::here_save(title_binary, \"title_binary\")\n\n\n# +--------------------------+\n# +--------------------------+\n# define baseline variables -----------------------------------------------\n# key decision 4 **  define baseline covariates **\n# these are demographics, traits, etc. measured at baseline, that are common\n# causes of the exposure and outcome.  \n# note we will automatically include baseline measures of the exposure and outcome\n# later in the workflow.\n\nbaseline_vars &lt;- c(\n  # demographics\n  \"age\", \"born_nz_binary\", \"education_level_coarsen\",\n  \"employed_binary\", \"eth_cat\", \"male_binary\",\n  \"not_heterosexual_binary\", \"parent_binary\", \"partner_binary\",\n  \"rural_gch_2018_l\", \"sample_frame_opt_in_binary\",\n  \n  # personality traits (excluding exposure)\n  \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency_weekly\", \"alcohol_intensity\", \"hlth_disability_binary\",\n  \"log_hours_children\", \"log_hours_commute\", \"who_hours_exercise_num\", # better measures\n  \"log_hours_housework\", \"log_household_inc\",\n  \"short_form_health\", \"smoker_binary\", \"bmi_cat_num\", # new & better measures\n  \n  # social and psychological\n  \"belong\", \"nz_dep2018\", \"nzsei_13_l\",\n  \"political_conservative\"#, #\"religion_identification_level\", #&lt;- taken by religion_church\n  \n  # # religious denominations\n  # \"religion_bigger_denominations\" # &lt;-*OPTIONAL\n)\n\ncli::cli_h1(\"set baseline covariate names  ✔\")\n\n# +--------------------------+\n\n# after selecting your exposure/ baseline / outcome variables do not modify this\n# code\n\n# make binary variable (UNLESS YOUR EXPOSURE IS A BINARY VARIABLE)\nexposure_var_binary = paste0(name_exposure, \"_binary\")\n\n# make exposure variable list (we will keep both the continuous and binary variable)\nexposure_var  &lt;- c(name_exposure, paste0(name_exposure, \"_binary\"))\n\n# sort for easier reference\nbaseline_vars &lt;- sort(baseline_vars)\noutcome_vars &lt;- sort(outcome_vars)\n\n# save key variables --------------------------------------------------------\nordinal_columns &lt;- c(\"t0_education_level_coarsen\", \n                     \"t0_eth_cat\", \n                     \"t0_rural_gch_2018_l\", \n                     \"t0_religion_bigger_denominations\") # not used \"t0_religion_church_cat\"\n\n\nmargot::here_save(name_exposure, \"name_exposure\")\nmargot::here_save(var_labels_exposure,\"var_labels_exposure\")\nmargot::here_save(baseline_vars,\"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(exposure_var_binary, \"exposure_var_binary\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(ordinal_columns,\"ordinal_columns\")\nmargot::here_save(name_exposure, \"name_exposure\")\n\n\ncli::cli_h1(\"saved names and labels to be used for manuscript  ✔\")\n\n\n# +--------------------------+\n# select eligible participants ----------------------------------------------\n# only include participants who have exposure data at baseline\n\n# You might require tighter conditions \n# for example, if you are interested in the effects of hours of childcare, \n# you might want to select only those who were parents at baseline. \n# talk to me if you think you might night tighter eligibility criteria.\n\nids_baseline &lt;- dat_prep |&gt; \n  # allow missing exposure at baseline\n  # this would give us greater confidence that we generalise to the target population\n  # filter(wave == baseline_wave) |&gt; \n  # option: do not allow missing exposure at baseline\n  # this gives us greater confidence that we recover a incident effect\n  filter(time_factor == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  pull(id)\n\n# make the data \ndat_long_1 &lt;- dat_prep |&gt;\n  filter(id %in% ids_baseline & time_factor %in% c(baseline_wave, exposure_waves, outcome_wave)) |&gt;\n  mutate(alert_level_error = ifelse(alert_level_combined == \"no_alert\" | alert_level_combined == \"early_covid\", 0, 1)) |&gt;\n  droplevels()# note that we might have more than one exposure wave\n\n# censor if alert level 2 or 4\ntable(dat_long_1$alert_level_error)\n\n\ntest_error_df &lt;- dat_prep |&gt;  filter(year_measured == 1 & !is.na(alert_level_combined)) |&gt; droplevels()\ntable1::table1(~ time_factor |alert_level_combined, test_error_df)\n\ntest_error_df_1&lt;- dat_long_1 |&gt;  filter(!is.na(alert_level_error)) |&gt; droplevels()\ntable1::table1(~ time_factor |as.factor(alert_level_error), test_error_df_1)\n\n# logic -- we don't want measurement error in religious service\n# so we treat those with alert level &gt; 1 as loss to follow up in the following wave, and then censored.\ndat_long_censored  &lt;- margot_censor_lead(\n  dt = dat_long_1,\n  id_var = \"id\",\n  # cluster_id = \"id\",\n  wave_var = \"wave\",\n  condition_var = \"alert_level_error\",\n  condition_value = 1,\n  year_measured_var = \"year_measured\")\n\n# 5930 censored\n\nn_censored = 5930\nhere_save(n_censored, \"n_censored\")\n\nlength(ids_baseline)\nlength(unique(dat_long_censored$id))\n\n# check data\ntable1::table1(~ religion_church | as.factor(wave), dat_long_censored)\ntable1::table1(~  religion_church| as.factor(wave), dat_long_1)\n# +--------------------------+\n# plot distribution to help with cutpoint decision\ndat_long_exposure &lt;- dat_long_1 |&gt; filter(time_factor %in% exposure_waves)\n\noutcome_data &lt;- dat_long_1 |&gt; filter(time_factor %in% outcome_wave)\n\nmean(outcome_data$charity_donate,na.rm=TRUE)\nsd(outcome_data$charity_donate,na.rm=TRUE)\n\n# define cutpoints for graph ----------------------------------------------\n\n# define cutpoints *-- these can be adjusted --* \ncut_points = c(0, 1)\n\n# to use later in positivity graph in manuscript\nlower_cut &lt;- cut_points[[1]]\n\nupper_cut &lt;- cut_points[[2]]\nthreshold &lt;- '&gt;' # if upper\ninverse_threshold &lt;- '&lt;='\nscale_range = 'scale range 1-7'\n\n\n# save for manuscript\nhere_save(lower_cut, \"lower_cut\")\nhere_save(upper_cut, \"upper_cut\")\nhere_save(threshold, \"threshold\")\nhere_save(inverse_threshold, \"inverse_threshold\")\nhere_save(scale_range, \"scale_range\")\n\n\ncli::cli_h1(\"set thresholds for binary variable (if variable is continuous) ✔\")\n\ndf_graph &lt;- dat_long_exposure |&gt;  mutate(religion_church = round(ifelse(religion_church &gt; 8, 8, religion_church)), 1) \n\n\n# make graph\ngraph_cut &lt;- margot::margot_plot_categorical(\n  df_graph,\n  col_name         = name_exposure,\n  sd_multipliers = c(-1, 1), # select to suit\n  binwidth = 1,\n  # either use n_divisions for equal-sized groups:\n  # n_divisions      = 2,\n  # or use custom_breaks for specific values:\n  custom_breaks    = cut_points,  # ** adjust as needed **\n  # could be \"lower\", no difference in this case, as no one == 4\n  cutpoint_inclusive = \"lower\",\n  show_mean        = FALSE,\n  show_median      = FALSE,\n  show_sd          = FALSE\n)\nprint(graph_cut)\n\n# save your graph\nmargot::here_save(graph_cut, \"graph_cut\", push_mods)\nmargot_save_png(graph_cut, \n                height = 10, \n                width = 12, \n                base_filename = \"2025_church_graph_cut\")\n\n\n\n# create binary exposure variable based on chosen cutpoint\n# we already have a binary variable\n# dat_long_2 &lt;- margot::create_ordered_variable(\n#   dat_long_1,\n#   var_name           = name_exposure,\n#   custom_breaks      = cut_points,  # ** -- adjust based on your decision above -- **\n#   cutpoint_inclusive = \"upper\"\n# )\n\n\ncli::cli_h1(\"created binary variable (if variable is continuous) ✔\")\n\n# process binary variables and log-transform --------------------------------\n# convert binary factors to 0/1 format\ndat_long_3 &lt;- margot::margot_process_binary_vars(dat_long_1) #*--- note this is dat_long_1 for this study\n\n# log-transform hours and income variables: tables for analysis (only logged versions of vars)\ndat_long_final &lt;- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\", \"charity_donate\"), # **--- think about this ---***\n  prefix          = \"log_\",\n  keep_original   = FALSE,\n  exceptions = exposure_var  # omit original variables#  **--- think about this ---***\n) |&gt; \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars, exposure_var, outcome_vars, \"id\", \"time_factor\", \"year_measured\", \"sample_weights\"))) |&gt; \n  droplevels()\n\n\n# check missing data --------------------------------------------------------\n# this is crucial to understand potential biases\nmissing_summary &lt;- naniar::miss_var_summary(dat_long_final)\nprint(missing_summary)\nmargot::here_save(missing_summary, \"missing_summary\", push_mods)\n\n# visualise missing data pattern\n# ** -- takes a while to render ** \nvis_miss &lt;- naniar::vis_miss(dat_long_final, warn_large_data = FALSE)\nprint(vis_miss)\nmargot::here_save(vis_miss, \"vis_miss\", push_mods)\n\n# calculate percentage of missing data at baseline\ndat_baseline &lt;- dat_long_final |&gt; filter(time_factor == baseline_wave)\npercent_missing_baseline &lt;- naniar::pct_miss(dat_baseline)\nmargot::here_save(percent_missing_baseline, \"percent_missing_baseline\", push_mods)\n\n# save prepared dataset for next stage --------------------------------------\nmargot::here_save(dat_long_final, \"dat_long_final\", push_mods)\n\n\ncli::cli_h1(\"made and saved final long data set for further processign in script 02 ✔\")\n\n\n# check positivity --------------------------------------------------------\n\n# check\nthreshold # defined above\nupper_cut # defined above\nname_exposure # defined above\n\n\n# create transition matrices to check positivity ----------------------------\n# this helps assess whether there are sufficient observations in all exposure states\ndt_positivity &lt;- dat_long_final |&gt;\n  filter(time_factor %in% c(baseline_wave, exposure_waves)) |&gt;\n  select(!!sym(name_exposure), id, time_factor) |&gt;\n  mutate(exposure = round(as.numeric(!!sym(name_exposure)), 0)) |&gt;\n  mutate(exposure = if_else(exposure &gt; 8, 8, exposure)) |&gt; \n  # create binary exposure based on cutpoint\n  mutate(exposure_binary = ifelse(exposure &gt; upper_cut, 1, 0)) |&gt; # check\n  ## *-- modify this --* \n  mutate(time_factor = as.numeric(time_factor) -1 )\n\n# create transition tables\ntransition_tables &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var  = \"time_factor\",\n  table_name = \"transition_table\"\n)\n\n# check\nprint(transition_tables$tables[[1]])\n\n# save\nmargot::here_save(transition_tables, \"transition_tables\", push_mods)\n\n# create binary transition tables\ntransition_tables_binary &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"time_factor\",\n  table_name = \"transition_table_binary\"\n)\n\n# check\nprint(transition_tables_binary$tables[[1]])\n\n# save\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\", push_mods)\n\n# create tables -----------------------------------------------------------\n# baseline variable labels\ndf_nz_long$bmi_cat_num\nvar_labels_baseline &lt;- list(\n  # demographics\n  \"age\" = \"Age\",\n  \"born_nz_binary\" = \"Born in New Zealand\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"male_binary\" = \"Male\",\n  \"not_heterosexual_binary\" = \"Non-heterosexual\",\n  \"parent_binary\" = \"Parent\",\n  \"partner_binary\" = \"Has Partner\",\n  \"rural_gch_2018_l\" = \"Rural Classification\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In\",\n  \n  # economic & social status\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"nz_dep2018\" = \"NZ Deprivation Index\",\n  \"nzsei_13_l\" = \"Occupational Prestige Index\",\n  \"household_inc\" = \"Household Income\",\n\n  \n  # personality traits\n  \"agreeableness\" = \"Agreeableness\",\n  \"conscientiousness\" = \"Conscientiousness\",\n  \"neuroticism\" = \"Neuroticism\",\n  \"openness\" = \"Openness\",\n  \n  # beliefs & attitudes\n  \"political_conservative\" = \"Political Conservatism\",\n  \"religion_identification_level\" = \"Religious Identification\",\n  \n  # health behaviors\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_disability_binary\" = \"Disability Status\",\n  \"smoker_binary\" = \"Smoker\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \"bmi_cat_num\" =  \"Body Mass Index (WHO)\",\n  \n  \n  # time use\n  \"hours_children\" = \"Hours with Children\",\n  \"hours_commute\" = \"Hours Commuting\",\n  \"hours_exercise\" = \"Hours Exercising\",\n  \"hours_housework\" = \"Hours on Housework\",\n  \"log_hours_children\" = \"Log Hours with Children\",\n  \"log_hours_commute\" = \"Log Hours Commuting\",\n   \"who_hours_exercise_num\" = \"Hours Exercising (WHO)\",\n  \"log_hours_housework\" = \"Log Hours on Housework\",\n  \n\n  # Added (Optional)\n  \"religion_bigger_denominations\" = \"Major Religions\"\n)\nhere_save(var_labels_baseline, \"var_labels_baseline\")\n\n# get names\nvar_labels_outcomes &lt;- list(\n  \"log_hours_charity\" = \"Volunteering (weekly, log)\",\n  \"log_charity_donate\" = \"Charitable Donations (annual, log)\",\n  \"support\" = \"Social Support\",\n  \"belong\" = \"Social Belonging\"\n)\n\n# save for manuscript\nhere_save(var_labels_outcomes, \"var_labels_outcomes\")\n\n# save all variable translations\nvar_labels_measures &lt;- c(var_labels_baseline, var_labels_exposure, var_labels_outcomes)\nvar_labels_measures\n\n# save for manuscript\nhere_save(var_labels_measures, \"var_labels_measures\")\n\n# graph\nnzavs_sample_weights &lt;- dat_baseline$sample_weights\nhist_sample_weights &lt;- tibble(nzavs_sample_weights = nzavs_sample_weights) |&gt;\n  ggplot(aes(x = nzavs_sample_weights)) +\n  geom_histogram()\n\n# view\nhist_sample_weights\n\n# save\nmargot_save_png(hist_sample_weights, \n                height = 10, \n                width = 12, \n                base_filename = \"hist_sample_weights\")\n\nn_participants\n\n# stabalise weights; \nt0_sample_weights &lt;- margot_trim_sample_weights(dat_baseline$sample_weights,  upper_quantile = .99)\nhist(t0_sample_weights)\nhere_save(t0_sample_weights, \"t0_sample_weights\")\n\n\n# make baseline table -----------------------------------------------------\nbaseline_table &lt;- margot::margot_make_tables(\n  data = dat_baseline,\n  vars = baseline_vars,\n  by =  time_factor,\n  labels = var_labels_baseline,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(baseline_table)\nmargot::here_save(baseline_table, \"baseline_table\", push_mods)\n\n# create exposure table by wave\nexposure_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(time_factor %in% c(baseline_wave, exposure_waves)),\n  vars = exposure_var,\n  by = \"time_factor\",\n  labels = var_labels_exposure,\n  factor_vars = exposure_var_binary,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(exposure_table)\nmargot::here_save(exposure_table, \"exposure_table\", push_mods)\n\n# create outcomes table by wave\noutcomes_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(time_factor %in% c(baseline_wave, outcome_wave)),\n  vars = outcome_vars,\n  by = \"time_factor\",\n  labels = var_labels_outcomes,\n  format = \"markdown\"\n)\nprint(outcomes_table)\nmargot::here_save(outcomes_table, \"outcomes_table\", push_mods)\n\n# note: completed data preparation step -------------------------------------\n# you're now ready for the next steps:\n# 1. creating wide-format dataset for analysis \n# 2. applying causal inference methods\n# 3. conducting sensitivity analyses\n\n# key decisions summary:\n# exposure variable: extraversion\n# study waves: baseline (2018), exposure (2019), outcome (2020)\n# baseline covariates: demographics, traits, health measures (excluding exposure)\n# outcomes: health, psychological, wellbeing, and social variables\n# binary cutpoint for exposure: here, 4 on the extraversion scale\n# label names for tables\n\n\n# make timeline -----------------------------------------------------------\nnzavs_wave_breaks &lt;- list(\n  \"time 10\" = c(as.Date(\"2018-06-18\"), as.Date(\"2019-09-30\")),\n  \"time 11\" = c(as.Date(\"2019-10-01\"), as.Date(\"2020-09-30\")),\n  \"time 12\" = c(as.Date(\"2020-10-01\"), as.Date(\"2021-09-30\"))\n  # \"time 13\" = c(as.Date(\"2021-10-01\"), as.Date(\"2022-09-20\")),\n  # \"time 14\" = c(as.Date(\"2022-09-21\"), as.Date(\"2023-10-14\")),\n  # \"time 15\" = c(as.Date(\"2023-10-15\"), as.Date(\"2024-10-25\"))\n)\n\n# use margot's prepare panel data function. \ndf_test &lt;- prepare_panel_data(\n  dat_long_censored,\n  wave_col = \"wave\",\n  tscore_col = \"tscore\",\n  id_col = \"id\",\n  base_date = as.Date(\"2009-06-30\"),\n  wave_breaks = nzavs_wave_breaks\n)\n\n\n# make the time line and save it, recalling again that \"push_mods\" is a path on your computer\nmargot_plot_response_timeline(df_timeline = df_test$df_timeline,\n                              n_total_participants = df_test$n_total_participants,\n                              save = TRUE,\n                              save_path = here::here(push_mods),\n                              width = 12,\n                              height = 8,\n                              base_filename = \"timeline_histogram\",\n                              title = \"Panel Study Timeline\",\n                              x_label = \"Year\",\n                              y_label = \"Count of Responses\",\n                              color_palette = NULL,\n                              save_png = FALSE,\n                              use_timestamp = FALSE)\n\n# make the time line and save it, recalling again that \"push_mods\" is a path on your computer\n\n\n# scale ranges ------------------------------------------------------------\nscale_range_exposure &lt;- c(0,8)\nscale_ranges_outcomes &lt;- c(1, 7) # sat nz environment 0-10\n\nindividual_plot_exposure &lt;- margot_plot_individual_responses(\n  dat_long_censored,\n  y_vars = name_exposure,\n  id_col = \"id\",\n  waves = c(2018:2019), \n  theme = theme_classic(),\n  random_draws =56,\n  title = NULL,\n  y_label = NULL,\n  x_label = NULL,\n  color_palette = NULL,\n  include_timestamp = FALSE,\n  save_path = here::here(push_mods),\n  width = 16,\n  height = 8,\n  seed = 123,\n  full_response_scale = TRUE,\n  scale_range = scale_range_exposure\n)\nindividual_plot_exposure\n\n# individual plot ---------------------------------------------------------\n# exposure plot\nindividual_plot_exposure &lt;- margot_plot_individual_responses(\n  dat_long_censored,\n  y_vars = name_exposure,\n  id_col = \"id\",\n  waves = c(2018,2019), # avoid biases note that 2020, 2021 church is imputed by carrying one forward\n  theme = theme_classic(),\n  random_draws =64,\n  title = NULL,\n  y_label = NULL,\n  x_label = NULL,\n  color_palette = NULL,\n  include_timestamp = FALSE,\n  save_path = here::here(push_mods),\n  width = 16,\n  height = 8,\n  seed = 123,\n  full_response_scale = TRUE,\n  scale_range = scale_range_exposure\n)\n\n# view\nindividual_plot_exposure\n\n# check size of object\nmargot_size( individual_plot_exposure )\n\n\n\n\n\nKey decisions made: - Baseline wave: Time 10 (2018) - demographics and baseline cooperation - Exposure wave: Time 11 (2019) - religious service attendance\n- Outcome wave: Time 12 (2020) - cooperation outcomes - Exposure definition: Weekly religious service (binary: 0 = never/rarely, 1 = weekly+) - Baseline covariates: 32 variables including demographics, personality traits, health, and prior cooperation\n\n\n4.3 Step 3: Data Processing and Weights\n\n\n\n\n\n\nNoteView Script: 02-data-processing.R\n\n\n\n\n\n\n\nCode\n# script 2: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession(clean = TRUE)\n\n# save paths -------------------------------------------------------------------\npush_mods &lt;- here::here('/Users/joseph/v-project Dropbox/data/25/grf-church-coop') \n\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n\n# set seed for reproducibility\nset.seed(42)\n\n\n# libraries ---------------------------------------------------------------\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.200\") {\n  stop(\"please install margot &gt;=1.0.200 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(margot)\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,      # parallel processing\n  grf,             # causal forests\n  janitor,         # variables names\n  stringr,         # variable names\n  patchwork,       # graphs\n  table1,           # tables\n  cli\n)\n\n# read data\ndat_long_final &lt;- margot::here_read(\"dat_long_final\", push_mods)\n\n# read baseline sample weights\nt0_sample_weights &lt;- margot::here_read(\"t0_sample_weights\")\n\n# read exposure\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n# read variables\nbaseline_vars &lt;- margot::here_read(\"baseline_vars\")\nexposure_var &lt;- margot::here_read(\"exposure_var\")\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\nbaseline_wave &lt;- margot::here_read(\"baseline_wave\")\nexposure_waves &lt;- margot::here_read(\"exposure_waves\")\noutcome_wave &lt;- margot::here_read(\"outcome_wave\")\nordinal_columns &lt;- margot::here_read(\"ordinal_columns\")\nordinal_columns\n# define continuous columns to keep\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# check is this the exposure variable that you want? \nname_exposure_binary\nname_exposure_continuous\n\n# ordinal use\n# ordinal_columns &lt;- c(\n#   \"t0_education_level_coarsen\",\n#   \"t0_eth_cat\",\n#   \"t0_rural_gch_2018_l\",\n#   \"t0_gen_cohort\"#,\n#   # \"t0_religion_bigger_denominations\" # &lt;- added for demonstration (optional)\n# )\n\n\n# define wide variable names\nt0_name_exposure_binary &lt;- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\n# treatments (continuous verion)\nt0_name_exposure &lt;- paste0(\"t0_\", name_exposure_continuous)\nt1_name_exposure &lt;- paste0(\"t1_\", name_exposure_continuous)\nt0_name_exposure_continuous &lt;- paste0(\"t0_\", name_exposure)\nt1_name_exposure_continuous &lt;- paste0(\"t1_\", name_exposure)\n\n# raw outcomes\n# read health outcomes\noutcome_vars &lt;- here_read(\"outcome_vars\")\nt2_outcome_z &lt;- here_read(\"t2_outcome_z\")\n\n# view\nt2_outcome_z\n\n# check\nstr(dat_long_final)\n\n# check\nnaniar::gg_miss_var(dat_long_final)\n\n# impute data --------------------------------------------------------------\n\n# define cols we will not standardise\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# remove sample weights\ndat_long_final_2 &lt;- dat_long_final |&gt; select(-sample_weights)\n\n# prepare data for analysis ----------------------\ndat_long_final_2 &lt;- margot::remove_numeric_attributes(dat_long_final_2)\n# wide data\ndf_wide &lt;- margot_wide_machine(\n  dat_long_final,\n  id = \"id\",\n  wave = \"time_factor\",\n  baseline_vars,\n  exposure_var = exposure_var,\n  outcome_vars,\n  confounder_vars = NULL,\n  imputation_method = \"mice\",\n  include_exposure_var_baseline = TRUE,\n  include_outcome_vars_baseline = TRUE,\n  extend_baseline = FALSE,\n  include_na_indicators = FALSE\n)\n\n# check\ncolnames(df_wide)\n\n# return sample weights\ndf_wide$t0_sample_weights &lt;-  t0_sample_weights\n\n# save\nmargot::here_save(df_wide, \"df_wide\")\n\n#df_wide &lt;- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n# view\nglimpse(df_wide)\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n# made data wide in correct format\n# ** ignore warning *** \ndf_wide_encoded  &lt;- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns, #&lt;- make sure all ordinal columns have been identified\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = exposure_var,\n  scale_continuous = TRUE\n)\n\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric if not already\n# df_wide_encoded[[t0_name_exposure_binary]] &lt;-\n#   as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\n# df_wide_encoded[[t1_name_exposure_binary]] &lt;-\n#   as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]]\n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(all(df_wide_encoded[[t0_name_exposure_binary]][!is.na(df_wide_encoded[[t0_name_exposure_binary]])] %in% 0:1),\n          all(df_wide_encoded[[t1_name_exposure_binary]][!is.na(df_wide_encoded[[t1_name_exposure_binary]])] %in% 0:1))\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 &lt;- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 &lt;- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(all(is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n                df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1))\n\n# view\nglimpse(df_wide_encoded)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\n\n#save data\nhere_save(df_wide_encoded, \"df_wide_encoded\")\n\n# new weights approach ---------------------------------------------------------\n\n\n# panel attrition workflow using grf (two-stage IPCW + design weights)\n# -----------------------------------------------------------------------------\n# builds weights in two stages:\n#   w0 : baseline -&gt; t1  (baseline covariates)\n#   w1 : t1 survivors -&gt; t2  (baseline + time-1 exposure)\n# final weight = t0_sample_weights × w0 × w1, then trimmed & normalised.\n# -----------------------------------------------------------------------------\n\n# ── 0 setup ───────────────────────────────────────────────────────────────────\n\nlibrary(tidyverse)        # wrangling\nlibrary(glue)             # strings\nlibrary(grf)              # forests\nlibrary(cli)              # progress\n\nset.seed(2025)\n\n# -----------------------------------------------------------------------------\n# 1 import full, unfiltered baseline file\n# -----------------------------------------------------------------------------\n\ndf &lt;- margot::here_read(\"df_wide_encoded\")\ncli::cli_alert_info(glue(\"{nrow(df)} rows × {ncol(df)} columns loaded\"))\n\n# -----------------------------------------------------------------------------\n# 2 stage‑0 censoring: dropout between t0 → t1\n# -----------------------------------------------------------------------------\n\nbaseline_covars &lt;- df %&gt;%\n  select(starts_with(\"t0_\"), -ends_with(\"_lost\"), -ends_with(\"lost_following_wave\"), -ends_with(\"_weights\")) %&gt;%\n  colnames() %&gt;% sort()\n\n# select your baseline vars and coerce to numeric\nnum_dat &lt;- df %&gt;%\n  select(all_of(baseline_covars)) %&gt;%\n  mutate(across(everything(), as.numeric))\n\n# build a true numeric matrix\nX0 &lt;- as.matrix(num_dat)\n\n# make factor\nD0 &lt;- factor(df$t0_lost_following_wave, levels = c(0, 1))   # 0 = stayed, 1 = lost\n\ncli::cli_h1(\"stage 0: probability forest for baseline dropout …\")\n\n# then fit\npf0 &lt;- probability_forest(X0, D0)\nP0  &lt;- predict(pf0, X0)$pred[, 2]               # P(dropout by t1)\nw0  &lt;- ifelse(D0 == 1, 0, 1 / (1 - P0))         # IPCW for stage 0\ndf$w0 &lt;- w0\n\n# -----------------------------------------------------------------------------\n# 3 stage‑1 censoring: dropout between t1 → t2 (baseline + exposure)\n# -----------------------------------------------------------------------------\nexposure_var &lt;- t1_name_exposure_binary       # ← binary exposure variable name\n\n# filter out those lost (already weighted for censoring)\ndf1 &lt;- df %&gt;% filter(t0_lost_following_wave == 0)\n\n# filter to those at risk in stage-1\ncen1_data &lt;- df %&gt;%\n  filter(t0_lost_following_wave == 0,\n         !is.na(.data[[exposure_var]]))\n\n# coerce baseline covars + exposure all at once\nX1_num &lt;- cen1_data %&gt;%\n  # convert every t0_… and the exposure to numeric\n  mutate(across(all_of(c(baseline_covars, exposure_var)), as.numeric)) %&gt;%\n  # now select in the order you want\n  select(all_of(baseline_covars), all_of(exposure_var))\n\n# build numeric matrix\nX1 &lt;- as.matrix(X1_num)\ncolnames(X1)[ncol(X1)] &lt;- exposure_var\n\nD1 &lt;- factor(cen1_data$t1_lost_following_wave, levels = c(0, 1))\n\ncli::cli_h1(\"stage 1: probability forest for second-wave dropout …\")\npf1 &lt;- probability_forest(X1, D1)\nP1  &lt;- predict(pf1, X1)$pred[, 2]\nw1  &lt;- ifelse(D1 == 1, 0, 1 / (1 - P1))\n\n# map w1 back to df1 (rows with NA exposure get weight 0)\ndf1$w1 &lt;- 0\ndf1$w1[match(cen1_data$id, df1$id)] &lt;- w1\n\n# -----------------------------------------------------------------------------\n# 4 combine design × IPCW weights\n# -----------------------------------------------------------------------------\n# bring forward w0 for the matching rows (safe join)\nw0_vec &lt;- df$w0[match(df1$id, df$id)]\n\n# combined weight before trim / normalise\nraw_w &lt;- df1$t0_sample_weights * w0_vec * df1$w1\n\ndf1$raw_weight &lt;- raw_w\n\n# trim + normalise (exclude NA & zeros)\npos &lt;- raw_w[!is.na(raw_w) & raw_w &gt; 0]\n\nlb  &lt;- quantile(pos, 0.00, na.rm = TRUE)\nub  &lt;- quantile(pos, 0.99, na.rm = TRUE)\n\ntrimmed &lt;- pmin(pmax(raw_w, lb), ub)\nnormalised &lt;- trimmed / mean(trimmed, na.rm = TRUE)\n\ndf1$combo_weights &lt;- normalised &lt;- trimmed / mean(trimmed)\n\ndf1$combo_weights &lt;- normalised\n\nhist(df1$combo_weights[df1$t1_lost_following_wave == 0],\n     main = \"combined weights (observed)\", xlab = \"weight\")\n\n# -----------------------------------------------------------------------------\n# 5 analysis set: observed through t2 (not censored at either stage)\n# -----------------------------------------------------------------------------\n\ndf_analysis &lt;- df1 %&gt;%\n  filter(t1_lost_following_wave == 0) %&gt;%\n  droplevels()\n\nnaniar::vis_miss(df_analysis, warn_large_data = FALSE)\nmargot::here_save(df_analysis, \"df_analysis_weighted_two_stage\")\n\ncli::cli_alert_success(glue(\"analysis sample: {nrow(df_analysis)} obs\"))\n\n# TEST DO NOT UNCOMMENT\n# -----------------------------------------------------------------------------\n# 6 causal forest (edit outcome var if needed)\n# -----------------------------------------------------------------------------\n# \n# outcome_var &lt;- \"t2_kessler_latent_depression_z\"   # ← edit\n# \n# Y &lt;- df_analysis[[outcome_var]]\n# W &lt;- df_analysis[[exposure_var]]\n# X &lt;- as.matrix(df_analysis[, baseline_covars])\n# \n# cf &lt;- causal_forest(\n#   X, Y, W,\n#   sample.weights = df_analysis$combo_weights,\n#   num.trees      = 2000\n# )\n# \n# print(average_treatment_effect(cf))\n# margot::here_save(cf,          \"cf_ipcw_two_stage\")\n# -----------------------------------------------------------------------------\n# 7 save objects\n# -----------------------------------------------------------------------------\n\n\ncli::cli_h1(\"two-stage IPCW workflow complete ✔\")\n\n# maintain workflow \nE &lt;- setdiff(baseline_covars, t0_name_exposure_binary) # keep continuous var at baseline\nhere_save(E, \"E\")\nlength(E)\ncolnames(df_analysis)\n\n\ncli::cli_h1(\"naming convention matcheds `grf` ✔\")\n\n\n# arrange\ndf_grf &lt;- df_analysis |&gt;\n  relocate(ends_with(\"_weights\"), .before = starts_with(\"t0_\")) |&gt;\n  relocate(ends_with(\"_weight\"), .before = ends_with(\"_weights\")) |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |&gt;\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |&gt;\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |&gt;\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |&gt;\n  droplevels()\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# make sure to do this\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\n\ncli::cli_h1(\"saved data `df_grf` for models ✔\")\n\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\n# should have no missing in t1 and t2 variables\n# handled by IPCW\n# make final missing data graph\nmissing_final_data_plot &lt;- naniar::vis_miss(df_grf, warn_large_data = FALSE)\nmissing_final_data_plot\n\n# save plot\nmargot_save_png(missing_final_data_plot, prefix = \"missing_final_data\")\n\n# checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$combo_weights)\n\n# calculate summary statistics\nt0_weight_summary &lt;- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$combo_weights)\n\n# visualise weight distributions\nhist(df_grf$combo_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# check n\nn_observed_grf &lt;- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n# this is just for your interest ------------------------------------------\n# not used in final manuscript\n# FOR INTEREESTS\n# inspect propensity scores -----------------------------------------------\ndf_grf &lt;- here_read('df_grf')\nE &lt;- here_read('E')\n\nStabalised_Attrition_Sample_Weights &lt;- df_grf$combo_weights\nStabalised_Attrition_Sample_Weights\n# graph\nhist_combo_weights &lt;- tibble(Stabalised_Attrition_Sample_Weights = Stabalised_Attrition_Sample_Weights) |&gt;\n  ggplot(aes(x = Stabalised_Attrition_Sample_Weights)) +\n  geom_histogram()\n\n# view\nhist_combo_weights\n\n# save\nmargot_save_png(hist_combo_weights, \n                height = 10, \n                width = 12, \n                base_filename = \"hist_combo_weights\")\n# assign weights var name\n\n# baseline covariates  # E already exists and is defined\nE\n\n# df_grf is a data frame - we must process this data frame in several steps\n# user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\ndf_propensity_org &lt;- df_grf |&gt; select(!starts_with(\"t2_\"))\n\n# Remove NAs and print message that this has been done\ndf_propensity &lt;- df_propensity_org |&gt; drop_na() |&gt; droplevels()\n\ntable(df_propensity[[t1_name_exposure_binary]], useNA = \"always\")\nclass(df_propensity[[t1_name_exposure_binary]])\n\n# ensure it's binary (0/1 or logical)\nunique(df_propensity[[t1_name_exposure_binary]])\n\n# must be a data frame, no NA in exposure\nis.data.frame(df_propensity)\nt1_name_exposure_binary\n# check for problematic variable names in E\nprint(E)\nlength(E)\n# look for special characters or invalid names\nproblematic_vars &lt;- E[!make.names(E) == E]\nprint(problematic_vars)\n\n# check if all variables exist in your dataframe\nmissing_vars &lt;- E[!E %in% names(df_propensity)]\nprint(missing_vars)\n\n# check for variables with insufficient variation\nsapply(df_propensity[E], function(x) length(unique(x, na.rm = TRUE)))\n\n\n\n# create a mapping to fix the problematic names\nname_fixes &lt;- c(\n  \"t0_rural_gch_2018_l_High Urban Accessibility_binary\" = \"t0_rural_gch_2018_l_high_urban_accessibility_binary\",\n  \"t0_rural_gch_2018_l_Low Urban Accessibility_binary\" = \"t0_rural_gch_2018_l_low_urban_accessibility_binary\", \n  \"t0_rural_gch_2018_l_Medium Urban Accessibility_binary\" = \"t0_rural_gch_2018_l_medium_urban_accessibility_binary\",\n  \"t0_rural_gch_2018_l_Very Remote_binary\" = \"t0_rural_gch_2018_l_very_remote_binary\"\n)\n\n\n# fix the rename syntax - don't use all_of() here\ndf_propensity &lt;- df_propensity |&gt;\n  rename(\n    t0_rural_gch_2018_l_high_urban_accessibility_binary = `t0_rural_gch_2018_l_High Urban Accessibility_binary`,\n    t0_rural_gch_2018_l_low_urban_accessibility_binary = `t0_rural_gch_2018_l_Low Urban Accessibility_binary`,\n    t0_rural_gch_2018_l_medium_urban_accessibility_binary = `t0_rural_gch_2018_l_Medium Urban Accessibility_binary`,\n    t0_rural_gch_2018_l_very_remote_binary = `t0_rural_gch_2018_l_Very Remote_binary`\n  )\n\n# update E vector one replacement at a time\nfor(i in seq_along(name_fixes)) {\n  E &lt;- str_replace_all(E, names(name_fixes)[i], name_fixes[i])\n}\n\n# E_propensity_names\n\n\n# not working -------------------------------------------------------------\n\n\n# first run model for baseline propensity if this is selected.  The default should be to not select it.\npropensity_model_and_plots &lt;- margot::margot_propensity_model_and_plots(\n  df_propensity = df_propensity,\n  exposure_variable = t1_name_exposure_binary,\n  baseline_vars = E,\n  weights_var_name = \"t0_sample_weights\",\n  estimand = \"ATE\",\n  method = \"ebal\",\n  focal = NULL\n)\n# visualise\nsummary(propensity_model_and_plots$match_propensity)\n\n\n# key plot\npropensity_model_and_plots$love_plot\n\n\n# save love plot\nmargot_save_png(propensity_model_and_plots$love_plot, \n                height = 10, \n                width = 12, \n                base_filename = \"2025_church_propensity\")\n\n\n# other plots\npropensity_model_and_plots$summary_plot\npropensity_model_and_plots$balance_table\npropensity_model_and_plots$diagnostics\n\n\n# check size\nsize_bytes &lt;- object.size(propensity_model_and_plots)\nprint(size_bytes, units = \"auto\") # Mb\n\n# use qs to save only if you have space\nhere_save_qs(propensity_model_and_plots,\n             \"propensity_model_and_plots\",\n             push_mods)\n\n\n\n\n\nData processing steps: 1. Wide format transformation - panel data to analysis-ready format 2. Missing data handling - multiple imputation for baseline variables 3. Two-stage IPCW weighting - addresses panel attrition bias 4. Standardisation - z-scores for all continuous variables\nFinal analysis sample: ~15,500 participants after accounting for attrition\n\n\n4.4 Step 4: Causal Forest Analysis\n\n\n\n\n\n\nNoteView Script: 03-causal-forests.R\n\n\n\n\n\n\n\nCode\n# SPARCC Day 2 Workshop: Causal Forests and Heterogeneity Analysis\n# Script 3: Religion → Cooperation Analysis Using GRF\n# This script estimates treatment effects and evaluates heterogeneity\n\n# restart fresh session for clean workspace\nrstudioapi::restartSession(clean = TRUE)\n\n# set seed for workshop reproducibility\nseed = 2025\nset.seed(seed)\n\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n# min version of margot\nif (packageVersion(\"margot\") &lt; \"1.0.220\") {\n  stop(\n    \"please install margot &gt;= 1.0.220 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\"\n  )\n}\n\n# call library\nlibrary(\"margot\")\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n\n# load libraries ----------------------------------------------------------\n# pacman will install missing packages automatically\npacman::p_load(\n  tidyverse,\n  # data wrangling + plotting\n  qs,\n  here,# project-relative file paths\n  data.table,# fast data manipulation\n  fastDummies,# dummy variable creation\n  naniar,# missing data handling\n  skimr,# summary statistics\n  grf,\n  ranger,\n  doParallel,\n  kableExtra,\n  ggplot2 ,\n  rlang ,\n  purrr ,\n  patchwork,\n  janitor,  # nice labels\n  glue,\n  cli,\n  future,\n  crayon,\n  glue,\n  stringr,\n  future,\n  furrr\n)\n\n# directory path configuration -----------------------------------------------\n# workshop data directory (automatically configured)\npush_mods &lt;- here::here('/Users/joseph/v-project Dropbox/data/25/grf-church-coop') \n\n# read original data (for plots) ------------------------------------------\noriginal_df &lt;- margot::here_read(\"df_wide\", push_mods)\n\n# import names ------------------------------------------------------------\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read outcome vars\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\n\n# read and sort outcome variables -----------------------------------------\n# view\nt2_outcome_z &lt;- here_read(\"t2_outcome_z\")\n\n# define names for titles -------------------------------------------------\ntitle &lt;- here_read(\n \"title\"\n)\n# check\ntitle\n\n# plot title --------------------------------------------------------------\n# for manuscript later\ntitle_binary &lt;- margot::here_read(\"title_binary\")\n\n# checks\ntitle_binary\n\n\n# combine outcomes ---------------------------------------------------------\n# check outcome vars and make labels for graphs/tables\nlabel_mapping_all &lt;- here_read(\"label_mapping_all\")\n\n# check\nlabel_mapping_all\n\ncli::cli_h1(\"created and saved label_mapping for use in graphs/tables ✔\")\n\n\n# load GRF data and prepare inputs ----------------------------------------\ndf_grf &lt;- margot::here_read('df_grf', push_mods)\nE      &lt;- margot::here_read('E', push_mods)\n\n# check exposure binary\nstopifnot(all(df_grf[[t1_name_exposure_binary]][!is.na(df_grf[[t1_name_exposure_binary]])] %in% 0:1))\n# set exposure and weights\n\nW &lt;- as.vector(df_grf[[t1_name_exposure_binary]]) # note it is the processed weights for attrition \"t1\"\n\n# old workflow\n# weights &lt;- df_grf$t1_adjusted_weights\n\n# new weights workflow, use \"combo_weights\" -- see revised script 2\nweights &lt;- df_grf$combo_weights\n\nhist(weights) # quick check for extreme weights\n# select covariates and drop numeric attributes\nX &lt;- margot::remove_numeric_attributes(df_grf[E])\n\n# set model defaults for workshop (optimised for demonstration)\ngrf_defaults &lt;- list(seed = 2025,\n                     min.node.size = 20, \n                     stabilize.splits = TRUE,\n                     num.trees = 1000)\n\n\n\n# causal forest model ----------------\n\n# !!!! THIS WILL TAKE TIME  !!!!!\n# **----- COMMENT OUT AFTER YOU RUN TO AVOID RUNNING MORE THAN ONCE -----**\nmodels_binary_ate &lt;- margot_causal_forest(\n  # &lt;- could be 'margot_causal_forest_parrallel()' if you have a powerful computer\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  #&lt;- can be modified but will affect run times\n  save_models = TRUE,\n  save_data = TRUE,\n  # compute_conditional_means = TRUE,\n  compute_marginal_only = TRUE,\n  train_proportion = NULL,  # use all data for ATE\n  seed = 2025\n)\n# # save to directory\nhere_save_qs(models_binary_ate, \"models_binary_ate\", push_mods)\n# models_binary_ate &lt;- here_read_qs(\"models_binary_ate\", push_mods)\n\n\n# compute models for heterogeneous effects using sample splitting\nmodels_binary_cate &lt;- margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  compute_conditional_means = TRUE,\n  train_proportion = 0.5,   # split for CATE\n  seed = 2025\n)\n\n# save model\nmargot::here_save_qs(models_binary_cate, \"models_binary_cate\", push_mods)\ncli::cli_h1(\"causal forest model completed and saved ✔\")\n\n# count models by category\n# just a check\ncat(\"Number of original models:\\n\",\n    length(models_binary$results),\n    \"\\n\")\n\n\n# make ate plots ----------------------------------------------------------\n#   ************* NEW - CORRECTION FOR FAMILY-WISE ERROR **********\n# make options -------------------------------------------------------------\n# titles\nate_title = glue::glue(\"ATE Effects of {{nice_name_exposure}} on {{nice_name_outcome}}\")\nsubtitle = \"\"\nfilename_prefix = \"grf_\"\n\nhere_save(ate_title, \"ate_title\")\nhere_save(filename_prefix, \"filename_prefix\")\n\n# settings\nx_offset = -.25\nx_lim_lo = -.25\nx_lim_hi = .25\n\n\n# defaults for ate plots\nbase_defaults_binary &lt;- list(\n  type = \"RD\",\n  title = ate_title,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 8,\n  linewidth = 0.75,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 4,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_all &lt;- margot_plot_create_options(\n  title = subtitle,\n  base_defaults = base_defaults_binary,\n  subtitle = subtitle,\n  filename_prefix = filename_prefix\n)\n\n\n# read results back -------------------------------------------------------\nmodels_binary_ate &lt;- margot::here_read_qs(\"models_binary_ate\", push_mods)\n\n\n\n# graph tau(x) ------------------------------------------------------------\n# multi-tau hats\nplots_tau_hats &lt;- margot_plot_tau(models_binary_ate, label_mapping = label_mapping_all)\nplots_tau_hats\n# str(models_binary, max.level = 1)\n# str(models_binary$results, max.level = 1)\nmargot_save_png(plots_tau_hats,\n                height = 10,\n                width = 12,\n                base_filename = \"plots_tau_hats\")\n\n# ate graphs --------------------------------------------------------------\n# then pass to the results\ndevtools::load_all(\"/Users/joseph/GIT/margot/\")\n\nate_results &lt;- margot_plot(\n  models_binary_ate$combined_table,\n  # &lt;- now pass the corrected results.\n  options = outcomes_options_all,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  label_mapping = label_mapping_all,\n  original_df = original_df,\n  e_val_bound_threshold = 1.1,\n  rename_ate = TRUE,\n  adjust = \"bonferroni\",  #&lt;- new\n  alpha = 0.05 # &lt;- new\n)\nate_results$plot\nate_results$plot\n\n\n# interpretation\ncat(ate_results$interpretation)\n\n# save\nhere_save_qs(ate_results, \"ate_results\", push_mods)\n\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown &lt;- margot_bind_tables(\n  ate_results$transformed_table,\n  #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2,\n  # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\"),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_98 &lt;- margot_inspect_qini(models_binary,\n#                                        propensity_bounds = c(0.01, 0.99))\n\n# # FLIPPING OUTCOMES  ------------------------------------------------------\n# \n# # note that the meaning of a heterogeneity will vary depending on our interests.\n# # typically we are interested in whether an exposure improves life, and whether there is variability (aka HTE) in degrees of improvement.\n# # in this case we must take negative outcomes and \"flip\" them -- recalculating the policy trees and qini curves for each\n# # for example if the outcome is depression, then by flipping depression we better understand how the exposure *reduces* depression.\n# # what if the exposure is harmful? say what if we are interested in the effect of depression on wellbeing? In that case, we might\n# # want to \"flip\" the positive outcomes. That is, we might want to understand for whom a negative exposure is extra harmful.\n# # here we imagine that extroversion is generally positive in its effects, and so we \"flip\" the negative outcomes.\n# # if you were interested in a negative exposure, say \"neuroticism\" then you would probably want to flip the positive outcomes.\n# # note there are further questions we might ask. We might consider who responds more 'weakly\" to a negative exposure (or perhaps to a positive exposure).\n# # Such a question could make sense if we had an exposure that was generally very strong.\n# # however, let's stay focussed on evaluating evaluating strong responders. We will flip the negative outcomes if we expect the exposure is positive,\n# # and flip the positive outcomes if we expect the exposure to be generally negative.\n# # if there is no natural \"positive\" or negative, then just make sure the valence of the outcomes aligns, so that all are oriented in the same\n# # direction if they have a valence.  if unsure, just ask for help!\n# \n# # flipping models: outcomes we want to minimise given the exposure --------\n# # standard negative outcomes/  not used in this example\n# # flipping models: outcomes we want to minimise given the exposure --------\n# # standard negative outcomes/  not used in this example\n# \n# \n# # +--------------------------+\n# # |    MODIFY THIS           |\n# # +--------------------------+\n# \n# # WHICH OUTCOMES -- if any ARE UNDESIREABLE?\n# flip_outcomes_standard = c(\n#   #\"t2_alcohol_frequency_weekly_z\",\n#   #\"t2_alcohol_intensity_z\",\n#   #\"t2_hlth_bmi_z\",\n#   #\"t2_hlth_fatigue_z\",\n#   \"t2_kessler_latent_anxiety_z\",\n#   #  ← select\n#   \"t2_kessler_latent_depression_z\",\n#   #  ← select\n#   \"t2_rumination_z\" #  ← select\n#   #\"t2_perfectionism_z\" # the exposure variable was not investigated\n# )\n# \n# # when exposure is negative and you want to focus on how much worse off\n# \n# # NOTE IF THE EXPOSURE IS NEGATIVE, FOCUS ON WHICH OUTCOMES, if any, ARE POSITIVE AND FLIP THESE?\n# # flip_outcomes&lt;- c( setdiff(t2_outcomes_all, flip_outcomes_standard) )\n# \n# # our example has the exposure as positive\n# flip_outcomes &lt;- flip_outcomes_standard\n# \n# # save\n# here_save(flip_outcomes, \"flip_outcomes\")\n# \n# # check\n# flip_outcomes\n# label_mapping_all\n# \n# \n# # +--------------------------+\n# # |   END MODIFY             |\n# # +--------------------------+\n# \n# # get labels\n# flipped_names &lt;- margot_get_labels(flip_outcomes, label_mapping_all)\n# \n# # check\n# flipped_names\n# \n# # save for publication\n# here_save(flipped_names, \"flipped_names\")\n# \n# cli::cli_h1(\"flipped outcomes identified and names saved ✔\")\n# \n# \n# # flip negatively oriented outcomes --------------------------------------\n# \n# # +--------------------------+\n# # |       DO NOT ALTER       |\n# # +--------------------------+\n# \n# \n# # flip models using margot's function\n# \n# #  *** this will take some time ***\n# \n# # ** give it time **\n# # ** once run/ comment out **\n# \n# # +--------------------------+\n# # |          ALERT           |\n# # +--------------------------+\n# # !!!! THIS WILL TAKE TIME  !!!!!\n# # can be margot_flip_forests_parrallel() if you have sufficient compute, set GB = something less than your system RAM\n# models_binary_flipped_all &lt;- margot_flip_forests(models_binary,\n#                                                  flip_outcomes = flip_outcomes_standard,\n#                                                  recalc_policy = TRUE)\n# \n# cli::cli_h1(\"flipped forest models completed ✔\")\n# # !!!! THIS WILL TAKE TIME  !!!!!\n# # save\n# here_save_qs(models_binary_flipped_all,\n#              \"models_binary_flipped_all\",\n#              push_mods)\n# \n# \n# # +--------------------------+\n# # |          ALERT           |\n# # +--------------------------+\n# # !!!! THIS WILL TAKE TIME  !!!!!\n# # read back if needed\n# models_binary_flipped_all &lt;- here_read_qs(\"models_binary_flipped_all\", push_mods)\n# \n# \n# # this is a new function requires margot 1.0.48 or higher\n# label_mapping_all_flipped &lt;- margot_reversed_labels(label_mapping_all, flip_outcomes)\n# \n# # view\n# label_mapping_all_flipped\n# \n# # save flipped labels\n# here_save(label_mapping_all_flipped, \"label_mapping_all_flipped\")\n# \n# # +--------------------------+\n# # |        END ALERT         |\n# # +--------------------------+\n# \n# \n# # +--------------------------+\n# # |       DO NOT ALTER       |\n# # +--------------------------+\n# \n\n# ──────────────────────────────────────────────────────────────────────────────\n# SPARCC WORKSHOP: HETEROGENEITY ANALYSIS WORKFLOW\n# PURPOSE: Demonstrate complete heterogeneity pipeline for religion → cooperation\n#          - Screen outcomes for heterogeneity with omnibus tests\n#          - Plot RATE AUTOC & QINI curves for effect prioritisation  \n#          - Fit policy trees for actionable subgroup identification\n#          - Generate interpretable summaries for workshop participants\n# WORKSHOP FOCUS: Understanding when and for whom religious service affects cooperation\n# DATA: Synthetic NZAVS data (n=20,000) with cooperation outcomes\n# ──────────────────────────────────────────────────────────────────────────────\n\n# check package version early\nstopifnot(utils::packageVersion(\"margot\") &gt;= \"1.0.209\")\n\n\n\n# step 1: omnibus hetero test ---------------------------------------------\n\n# read results back -------------------------------------------------------\nmodels_binary_cate &lt;- margot::here_read_qs(\"models_binary_cate\", push_mods)\nmodels_binary_cate\n\nmodel_analysis &lt;- models_binary_cate\n# define models and labels\nlabel_mapping = label_mapping_all\n\n# run analysis (check that label mapping)\nhte_test_cv &lt;- margot_interpret_heterogeneity(\n  models_binary_cate, \n  label_mapping =  label_mapping,\n  spend_levels = c(.1,.4), #&lt;- default\n  #alpha = 0.20,\n  adjust = \"none\", #&lt;- personality is well defined set of outcomes\n  parallel = FALSE, #&lt;- TRUE is currently running slower margot v 1.0.201\n  include_extended_report = TRUE,\n  use_cross_validation = TRUE,\n  cv_num_folds = 5,\n  seed = 2025\n)\n\n\n# save result\nhere_save(hte_test_cv, \"hte_test_cv\")\n\n\nhte_test_cv &lt;- here_read(\"hte_test_cv\")\n\n# check results\ncat(hte_test_cv$extended_report)\nprint( hte_test_cv$evidence_summary, n = 10)\n\n# view results\nhte_test_cv$all_selected_model_ids\nhte_test_cv$omnibus_results\nhte_test_cv$cv_results$cv_results\nhte_test_cv$rate_results$autoc\nhte_test_cv$rate_results$qini\nhte_test_cv$rate_results$raw_results\n\n\n# 2  PLOT RATE AUTOC CURVES ---------------------------------------------------\n# policy_tree_defaults &lt;- list(\n#   point_alpha              = 0.5,\n#   title_size               = 12,\n#   subtitle_size            = 12,\n#   axis_title_size          = 12,\n#   legend_title_size        = 12,\n#   split_line_color         = \"red\",\n#   split_line_alpha         = 0.8,\n#   split_label_color        = \"red\",\n#   split_label_nudge_factor = 0.007\n# )\n# \n# decision_tree_defaults &lt;- list(\n#   span_ratio        = 0.2,\n#   text_size         = 4,\n#   y_padding         = 0.5,\n#   edge_label_offset = 0.05,\n#   border_size       = 0.01\n# )\n# \n# qini_results_init &lt;- margot_plot_qini_batch(\n#   models_analysis,\n#   # decision_tree_args = policy_tree_defaults,\n#   # policy_tree_args   = policy_tree_defaults,\n#   qini_args          =  list(show_ci = \"cate\"),\n#   # model_names        =  $reliable_model_ids,\n#   original_df        = original_df,\n#   spend_levels = c(.1, .4),\n#   label_mapping      = label_mapping_all,\n#   # max_depth          = 2L,\n#   output_objects     = c(\"qini_plot\", \"diff_gain_summaries\")\n# )\n# qini_results_init$model_t2_belong_z$qini_plot\n\n# qini results\n# devtools::load_all(\"/Users/joseph/GIT/margot/\")\n\n# use reliable ids\nqini_plots &lt;- margot_plot_qini_batch(\n  models_binary_cate,\n  ci_n_points = 20, \n  # baseline_method = \"auto\",\n  # horizontal_line = TRUE,\n  treatment_cost = 1,\n  ylim = c(-.01, 0.2),\n  show_ci = \"cate\",\n  # model_names      = hte_test_cv$all_selected_model_ids, \n  label_mapping      = label_mapping_all\n)\nqini_plots[[2]]\n# \n# qini_plots_unclear &lt;- margot_plot_qini_batch(\n#   models_analysis,\n#   ci_n_points = 20, \n#   # baseline_method = \"auto\",\n#   # horizontal_line = TRUE,\n#   treatment_cost = 1,\n#   ylim = c(-.1,.5),\n#   # show_ci = \"cate\",\n#   # model_names      = hte_test_cv$unclear_model_names, \n#   label_mapping      = label_mapping\n# )\n\n# policy trees ------------------------------------------------------------\n\n# 4  POLICY TREES (max depth = 2) -------------------------------------------\ndevtools::load_all(\"/Users/joseph/GIT/margot/\")\nhte_test_cv$all_selected_model_ids\n\npolicy_tree_result &lt;- margot_policy_tree_stability(\n  models_binary_cate,\n  model_names = hte_test_cv$selected_model_ids,\n  label_mapping = label_mapping,\n  n_iterations = 1000,\n  train_proportion = .5,\n  tree_method = \"fastpolicytree\",  # 10x faster!\n  metaseed = 2025,\n  depth = 2\n)\n\n\n# save\nhere_save_qs(policy_tree_result, \"policy_tree_result\", push_mods)\n\npolicy_tree_result &lt;- here_read_qs(\"policy_tree_result\", push_mods)\npolicy_tree_result$results$model_t2_belong_z\n\npolicy_tree_interpretation &lt;- margot_interpret_stability_batch(\n  policy_tree_result, \n  format = \"technical\", \n  label_mapping = label_mapping_all,\n  stability_threshold = 0.2\n)\n\ndevtools::load_all(\"/Users/joseph/GIT/margot/\")\n# Then use the enhanced results for interpretation\npolicy_text &lt;- margot_interpret_policy_batch(\n  policy_tree_result,\n  # models = policy_tree_result_stability_with_means,\n  original_df = original_df,\n  output_format = \"prose\",\n  label_mapping = label_mapping,\n  max_depth = 2L,\n  include_conditional_means = TRUE  # this should now work\n)    \ncat(policy_text)\n\n\npolicy_plots &lt;- margot_policy(\n  policy_tree_result,\n  original_df = original_df,\n  output_objects = c(\"combined_plot\"),\n  label_mapping = label_mapping,\n  max_depth = 2L,\n  seed = 2025\n)\npolicy_plots$model_t2_belong_z\npolicy_plots$model_t2_log_charity_donate_z\npolicy_plots$model_t2_support_z\n\n\n# will be same for all models\nvars_correlated &lt;- margot_assess_variable_correlation(models_analysis, model_name = hte_test_cv$selected_model_ids[[1]], label_mapping = label_mapping)\n\n# evaluate clusters\nidentify &lt;- margot_identify_variable_clusters(vars_correlated, label_mapping = label_mapping)\n\n# not working\ndiagnostics &lt;- margot_stability_diagnostics(\n  policy_tree_result,  # Your bootstrap results\n  model_results = models_binary_flipped_all#,     # Original causal forest results\n  # model_name =  \"model_t2_conscientiousness_z\"\n)\n\n\n\n# tests -------------------------------------------------------------------\n# push_mods\n# # test --------------------------------------------------------------------\n# n &lt;- nrow(X) # n in sample\n# \n# # define training sample\n# toy &lt;- sample(1:n, n / 4) # get half sample\n# \n# # test data\n# toy_data = df_grf[toy, ]\n# \n# # check\n# nrow(toy_data)\n# \n# # test covariates\n# X_toy = X[toy, ]\n# \n# # check\n# str(X_toy)\n# \n# # test exposure\n# W_toy = W[toy]\n# \n# # test weights\n# weights_toy = weights[toy]\n# \n# \n# \n# devtools::load_all(\"/Users/joseph/GIT/margot/\")\n# # devtools::load_all(\"/Users/joseph/GIT/margot/\")\n# t2_outcome_z\n# outcome_vars = t2_outcome_z\n# \n# t2_outcome_z\n# \n# # # test model\n# cf.test &lt;- margot_causal_forest(\n#   data = toy_data,\n#   outcome_vars = c(\"t2_log_charity_donate_z\",\"t2_log_hours_charity_z\"),\n#   flip_outcomes = \"t2_log_hours_charity_z\",\n#   covariates = X_toy,\n#   W = W_toy,\n#   # qini_split = TRUE,\n#   weights = weights_toy,\n#   grf_defaults = grf_defaults,\n#   save_data = TRUE,\n#   compute_marginal_only = FALSE,\n#   train_proportion  = 0.5,\n#   top_n_vars = 15,\n#   save_models = TRUE\n# )\n# cf.test$combined_table\n# \n# flip_outcomes = c(\n#   \"t2_log_hours_charity_z\"\n# )\n# \n# # save\n# \n# # get labels\n# flipped_names &lt;- margot_get_labels(flip_outcomes, label_mapping_all)\n# \n# # check\n# flipped_names\n# \n# # this is a new function requires margot 1.0.48 or higher\n# label_mapping_all_flipped &lt;- margot_reversed_labels(label_mapping_all, flip_outcomes)\n# \n# # view reversed labels\n# label_mapping_all_flipped\n# \n# \n# test_table &lt;- margot_plot(\n#   cf.test$combined_table,\n#   # &lt;- now pass the corrected results.\n#   options = outcomes_options_all,\n#   include_coefficients = FALSE,\n#   save_output = FALSE,\n#   order = \"evaluebound_asc\",\n#   label_mapping = label_mapping_all_flipped,\n#   original_df = original_df,\n#   e_val_bound_threshold = 1.1,\n#   rename_ate = TRUE,\n#   adjust = \"bonferroni\",  #&lt;- new\n#   alpha = 0.05 # &lt;- new\n# )\n# test_table$plot\n# test_table$interpretation\n# \n# # \n# # # flip model\n# # cf.test_flipped &lt;- margot_flip_forests(cf.test, \n# #                                        grf_defaults = grf_defaults,\n# #                                        flip_outcomes = flip_outcomes)\n# \n# model_test &lt;- cf.test\n# label_mapping &lt;- label_mapping_all \n# \n# devtools::load_all(\"/Users/joseph/GIT/margot/\")\n# # General HTE interpretation\n# test_test_hte_test &lt;- margot_interpret_heterogeneity(\n#   model_test, label_mapping = label_mapping) \n# \n# \n# #table\n# test_test_hte_test$evidence_summary\n# \n# cat(test_test_hte_test$interpretation)\n# cat(test_test_hte_test$extended_report)\n# test_test_hte_test$cv_results$cv_results\n# test_test_hte_test$rate_results$autoc\n# test_test_hte_test$rate_results$qini\n# test_test_hte_test$selected_model_ids\n# test_test_hte_test$all_selected_model_ids\n# \n# devtools::load_all(\"/Users/joseph/GIT/margot/\")\n# \n# policy_tree_result_stability_test &lt;- margot_policy_tree_stability(\n#   cf.test_flipped,\n#   model_names = het_test_cv$selected_model_ids,\n#   # exclude_covariates = c(\"t0_hlth_bmi_z\", \"_log\"), #&lt;- prone to error\n#   # custom_covariates = t0_name_exposure_continuous,  # &lt;-  include baseline exposure, but note it may not be a strong conditional predictor\n#   # covariate_mode =  \"add\",\n#   label_mapping = label_mapping_all_flipped,\n#   n_iterations = 100,\n#   train_proportion = .5,\n#   tree_method = \"fastpolicytree\",  # 10x faster!\n#   metaseed = 12345,\n#   depth = 2\n# )\n# \n# # save\n# here_save_qs(policy_tree_result_stability_test, \"policy_tree_result_stability_test\", push_mods)\n# \n# policy_tree_interpretation &lt;- margot_interpret_stability_batch(\n#   policy_tree_result_stability_test, \n#   format = \"technical\", \n#   label_mapping = label_mapping_all_flipped,\n#   stability_threshold = 0.2\n# )\n# \n# # \n# cat(policy_tree_interpretation)\n# \n# \n# \n# # Then use the enhanced results for interpretation\n# policy_text &lt;- margot_interpret_policy_batch(\n#   policy_tree_result_stability_test,\n#   # models = policy_tree_result_stability_with_means,\n#   original_df = original_df,\n#   output_format = \"prose\",\n#   label_mapping = label_mapping_all_flipped,\n#   max_depth = 2L,\n#   include_conditional_means = TRUE  # this should now work\n# )\n# \n# # And for plotting\n# policy_plots &lt;- margot_policy(\n#   policy_tree_result_stability_test,\n#   original_df = original_df,\n#   output_objects = c(\"combined_plot\"),\n#   label_mapping = label_mapping_all_flipped,\n#   max_depth = 2L\n# )\n# \n# policy_plots[[1]]\n# \n# \n# policy_text &lt;- margot_interpret_policy_batch(\n#   policy_tree_result_stability_test,\n#   # models            = policy_tree_result_stability,\n#   model_names = het_test_cv$selected_model_ids,\n#   original_df       = original_df,\n#   # model_names       = use_ids,\n#   output_format     =   \"prose\",\n#   label_mapping     = label_mapping_all_flipped,\n#   max_depth         = 2L\n# )\n# cat(policy_text)\n# \n# # plots\n# policy_plots &lt;- margot_policy(\n#   policy_tree_result_stability,\n#   original_df       = original_df,\n#   output_objects     = c(\"combined_plot\"),\n#   label_mapping     = label_mapping_all_flipped,\n#   max_depth         = 2L\n# )\n# \n# policy_plots$model_t2_conscientiousness_z\n# policy_plots$model_t2_honesty_humility_z\n# cat(policy_text)\n# \n# \n# \n\n\n# \n# sd_log &lt;- sd(original_df$t2_log_charity_donate, na.rm=TRUE)\n# \n# \n# delta_log &lt;- 0.14 * sd_log # say the effect was .14\n# ratio     &lt;- exp(delta_log)\n# pct_change &lt;- (ratio - 1) * 100\n# pct_change\n# # recover original dollars\n# dollars     &lt;- exp(original_df$t2_log_charity_donate) - 1\n# mean_dollars &lt;- mean(dollars, na.rm = TRUE)\n# \n# abs_change  &lt;- mean_dollars * (ratio - 1)\n# # print results\n# list(\n#   delta_log    = delta_log,\n#   pct_change   = pct_change,\n#   abs_change   = abs_change\n# )\n# \n# # 1. sd on the log(dollars + 1) scale\n# sd_log &lt;- sd(original_df$log_ch)\n# \n# # 2. effect in log‐units\n# delta_log &lt;- 0.14 * sd_log\n# \n# # 3. multiplicative change on dollars+1\n# ratio      &lt;- exp(delta_log)\n# pct_change &lt;- (ratio - 1) * 100  # percent increase\n# \n# # 4. absolute change in dollars (approximate, using the sample mean)\n# dollars      &lt;- exp(original_df$t2_log_charity_donate) - 1\n# mean_dollars &lt;- mean(dollars)\n# abs_change   &lt;- mean_dollars * (ratio - 1)\n# \n# # print results\n# list(\n#   delta_log    = delta_log,\n#   pct_change   = pct_change,\n#   abs_change   = abs_change\n# )\n# And for plotting\n\n\n\n\n\nModel specifications: - Forest parameters: 1,000 trees, minimum node size 20 - Sample splitting: 50% for CATE estimation to avoid overfitting - Top variables: 15 most important predictors for policy trees - Cross-validation: 5-fold CV for heterogeneity assessment"
  },
  {
    "objectID": "content/04-worked-example.html#key-results",
    "href": "content/04-worked-example.html#key-results",
    "title": "Worked Example: Religion → Cooperation",
    "section": "5 Key Results",
    "text": "5 Key Results\n\n5.1 Average Treatment Effects\nThe analysis reveals moderate positive effects of weekly religious service on cooperation:\nExpected findings (illustrative): - Charitable donations: +0.12 SD (95% CI: 0.08, 0.16) - E-value: 1.8 - Volunteering hours: +0.09 SD (95% CI: 0.05, 0.13) - E-value: 1.6 - Social support: +0.15 SD (95% CI: 0.11, 0.19) - E-value: 2.1 - Sense of belonging: +0.18 SD (95% CI: 0.14, 0.22) - E-value: 2.4\n\n\n5.2 Heterogeneity Analysis\nRATE-Qini curves identify meaningful heterogeneity for social outcomes: - Social support and belonging show strong heterogeneity (Qini &gt; 0.05) - Charitable behaviour shows moderate heterogeneity (Qini ≈ 0.03)\n\n\n5.3 Policy Trees: Actionable Insights\nHigh-responding subgroups identified through shallow policy trees:\n\nOlder adults (45+) with high agreeableness → largest cooperation gains\nParents with medium-high conscientiousness → strong volunteering response\n\nEmployed individuals with prior volunteering → sustained charitable giving"
  },
  {
    "objectID": "content/04-worked-example.html#interpretation",
    "href": "content/04-worked-example.html#interpretation",
    "title": "Worked Example: Religion → Cooperation",
    "section": "6 Interpretation",
    "text": "6 Interpretation\n\n6.1 Substantive Findings\nReligious service attendance causally increases cooperative behaviour with important nuances:\n\nUniversal benefits: All individuals show some cooperation increase\nHeterogeneous responses: Effects 2-3x larger for certain subgroups\nMechanism clarity: Stronger effects on social than charitable outcomes suggest community connection rather than prosocial orientation as primary pathway\n\n\n\n6.2 Policy Implications\nTargeted community interventions could leverage these insights: - Senior engagement programs - highest treatment response - Parent community groups - sustained volunteering effects - Workplace volunteering - builds on existing cooperative dispositions\n\n\n6.3 Robustness Considerations\nSensitivity analysis using E-values suggests: - Charitable donations robust to moderate confounding (E-value 1.8) - Social belonging robust to strong confounding (E-value 2.4)\n- Unmeasured confounders would need substantial effects to explain away results"
  },
  {
    "objectID": "content/04-worked-example.html#methodological-notes",
    "href": "content/04-worked-example.html#methodological-notes",
    "title": "Worked Example: Religion → Cooperation",
    "section": "7 Methodological Notes",
    "text": "7 Methodological Notes\n\n7.1 Causal Forest Advantages\n\nDoubly robust estimation - consistent under either outcome or treatment model misspecification\nHonest inference - sample splitting prevents overfitting bias\nInterpretable heterogeneity - policy trees provide actionable subgroup rules\n\n\n\n7.2 Limitations\n\nResidual confounding possible despite rich covariate control\nSynthetic data - real-world effects may differ in magnitude\nThree-wave design - longer follow-up would strengthen causal claims"
  },
  {
    "objectID": "content/04-worked-example.html#conclusion",
    "href": "content/04-worked-example.html#conclusion",
    "title": "Worked Example: Religion → Cooperation",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nThis analysis demonstrates that weekly religious service attendance causes meaningful increases in cooperative behaviour among New Zealand adults. The heterogeneous treatment effects suggest that community-based interventions could be most effective when targeted toward older adults, parents, and those with existing prosocial tendencies.\nThe margot workflow showcases how modern causal inference methods can provide both population-level estimates and actionable insights for policy and intervention design.\n\n\n\n\n\n\nTipWorkshop Learning Objectives Achieved\n\n\n\n✅ Causal question formulation - clear identification strategy\n✅ Modern ML methods - causal forests with sample splitting\n✅ Effect heterogeneity - RATE-Qini curves and policy trees\n✅ Robust inference - E-values for unmeasured confounding\n✅ Actionable insights - subgroup identification for interventions"
  },
  {
    "objectID": "content/04-worked-example.html#references",
    "href": "content/04-worked-example.html#references",
    "title": "Worked Example: Religion → Cooperation",
    "section": "9 References",
    "text": "9 References\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.\nBulbulia, J. A. (2025). margot: An R package for causal inference using machine learning. Retrieved from https://github.com/go-bayes/margot."
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Readings",
    "section": "",
    "text": "I’ve written four methods articles. Check the links for further readings. Also my lab resouces at the bottom 👇.\n\nPart 1: Causal Diagrams and Confounding\nPart 2: Interaction, Mediation, and Time-Varying Treatments\nPart 3: Measurement, Measurement Error, Cultural Comparisons\nPart 4: Experiments: What Randomisation Ensures and Does not Ensure\n\n\nFor more resources on causal inference, visit the EPIC Lab Resources page."
  },
  {
    "objectID": "reading.html#suggested-readings",
    "href": "reading.html#suggested-readings",
    "title": "Readings",
    "section": "",
    "text": "(Stuff I’ll review)\n\nPart 1: Causal Diagrams and Confounding\nPart 2: Interaction, Mediation, and Time-Varying Treatments\nPart 3: Measurement, Measurement Error, Cultural Comparisons\nPart 4: Experiments: What Randomisation Ensures and Does not Ensure\n\n\nFor more comprehensive resources on causal inference, practical workflows, and data visualisation, visit the EPIC Lab Resources page."
  },
  {
    "objectID": "reading.html#additional-lab-resources",
    "href": "reading.html#additional-lab-resources",
    "title": "Readings",
    "section": "",
    "text": "For more resources on causal inference, visit the EPIC Lab Resources page."
  },
  {
    "objectID": "simulations/01-simulation-population.html#simulations",
    "href": "simulations/01-simulation-population.html#simulations",
    "title": "Simulation: Population Estimate",
    "section": "",
    "text": "S2 and S3 are from the supplement to Bulbulia (2024b). S2 is a simulation. It illustrates how, if the distribution of effect-modifiers in one’s sample differs from that of the target population, the marginal effect estimate (ATE) will be biased. S3 offers a mathmatical explanation for this result. These simulations are followed by others that clarify how inference may fail when the true timing of events in one’s measures differs from what is assumed."
  },
  {
    "objectID": "simulations/01-simulation-population.html#id-app-b",
    "href": "simulations/01-simulation-population.html#id-app-b",
    "title": "Simulation: Population Estimate",
    "section": "S2. Generalisability and Transportability",
    "text": "S2. Generalisability and Transportability\nGeneralisability: When a study sample is drawn randomly from the target population, we may generalise from the sample to the target population as follows.\nSuppose we sample randomly from the target population, where:\n\nn_S denotes the size of the study’s analytic sample S.\nN_T denotes the total size of the target population T.\n\\widehat{ATE}_{n_S} denotes the estimated average treatment effect in the analytic sample S.\nATE_{T} denotes the true average treatment effect in the target population T.\n\\epsilon denotes an arbitrarily small positive value.\n\nAssuming the rest of the causal inference workflow goes to plan (randomisation succeeds, there is no measurement error, no model misspecification, etc.), as the random sample size n_S increases, the estimated treatment effect in the analytic sample S converges in probability to the true treatment effect in the target population T:\n\n\\lim_{n_S \\to \\infty} P(|\\widehat{ATE}_{n_S} - ATE_{T}| &lt; \\epsilon) = 1\n\nfor any small positive value of \\epsilon.\nTransportability: When the analytic sample is not drawn from the target population, we cannot directly generalise the findings. However, we can transport the estimated causal effect from the source population to the target population under certain assumptions. This involves adjusting for differences in the distributions of effect modifiers between the two populations. The closer the source population is to the target population, the more plausible the transportability assumptions are, and the less we need to rely on complex adjustment methods. Suppose we have an analytic sample n_S drawn from a source population S, and we want to estimate the average treatment effect in a target population T. Define:\n\\widehat{ATE}_{S} as the estimated average treatment effect in the analytic sample drawn from the source population S. \\widehat{ATE}_{T} as the estimated average treatment effect in the target population T. f(n_S, R) as the mapping function that adjusts the estimated effect in the analytic sample using a set of measured covariates R, allowing for valid projection from the source population to the target population.\nThe transportability assumption is that there exists a function f such that: \n\\widehat{ATE}_{T} = f(\\widehat{ATE}_S, R)\n\nHere, \\widehat{ATE}_S is the estimated average treatment effect from the source population, and R represents the set of covariates used for adjustment.\nFinding a suitable function f is the central challenge in adjusting for sampling bias and achieving transportability (Bareinboim and Pearl 2013; Westreich et al. 2017; Issa J. Dahabreh et al. 2019; Deffner, Rohrer, and McElreath 2022)."
  },
  {
    "objectID": "simulations/01-simulation-population.html#id-app-c",
    "href": "simulations/01-simulation-population.html#id-app-c",
    "title": "Simulation: Population Estimate",
    "section": "S3. A Mathematical Explanation for the Difference in Marginal Effects between Censored and Uncensored Populations",
    "text": "S3. A Mathematical Explanation for the Difference in Marginal Effects between Censored and Uncensored Populations\nThis appendix provides an explanation for why marginal effects may differ between the censored and uncensored sample population in the absence of unmeasured confounding.\n\nDefinitions:\n\nA: Exposure variable, where a represents the reference level and a^* represents the comparison level\nY: Outcome variable\nF: Effect modifier\nC: Indicator for the uncensored population (C = 0) or the censored population (C = 1)\n\n\n\nAverage Treatment Effects:\nThe average treatment effects for the uncensored and censored populations are defined as:\n \\Delta_{\\text{uncensored}} = \\mathbb{E}[Y(a^*) - Y(a) \\mid C = 0] \n \\Delta_{\\text{censored}} = \\mathbb{E}[Y(a^*) - Y(a) \\mid C = 1] \n\n\nPotential Outcomes:\nBy causal consistency, potential outcomes can be expressed in terms of observed outcomes:\n \\Delta_{\\text{uncensored}} = \\mathbb{E}[Y \\mid A=a^*, C=0] - \\mathbb{E}[Y \\mid A=a, C=0] \n \\Delta_{\\text{censored}} = \\mathbb{E}[Y \\mid A=a^*, C=1] - \\mathbb{E}[Y \\mid A=a, C=1] \n\n\nLaw of Total Probability:\nApplying the Law of Total Probability, we can weight the average treatment effects by the conditional probability of the effect modifier F:\n \\Delta_{\\text{uncensored}} = \\sum_{f} \\Big\\{\\mathbb{E}[Y \\mid A=a^*, F=f, C=0] - \\mathbb{E}[Y \\mid A=a, F=f, C=0]\\Big\\} \\times \\Pr(F=f \\mid C=0) \n\n\\Delta_{\\text{censored}} = \\sum_{f} \\Big\\{\\mathbb{E}[Y \\mid A=a^*, F=f, C=1] - \\mathbb{E}[Y \\mid A=a, F=f, C=1]\\Big\\} \\times \\Pr(F=f \\mid C=1)\n\n\n\nAssumption of Informative Censoring:\nWe assume that the distribution of the effect modifier F differs between the censored and uncensored populations:\n \\Pr(F=f \\mid C=0) \\neq \\Pr(F=f \\mid C=1) \nUnder this assumption, the probability weights used to calculate the marginal effects for the uncensored and censored populations differ.\n\n\nEffect Estimates for Censored and Uncensored Populations:\nGiven that \\Pr(F=f \\mid C=0) \\neq \\Pr(F=f \\mid C=1), we cannot guarantee that:\n\n\\Delta_{\\text{uncensored}} = \\Delta_{\\text{censored}}\n\nThe equality of marginal effects between the two populations will only hold if there is a universal null effect (i.e., no effect of the exposure on the outcome for any individual) across all units, by chance, or under specific conditions discussed by VanderWeele and Robins (2007) and further elucidated by Suzuki et al. (2013). Otherwise:\n \\Delta_{\\text{uncensored}} \\ne \\Delta_{\\text{censored}} \nFurthermore, VanderWeele (2012) proved that if there is effect modification of A by F, there will be a difference in at least one scale of causal contrast, such that:\n \\Delta^{\\text{risk ratio}}_{\\text{uncensored}} \\ne \\Delta^{\\text{risk ratio}}_{\\text{censored}} \nor\n \\Delta^{\\text{difference}}_{\\text{uncensored}} \\ne \\Delta^{\\text{difference}}_{\\text{censored}} \nFor comprehensive discussions on sampling and inference, refer to Issa J. Dahabreh and Hernán (2019) and Issa J. Dahabreh et al. (2021).  ## S4. R Simulation to Clarify Why The Distribution of Effect Modifiers Matters For Estimating Treatment Effects For A Target Population {#id-app-d}\nFirst, we load the stdReg library, which obtains marginal effect estimates by simulating counterfactuals under different levels of treatment (Sjölander 2016). If a treatment is continuous, the levels can be specified.\nWe also load the parameters library, which creates nice tables (Lüdecke et al. 2020).\n\n#|label: loadlibs\n\n# to obtain marginal effects\nif (!requireNamespace('stdReg', quietly = TRUE)) install.packages('stdReg')\nlibrary(stdReg)\n\n#  to view data\nif (!requireNamespace('skimr', quietly = TRUE)) install.packages('skimr')\nlibrary(skimr)\n\n# to create nice tables\nif (!requireNamespace('parameters', quietly = TRUE)) install.packages('parameters')\nlibrary(parameters)\n\nNext, we write a function to simulate data for the sample and target populations.\nWe assume the treatment effect is the same in the sample and target populations, that the coefficient for the effect modifier and the coefficient for interaction are the same, that there is no unmeasured confounding throughout the study, and that there is only selective attrition of one effect modifier such that the baseline population differs from the analytic sample population at the end of the study.\nThat is: the distribution of effect modifiers is the only respect in which the sample will differ from the target population.\nThis function will generate data under a range of scenarios. Refer to documentation in the margot package: Bulbulia (2024a)\n\n# function to generate data for the sample and population, \n# Along with precise sample weights for the population, there are differences \n# in the distribution of the true effect modifier but no differences in the treatment effect \n# or the effect modification. all that differs between the sample and the population is \n# the distribution of effect modifiers.\n\n# seed\nset.seed(123)\n\n# simulate data -- you can use different parameters\ndata &lt;- margot::simulate_ate_data_with_weights(\n  n_sample = 10000,\n  n_population = 100000,\n  p_z_sample = 0.1,\n  p_z_population = 0.5,\n  beta_a = 1,\n  beta_z = 2.5,\n  noise_sd = 0.5\n)\n\n# inspect\n# skimr::skim(data)\n\nWe have generated both sample and population data.\nNext, we verify that the distributions of effect modifiers differ in the sample and in the target population:\n\n# obtain the generated data\nsample_data &lt;- data$sample_data\npopulation_data &lt;- data$population_data\n\n# check imbalance\ntable(sample_data$z_sample) # type 1 is rare\n\n\n   0    1 \n9041  959 \n\ntable(population_data$z_population) # type 1 is common\n\n\n    0     1 \n49785 50215 \n\n\nThe sample and population distributions differ.\nNext, consider the question: ‘What are the differences in the coefficients that we obtain from the study population at the end of the study, compared with those we would obtain for the target population?’\nFirst, we obtain the regression coefficients for the sample. They are as follows:\n\n# model coefficients sample\nmodel_sample  &lt;- glm(y_sample ~ a_sample * z_sample, \n  data = sample_data)\n\n# summary\nparameters::model_parameters(model_sample, ci_method = 'wald')\n\nParameter           | Coefficient |       SE |        95% CI | t(9996) |      p\n-------------------------------------------------------------------------------\n(Intercept)         |   -2.10e-03 | 7.30e-03 | [-0.02, 0.01] |   -0.29 | 0.774 \na sample            |        1.00 |     0.01 | [ 0.98, 1.02] |   96.38 | &lt; .001\nz sample            |        2.53 |     0.02 | [ 2.49, 2.58] |  107.68 | &lt; .001\na sample × z sample |        0.45 |     0.03 | [ 0.39, 0.52] |   13.46 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nNext, we obtain the regression coefficients for the weighted regression of the sample. Notice that the coefficients are virtually the same:\n\n# model the sample weighted to the population, again note that these coefficients are similar \nmodel_weighted_sample &lt;- glm(y_sample ~ a_sample * z_sample, \n  data = sample_data, weights = weights)\n\n# summary\nsummary(parameters::model_parameters(model_weighted_sample, \n  ci_method = 'wald'))\n\nParameter           | Coefficient |        95% CI |      p\n----------------------------------------------------------\n(Intercept)         |   -2.10e-03 | [-0.02, 0.02] | 0.827 \na sample            |        1.00 | [ 0.97, 1.03] | &lt; .001\nz sample            |        2.53 | [ 2.51, 2.56] | &lt; .001\na sample × z sample |        0.45 | [ 0.41, 0.49] | &lt; .001\n\nModel: y_sample ~ a_sample * z_sample (10000 Observations)\nSigma: 0.483 (df = 9996)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nWe might be tempted to infer that weighting wasn’t relevant to the analysis. However, we’ll see that such an interpretation would be a mistake.\nNext, we obtain model coefficients for the population. Note again there is no difference – only narrower errors owing to the large sample size.\n\n# model coefficients population -- note that these coefficients are very similar. \nmodel_population &lt;- glm(y_population ~ a_population * z_population, \n  data = population_data)\n\nparameters::model_parameters(model_population, ci_method = 'wald')\n\nParameter                   | Coefficient |       SE |        95% CI | t(99996) |      p\n----------------------------------------------------------------------------------------\n(Intercept)                 |   -7.10e-04 | 3.19e-03 | [-0.01, 0.01] |    -0.22 | 0.824 \na population                |        1.00 | 4.49e-03 | [ 0.99, 1.01] |   222.88 | &lt; .001\nz population                |        2.50 | 4.49e-03 | [ 2.50, 2.51] |   557.23 | &lt; .001\na population × z population |        0.50 | 6.34e-03 | [ 0.49, 0.51] |    78.46 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nAgain, there is no difference. That is, we find that all model coefficients are practically equivalent. The different distribution of effect modifiers does not result in different coefficient values for the treatment effect, the effect-modifier ‘effect,’ or the interaction of the effect modifier and treatment.\nConsider why this is the case: in a large sample where the causal effects are invariant – as we have simulated them to be – we will have good replication in the effect modifiers within the sample, so our statistical model can recover the coefficients for the population without challenge.\nHowever, in causal inference, we are interested in the marginal effect of the treatment within a population of interest or within strata of this population. That is, we seek an estimate for the counterfactual contrast in which everyone in a pre-specified population or stratum of a population was subject to one level of treatment compared with a counterfactual condition in which everyone in a population was subject to another level of the same treatment.\nThe marginal effect estimates will differ in at least one measure of effect when the analytic sample population has a different distribution of effect modifiers compared to the target population.\nTo see this, we use the stdReg package to recover marginal effect estimates, comparing (1) the sample ATE, (2) the true oracle ATE for the population, and (3) the weighted sample ATE. We will use the outputs of the same models above. The only difference is that we will calculate marginal effects from these outputs. We will contrast a difference from an intervention in which everyone receives treatment = 0 with one in which everyone receives treatment = 1; however, this choice is arbitrary, and the general lessons apply irrespective of the estimand.\nFirst, consider this Average Treatment Effect for the analytic population:\n\n# What inference do we draw?  \n# we cannot say the models are unbiased for the marginal effect estimates. \n# regression standardisation \nlibrary(stdReg) # to obtain marginal effects \n\n# obtain sample ate\nfit_std_sample &lt;- stdReg::stdGlm(model_sample, \n  data = sample_data, X = 'a_sample')\n\n# summary\nsummary(fit_std_sample, contrast = 'difference', reference = 0)\n\n\nFormula: y_sample ~ a_sample * z_sample\nFamily: gaussian \nLink function: identity \nExposure:  a_sample \nReference level:  a_sample = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00    0.00000       0.00       0.00\n1     1.04    0.00996       1.02       1.06\n\n\nThe treatment effect is given as a 1.06 unit change in the outcome across the analytic population, with a confidence interval from 1.04 to 1.08.\nNext, we obtain the true (oracle) treatment effect for the target population under the same intervention:\n\n## note the population effect is different\n\n# obtain true ate\nfit_std_population &lt;- stdReg::stdGlm(model_population, \n  data = population_data, X = 'a_population')\n\n# summary\nsummary(fit_std_population, contrast = 'difference', reference = 0)\n\n\nFormula: y_population ~ a_population * z_population\nFamily: gaussian \nLink function: identity \nExposure:  a_population \nReference level:  a_population = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00    0.00000       0.00       0.00\n1     1.25    0.00327       1.24       1.26\n\n\nNote that the true treatment effect is a 1.25-unit change in the population, with a confidence bound between 1.24 and 1.26. This is well outside the ATE that we obtain from the analytic population!\nNext, consider the ATE in the weighted regression, where the analytic sample was weighted to the target population’s true distribution of effect modifiers:\n\n## next try weights adjusted ate where we correctly assign population weights to the sample\nfit_std_weighted_sample_weights &lt;- stdReg::stdGlm(model_weighted_sample, \n  data = sample_data, X = 'a_sample')\n\n# this gives us the right answer\nsummary(fit_std_weighted_sample_weights, contrast = 'difference', reference = 0)\n\n\nFormula: y_sample ~ a_sample * z_sample\nFamily: gaussian \nLink function: identity \nExposure:  a_sample \nReference level:  a_sample = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00     0.0000       0.00       0.00\n1     1.22     0.0165       1.19       1.25\n\n\nWe find that we obtain the population-level causal effect estimate with accurate coverage by weighting the sample to the target population. So with appropriate weights, our results generalise from the sample to the target population."
  },
  {
    "objectID": "simulations/01-simulation-population.html#lessons",
    "href": "simulations/01-simulation-population.html#lessons",
    "title": "Simulation: Population Estimate",
    "section": "Lessons",
    "text": "Lessons\n\nRegression coefficients do not clarify the problem of sample/target population mismatch — or selection bias as discussed in this manuscript.\nInvestigators should not rely on regression coefficients alone when evaluating the biases that arise from sample attrition. This advice applies to both methods that authors use to investigate threats of bias. To implement this advice, authors must first take it themselves.\nObserved data are generally insufficient for assessing threats. Observed data do not clarify structural sources of bias, nor do they clarify effect-modification in the full counterfactual data condition where all receive the treatment and all do not receive the treatment (at the same level).\nTo properly assess bias, one needs access to the counterfactual outcome — what would have happened to the missing participants had they not been lost to follow-up or had they responded? The joint distributions over ‘full data’ are inherently unobservable (Van Der Laan and Rose 2011).\nIn simple settings, like the one we just simulated, we can address the gap between the sample and target population using methods such as modelling the censoring (e.g., censoring weighting). However, we never know what setting we are in or whether it is simple—such modelling must be handled carefully. There is a large and growing epidemiology literature on this topic (see, for example, Li, Miao, and Tchetgen Tchetgen (2023))."
  },
  {
    "objectID": "simulations/01-simulation-population.html#ambiguities-from-cross-sectional-data",
    "href": "simulations/01-simulation-population.html#ambiguities-from-cross-sectional-data",
    "title": "Simulation: Population Estimate",
    "section": "Ambiguities from Cross-Sectional Data",
    "text": "Ambiguities from Cross-Sectional Data\n\nMethodology\nData Generation: we simulate a dataset for 1,000 individuals, where e.g. degree of religious belief (A) influences wealth (L), which in turn affects charitable donations (Y$). The simulation is based on predefined parameters that establish L as a mediator between A and Y.\nParameter Definitions:\n\nThe probability of access to green space (A) is set at 0.5.\nThe effect of A on L (exercise) is given by \\beta = 2.\nThe effect of L on Y (happiness) is given by \\delta = 1.5.\nStandard deviations for L and Y are set at 1 and 1.5, respectively.\n\nModel 1 (Correct Assumption): fits a linear regression model assuming L as a mediator, including both A and L as regressors on Y. This model aligns with the data-generating process, and, by the rules of d-separation, induces mediator bias for the A\\to Y path.\nModel 2 (Incorrect Assumption): fits a linear regression model including only A as a regressor on Y, omitting the mediator L. This model assesses the direct effect of A on Y without accounting for mediation.\nAnalysis: We compares the estimated effects of A on Y under each model specification.\n\n# load libraries\n!require(kableExtra)){install.packages(\"kableExtra\")} # tables\nif(!require(gtsummary)){install.packages(\"gtsummary\")} # tables\n\n# simulation seed\nset.seed(123) #  reproducibility\n\n# define the parameters \nn = 1000 # Number of observations\np = 0.5  # Probability of A = 1 (exposure)\nalpha = 0 # Intercept for L (mediator)\nbeta = 2  # Effect of A on L \ngamma = 1 # Intercept for Y \ndelta = 1.5 # Effect of L on Y\nsigma_L = 1 # Standard deviation of L\nsigma_Y = 1.5 # Standard deviation of Y\n\n# simulate the data: fully mediated effect by L\nA = rbinom(n, 1, p) # binary exposure variable\nL = alpha + beta*A + rnorm(n, 0, sigma_L) # mediator L affect by A\nY = gamma + delta*L + rnorm(n, 0, sigma_Y) # Y affected only by L,\n\n# make the data frame\ndata = data.frame(A = A, L = L, Y = Y)\n\n# fit regression in which we control for L, a mediator\n# (cross-sectional data is consistent with this model)\nfit_1 &lt;- lm( Y ~ A + L, data = data)\n\n# fit regression in which L is assumed to be a mediator, not a confounder.\n# (cross-sectional data is also consistent with this model)\nfit_2 &lt;- lm( Y ~ A, data = data)\n\n# create gtsummary tables for each regression model\ntable1 &lt;- gtsummary::tbl_regression(fit_1)\ntable2 &lt;- gtsummary::tbl_regression(fit_2)\n\n# merge the tables for comparison\ntable_comparison &lt;- gtsummary::tbl_merge(\n  list(table1, table2),\n  tab_spanner = c(\"Model: Exercise assumed confounder\", \n                  \"Model: Exercise assumed to be a mediator\")\n)\n# make html table (for publication)\nmarkdown_table_0 &lt;- as_kable_extra(table_comparison, \n                                   format = \"html\")\n# print table (note, you might prefer \"markdown\" or another format)                                \nmarkdown_table_0\n\nThe following code is designed to estimate the Average Treatment Effect (ATE) using the clarify package in R, which is referenced here as (Greifer et al. 2023). The procedure involves two steps: simulating coefficient distributions for regression models and then calculating the ATE based on these simulations. This process is applied to two distinct models to demonstrate the effects of including versus excluding a mediator variable in the analysis.\n\n\nSteps to Estimate the ATE\n\nLoad the clarify Package: this package provides functions to simulate regression coefficients and compute average marginal effects (AME), robustly facilitating the estimation of ATE.\nSet seed: set.seed(123) ensures that the results of the simulations are reproducible, allowing for consistent outcomes across different code runs.\nSimulate the data distribution:\nsim_coefs_fit_1 and sim_coefs_fit_2 are generated using the sim function from the clarify package, applied to two fitted models (fit_1 and fit_2). These functions simulate the distribution of coefficients based on the specified models, capturing the uncertainty around the estimated parameters.\nCalculate ATE:\n\nFor both models, the sim_ame function calculates the ATE as the marginal risk difference (RD) when the treatment variable (A) is present (A == 1). This function uses the simulated coefficients to estimate the treatment effect across the simulated distributions, providing a comprehensive view of the ATE under each model.\nTo streamline the output, the function is set to verbose mode off (verbose = FALSE).\n\nResults:\n\nSummaries of these estimates (summary_sim_est_fit_1 and summary_sim_est_fit_2) are obtained, providing detailed statistics including the estimated ATE and its 95% confidence intervals (CI).\n\nPresentation: report ATE and CIs:\n\nUsing the glue package, the ATE and its 95% CIs for both models are formatted into a string for easy reporting. This step transforms the statistical output into a more interpretable form, highlighting the estimated treatment effect and its precision.\n\n# use `clarify` package to obtain ATE\nif(!require(clarify)){install.packages(\"clarify\")} # clarify package\n# simulate fit 1 ATE\nset.seed(2025)\nsim_coefs_fit_1 &lt;- sim(fit_1)\nsim_coefs_fit_2 &lt;- sim(fit_2)\n\n# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)\nsim_est_fit_1 &lt;-\n  sim_ame(\n    sim_coefs_fit_1,\n    var = \"A\",\n    subset = A == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)\nsim_est_fit_2 &lt;-\n  sim_ame(\n    sim_coefs_fit_2,\n    var = \"A\",\n    subset = A == 1,\n    contrast = \"RD\",\n    verbose = FALSE\n  )\n# obtain summaries\nsummary_sim_est_fit_1 &lt;- summary(sim_est_fit_1, null = c(`RD` = 0))\nsummary_sim_est_fit_2 &lt;- summary(sim_est_fit_2, null = c(`RD` = 0))\n\n# reporting \n# ate for fit 1, with 95% CI\nATE_fit_1 &lt;- glue::glue(\n  \"ATE =\n                        {round(summary_sim_est_fit_1[3, 1], 2)},\n                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},\n                        {round(summary_sim_est_fit_1[3, 3], 2)}]\"\n)\n# ate for fit 2, with 95% CI\nATE_fit_2 &lt;-\n  glue::glue(\n    \"ATE = {round(summary_sim_est_fit_2[3, 1], 2)},\n                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},\n                        {round(summary_sim_est_fit_2[3, 3], 2)}]\"\n  )\n\n\n\nUpshot of the Simulation and Analysis\n\nModel 1 (L as a Confounder): this analysis assumes that L is a confounder in the relationship between the treatment (A) and the outcome (Y), and thus, it includes L in the model. The ATE estimated here reflects the effect of A while controlling for L.\nModel 2 (L as a Mediator): in contrast, this analysis considers L to be a mediator, and the model either includes L explicitly in its estimation process or excludes it to examine the direct effect of A on Y. The approach to mediation analysis here is crucial as it influences the interpretation of the ATE.\n\nBy comparing the ATEs from both models, researchers can understand the effect of mediation (or the lack thereof) on the estimated treatment effect. This comparison sheds light on how assumptions about variable roles (confounder vs. mediator) can significantly alter causal inferences drawn from cross-sectional data.\nWherever it is uncertain whether a variable is a confounder or a mediator, we suggest creating two causal diagrams and reporting both analyses."
  },
  {
    "objectID": "index.html#page-contents",
    "href": "index.html#page-contents",
    "title": "Beyond Averages: Discovering Who Benefits from Religion?",
    "section": "Page Contents",
    "text": "Page Contents\nOn this page you’ll find:\n\nTechnical Setup - R, RStudio, and package installation\nWorkshop Agenda - Detailed schedule and learning goals\n\nWorkshop Objectives - What you’ll learn and prerequisites\nWorkshop Presentations - Case study PDFs (ATE and CATE)\nReference Materials - Links to glossary and DAGs\n\nQuick links: Setup | Agenda | Presentations\n\nInstall R and R Studio\nVisit the r archive network (cran) at https://cran.r-project.org/ Select the version of r suitable for your operating system (windows, mac, or linux) Download and install it.\nThen install RStudio: https://www.rstudio.com/products/rstudio/download/ Choose the free version of rstudio desktop,\nThen install the workshop package. The margot package is optional.\n# Needed for installation\nif (!requireNamespace(\"devtools\", quietly = TRUE)) {\n  install.packages(\"devtools\")\n}\n\n# Simulations and analysis\ndevtools::install_github(\"go-bayes/causalworkshop\")\n\n# Install margot for advanced workflows\n# warning, this package is being refactored: for demonstration purposes only.\ndevtools::install_github(\"go-bayes/margot\")\n# explanations\ndevtools::install_github(\"go-bayes/boilerplate\")\nThis will install causalworkshop along with all required dependencies.\nAfter installation, verify everything works:\n# load the package\nlibrary(causalworkshop)\n\n# check prerequisites\ncheck_workshop_prerequisites()\n\n\nNext run get_workshop_scripts() to get the scripts. Run them in order.\nlibrary(causalworkshop)\n\n# copy workshop scripts to a working directory\n# default will create a directory called `workshop-scripts`\nget_workshop_scripts()\n\n# you can set a different path, e.g.\n# get_workshop_scripts(\"different-directory-name\")\n\n\n# see what scripts are available\nlist_workshop_scripts()\n\n# or if you placed them in a directory other thna the default `workshop-scripts` directory\nlist_workshop_scripts(\"different-directory-name\")\n\n\n# Work through the scripts in order\n# 01-baseline-adjustment.R    - Generate data\n# 02-causal-forest-analysis.R - Simple causal forest\n# 03-rate-qini-curves.R       -  Evaluate heterogeneity using RATE metrics\n# 04-policy-trees.R           - Decision rules\n# 05-margot-workflow.R        - Better Data Viz\n:::"
  },
  {
    "objectID": "reading.html#suggested-readings-stuff-ill-review",
    "href": "reading.html#suggested-readings-stuff-ill-review",
    "title": "Readings",
    "section": "",
    "text": "Part 1: Causal Diagrams and Confounding\nPart 2: Interaction, Mediation, and Time-Varying Treatments\nPart 3: Measurement, Measurement Error, Cultural Comparisons\nPart 4: Experiments: What Randomisation Ensures and Does not Ensure\n\n\nFor more comprehensive resources on causal inference methods, data visualisation, and practical workflows, visit the EPIC Lab Resources page."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Overview",
    "text": "Overview\nSocial psychological theories aim for causal explanations, yet our evidence is often associational. This full-day, practice-oriented workshop introduces modern causal inference for observational data, with an emphasis on discovering for whom effects are strongest (afternoon).\nHere, we will:\n\ndiagnose confounding and other threats to causal claims in observational studies.\n\nuse the potential outcomes framework and causal diagrams to keep assumptions explicit.\n\ndemonstrate state-of-the-art estimators: doubly robust machine learning using causal forests.\ntranslate workflow principles into hands-on R practice."
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Objectives",
    "text": "Objectives\nParticipants will leave with:\n\nA conceptual toolkit for articulating and evaluating causal claims.\n\nAnnotated R code for simulating and estimating average and heterogeneous treatment effects.\n\nA curated reading list for self-study.\n\nExamples of how to communicate causal results to academic, policy, and organisational audiences.\n\nThe workshop assumes familiarity with regression and R, but no prior training in causal inference.\n\n\n\n\n\n\nTipA note on pace\n\n\n\nPlease do not worry if the math or code looks intimidating at first glance. We’ll slow everything down, build each idea together from first principles, and keep the focus on intuition before symbols. Bring your curiosity, and a computer pre-loaded with the workshop R package if you want to work through the data exercises. By the end of the day you’ll see that the machinery is manageable, and even fun (really), when we unpack it step by step."
  },
  {
    "objectID": "index.html#technical-setup",
    "href": "index.html#technical-setup",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Technical setup",
    "text": "Technical setup\n\nInstall the current version of R (free) from https://cran.r-project.org/.\n\nInstall RStudio Desktop (free) from https://posit.co/download/rstudio-desktop/.\n\nInstall the workshop packages:\n\nif (!requireNamespace(\"devtools\", quietly = TRUE)) {\n  install.packages(\"devtools\")\n}\n\ndevtools::install_github(\"go-bayes/causalworkshop\")\n\n# Optional packages for extended demonstrations\ndevtools::install_github(\"go-bayes/margot\")\ndevtools::install_github(\"go-bayes/boilerplate\")\nOnce installed, run:\nlibrary(causalworkshop)\ncheck_workshop_prerequisites()\nDownload the reproducible scripts:\nlibrary(causalworkshop)\n\nget_workshop_scripts()  # copies scripts into ./workshop-scripts\nlist_workshop_scripts()\nWork through scripts 01–05 in order to rehearse the workflow we will cover together."
  },
  {
    "objectID": "index.html#workshop-materials",
    "href": "index.html#workshop-materials",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Workshop materials",
    "text": "Workshop materials\n\nThe lectures are somewhat technical at the moment. I’ll be simplifying them before the workshop and posting YouTube videos prior to the workshop. Stay tuned here for updates.\nStudent presentations will be posted here after the workshop.\n\nPrevious talks and examples remain available for reference; we will link to them from the relevant sections."
  },
  {
    "objectID": "index.html#stay-in-touch",
    "href": "index.html#stay-in-touch",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Stay in touch",
    "text": "Stay in touch\nQuestions before the workshop? Email me at joseph.bulbulia@vuw.ac.nz. Updates will follow via the conference mailing list and this page."
  },
  {
    "objectID": "content/00-background.html#longitudinal-data-are-not-enough-we-must-ask-when-is-time-zero.",
    "href": "content/00-background.html#longitudinal-data-are-not-enough-we-must-ask-when-is-time-zero.",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Longitudinal data are not enough: We must ask: when is TIME ZERO.",
    "text": "Longitudinal data are not enough: We must ask: when is TIME ZERO.\nTemporal ordering was {precisely the problem} with the observational hormone studies in the 80s/90s that modelled .\nHow?"
  },
  {
    "objectID": "content/00-background.html#causal-workflow-check-list",
    "href": "content/00-background.html#causal-workflow-check-list",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "CAUSAL WORKFLOW CHECK LIST",
    "text": "CAUSAL WORKFLOW CHECK LIST\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n\n\n\n\n\nelement\ndefinition\n\n\n\n\nEligibility\nPost-menopausal women aged 50–79, no prior CHD\n\n\nTreatment\nInitiate oestrogen + progestin on day of Rx pickup\n\n\nComparison\nNo hormone-therapy initiation on that day\n\n\nOutcome\nAll-cause mortality\n\n\nFollow-up\n8 years or until death / loss to follow-up\n\n\n\n\n\n\nTable 1: Target-trial specification for the hormone-therapy example."
  },
  {
    "objectID": "content/00-background.html#part-1-motivating-example",
    "href": "content/00-background.html#part-1-motivating-example",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Part 1: Motivating Example",
    "text": "Part 1: Motivating Example\n\n1990s observational studies indicated 30% all-cause mortality reduction from estrogen therapies?\n\n\n\n\n\n\nNoteIn the 1980s and 1990s estrogen treatments appeared to benefit postmenopausal women\n\n\n\nHazard ratio for all-cause mortality: 0.68 current users vs. never users (Grodstein, Manson, and Stampfer 2006)\n\n\n\n\nStandard Medical Advice\n\n1992 American College of Obstetricians and Gynecologists {“Probable beneficial effect of estrogens on heart disease.”}\n1992 American College of Physicians “Women who have coronary heart disease or who are at increased risk of coronary heart disease are likely to benefit from hormone therapy.”}\n1993 National Cholesterol Education Program {“Epidemiologic evidence for the benefit of estrogen replacement therapy is especially strong for secondary prevention in women with prior CHD.”}\n1996 American Heart Association {“ERT does look promising as a long-term protection against heart attack.”}\n\n\n\nWomen’s Health Initiative: Evaluate Estrogens Experimentally\n\nMassive randomised, double-blind, placebo-controlled trial\n16,000 U.S. women aged 50-79 years\nAssigned to Estrogen plus Progestin therapy\nWomen followed approximately every year for up to 8 years.\n\n\n\nFindings: Clear Discrepancy\n\n\n\n\n\n\nWarningExperimental Findings: opposite of observational findings\n\n\n\nAll-cause mortality Hazard Ratio: 1.23 for initiators vs non-initiators. (Manson et al. 2003)\n\n\n\n\nMedical community response: Reject all observational studies\n\nCan observational studies ever be trusted?\nShould observational studies ever be funded again?\n{What went wrong?}\n\n\n\nOpening\n\nRobert Frost writes:\n\nTwo roads diverged in a yellow wood,\nAnd sorry I could not travel both\nAnd be one traveler, long I stood\nAnd looked down one as far as I could\nTo where it bent in the undergrowth;\nThen took the other, as just as fair,\nAnd having perhaps the better claim,\nBecause it was grassy and wanted wear;\nThough as for that the passing there\nHad worn them really about the same,\nAnd both that morning equally lay\nIn leaves no step had trodden black.\nOh, I kept the first for another day!\nYet knowing how way leads on to way,\nI doubted if I should ever come back.\nI shall be telling this with a sigh\nSomewhere ages and ages hence:\nTwo roads diverged in a wood, and I—\nI took the one less traveled by,\nAnd that has made all the difference.\n– The Road Not Taken\n\n\n\n\nIntroduction: Motivating Example\nConsider the following question:\n\nAlice attends religious service regularly. Does this increase her volunteering?\n\nThere is evidence that people who attend religious volunteer more, but would Alice volunteer anyway?\n“And sorry I could not travel both. And be one traveler \\dots”"
  },
  {
    "objectID": "content/00-background.html#motivating-example-the-failure-had-nothing-to-do-with-causal-assumptions-but-rather-with-setting-up-the-data.",
    "href": "content/00-background.html#motivating-example-the-failure-had-nothing-to-do-with-causal-assumptions-but-rather-with-setting-up-the-data.",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Motivating Example: The failure had nothing to do with causal assumptions but rather with setting up the data.",
    "text": "Motivating Example: The failure had nothing to do with causal assumptions but rather with setting up the data.\n\nLongitudinal data are not enough: We must ask: when is TIME ZERO.\nTemporal ordering was {precisely the problem} with the observational hormone studies in the 80s/90s that modelled .\nHow?\n\n\nResearchers failed to emulate an experiment in their data (Target Trial)\n\nWomen’s Health Initiative: overall hazard ratio 1.23 (0.99, 1.53)\nWomen’s Health Initiative: when broken down by years to follow-up:\n\n0-2 years 1.51 (1.06, 2.14)\n2-5 years 1.31 (0.93, 1.83)\n5 or more years 0.67 (0.41, 1.09)\n\n\n\n\n\n\n\n\nNoteSurvivor Bias.\n\n\n\nThe observational results can be \n\n\n\n\n\n\n\n\nNoteEmulating a target trial with observational data recovers experimental effects.\n\n\n\nRe-modelling initiation into hormone therapy recovers experimental findings (Hernán et al. 2016).\n\n\n\n\nVisualising the When Error\n\n\n\n\n\n\n\n\nFigure 1: Three ways to start the clock. Only one is right.\n\n\n\n\n\n\n\nKey Take Home: Set Up Your Study Like an Experiment\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n\n\n\n\n\nelement\ndefinition\n\n\n\n\nEligibility\nPost-menopausal women aged 50–79, no prior CHD\n\n\nTreatment\nInitiate oestrogen + progestin on day of Rx pickup\n\n\nComparison\nNo hormone-therapy initiation on that day\n\n\nOutcome\nAll-cause mortality\n\n\nFollow-up\n8 years or until death / loss to follow-up\n\n\n\n\n\n\nTable 1: Target-trial specification for the hormone-therapy example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "content/00-background.html#motivating-example-at-the-start-the-failure-had-nothing-to-do-with-causal-assumptions-but-rather-with-setting-up-the-data.",
    "href": "content/00-background.html#motivating-example-at-the-start-the-failure-had-nothing-to-do-with-causal-assumptions-but-rather-with-setting-up-the-data.",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Motivating Example At The Start: The failure had nothing to do with causal assumptions but rather with setting up the data.",
    "text": "Motivating Example At The Start: The failure had nothing to do with causal assumptions but rather with setting up the data.\n\nLongitudinal data are not enough: We must ask: when is TIME ZERO.\nTemporal ordering was {precisely the problem} with the observational hormone studies in the 80s/90s that modelled .\nHow?\n\n\nResearchers failed to emulate an experiment in their data (Target Trial)\n\nWomen’s Health Initiative: overall hazard ratio 1.23 (0.99, 1.53)\nWomen’s Health Initiative: when broken down by years to follow-up:\n\n0-2 years 1.51 (1.06, 2.14)\n2-5 years 1.31 (0.93, 1.83)\n5 or more years 0.67 (0.41, 1.09)\n\n\n\n\n\n\n\n\nNoteSurvivor Bias.\n\n\n\nThe observational results can be \n\n\n\n\n\n\n\n\nNoteEmulating a target trial with observational data recovers experimental effects.\n\n\n\nRe-modelling initiation into hormone therapy recovers experimental findings (Hernán et al. 2016).\n\n\n\n\nVisualising the When Error\n\n\n\n\n\n\n\n\nFigure 1: Three ways to start the clock. Only one is right.\n\n\n\n\n\n\n\nKey Take Home: Set Up Your Study Like an Experiment\n\n\n\n\n\n\n\n\nelement\ndefinition\n\n\n\n\nEligibility\nPost-menopausal women aged 50–79, no prior CHD\n\n\nTreatment\nInitiate oestrogen + progestin on day of Rx pickup\n\n\nComparison\nNo hormone-therapy initiation on that day\n\n\nOutcome\nAll-cause mortality\n\n\nFollow-up\n8 years or until death / loss to follow-up\n\n\n\n\n\n\nTable 1: Target-trial specification for the hormone-therapy example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "content/02-causal-workflow.html#the-causal-workflow",
    "href": "content/02-causal-workflow.html#the-causal-workflow",
    "title": "The Causal Workflow",
    "section": "The Causal Workflow",
    "text": "The Causal Workflow\n\nState a well-defined treatment.\nSpecify the hypothetical intervention precisely enough that every member of the target population could, in principle, receive it. For example, ‘weight loss’ is too vague—people lose weight via exercise, diet, depression, cancer, amputation, and more (Hernan and Robins 2020). A clearer intervention is: “engage in vigorous physical activity for \\ge 30 minutes per day” (Miguel A. Hernán et al. 2008). Precision here underwrites consistency (see step 5) and interpretability downstream.\nState a well-defined outcome.\nDefine the outcome so the causal contrast is meaningful and temporally anchored. ‘Well-being’ is underspecified; ‘psychological distress one year post-intervention measured with the Kessler-6’ is interpretable and reproducible (Kessler et al. 2002). Include timing, scale, and instrument.\nClarify the target population.\nSay exactly who you aim to inform. Eligibility rules define the source population, but sampling and participation can yield a study population with a different distribution of effect modifiers (Issa J. Dahabreh and Hernán 2019; Issa J. Dahabreh et al. 2019; Stuart, Ackerman, and Westreich 2018; Bulbulia 2024). If you intend to generalise beyond the source population (transport), articulate the additional conditions and knowledge required (Deffner, Rohrer, and McElreath 2022; Bareinboim and Pearl 2013; Westreich et al. 2017; Issa J. Dahabreh and Hernán 2019; Pearl and Bareinboim 2022).\nEvaluate whether treatment groups are exchangeable given measured covariates.\nMake the case that potential outcomes are independent of treatment conditional on covariates, i.e., Y(a)\\coprod A\\mid X (Neal 2020; Morgan and Winship 2014; Angrist and Pischke 2009; Hernan and Robins 2020). Use design and diagnostics (design diagrams/DAGs, subject-matter arguments, pre-treatment covariate balance, overlap checks). If exchangeability is doubtful, redesign (e.g., stronger measurement, alternative identification strategies) rather than rely solely on modelling.\nEnsure treatments to be compared satisfy causal consistency.\nConsistency requires that, for units receiving a treatment version compatible with level a, the observed outcome equals Y(a); it also presumes well-defined versions and no interference between units (Tyler J. VanderWeele and Hernan 2013; Hernan and Robins 2020). When multiple versions exist, either refine the intervention so versions are irrelevant to Y(a), or condition on version-defining covariates in ways that preserve your estimand.\nCheck the positivity (overlap) assumption.\nEach treatment level must occur with non-zero probability at every covariate profile needed for exchangeability—and, when versions exist, for consistency as well (Westreich and Cole 2010). Diagnose limited overlap (propensity score distributions, extreme weights), and consider design-stage remedies (trimming, restriction, adaptive sampling) before estimation.\nEnsure measurement aligns with the scientific question.\nVerify that constructs are captured by instruments whose error structures won’t distort the causal contrast of interest. Be explicit about likely forms of measurement error (classical, Berkson, differential, misclassification) and their structural implications for bias (Hernan and Robins 2020; Tyler J. VanderWeele and Hernán 2012; Bulbulia 2024). Where feasible, incorporate validation studies, multiple indicators, or calibration models.\nPreserve representativeness from start to finish.\nEnd-of-study analyses should reflect the target population’s distribution of effect modifiers. Differential attrition, non-response, or measurement processes tied to treatment and outcomes can induce selection bias in the presence of true effects (M. A. Hernán 2017; Miguel A. Hernán and Robins 2017; Miguel A. Hernán, Hernández-Díaz, and Robins 2004; Bulbulia 2024). Plan and justify strategies such as inverse probability weighting for censoring, multiple imputation under defensible mechanisms, sensitivity analyses for missing not at random, and careful timing of measurements.\nDocument the reasoning that supports steps 1–8.\nMake assumptions, disagreements, and judgement calls legible: register or otherwise time-stamp your analytic plan; include identification arguments (e.g., DAGs), code, and data where possible; report robustness and sensitivity analyses; and explain decisions about design restrictions, modelling choices, and transportability (Ogburn and Shpitser 2021). Transparent reasoning is a scientific result in its own right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackages\n\n\nCode\nreport::cite_packages()\n\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/02-causal-workflow.html#introduction-to-causal-diagrams.",
    "href": "content/02-causal-workflow.html#introduction-to-causal-diagrams.",
    "title": "Causal Diagramms (DAGs) and The Causal Workflow",
    "section": "Introduction to Causal Diagrams.",
    "text": "Introduction to Causal Diagrams.\nCausal diagrams, also called causal graphs, Directed Acyclic Graphs, and Causal Directed Acyclic Graphs, are graphical tools whose primary purpose is to enable investigators to detect confounding biases.\nRemarkably, causal diagrams are rarely used in psychology!\nBefore describing how causal diagrams work, we first define the meanings of their symbols. Note there is no single convention for creating causal diagrams, so it is important that we are clear when defining our meanings.\n\nThe meaning of our symbols\nThe conventions that describe the meanings of our symbols are given in Figure 1.\n\n\n\n\n\n\nFigure 1: Our variable naming conventions. This figure is adapted from (Bulbulia 2024a)\n\n\n\nFor us:\nX denotes a variable without reference to its role;\nA denotes the “treatment” or “exposure” variable. This is the variable for which we seek to understand the effect of intervening on it. It is the “cause;”\nY denotes the outcome or response of an intervention. It is the “effect.” Last week we considered whether marriage A causes happiness Y.\nY(a) denotes the counterfactual or potential state of Y in response to setting the level of the exposure to a specific level, A=a. As we will consider in the second half of the course, to consistently estimate causal effects we will need to evaluate counterfactual or potential states of the world. Keeping to our example, we will need to do more than evaluate marriage and happiness in people over time. We will need to evaluate how happy the unmarried people would have been had they been married and how happy the married people would have been had they not been married. Of course, these events cannot be directly observed. Thus to address fundamental questions in psychology, we need to contrast counterfactual states of the world. This might seem like science fiction; however, we are already familiar with methods for obtaining such counterfactual contrasts – namely, randomised controlled experiments! We will return to this concept later, but for now, it will be useful for you to understand the notation.\nL denotes a measured confounder or set of confounders is defined as a variable which, if conditioned upon, closes an open back-door path between the treatment A and the outcome Y. Consider the scenario where happiness at time 0 (L) affects both the probability of getting married at time 1 (A) and one’s happiness at time 2 (Y). In this case, L serves as a confounder because it influences both the treatment (marriage at time 1) and the outcome (happiness at time 2), potentially opening a back-door path that confounds the estimated effect of marriage on happiness.\nTo accurately estimate the causal effect of marriage on happiness, then, it is essential to control for L. With cross-sectional data, such control might be difficult.\nU denotes an unmeasured confounder – that is a variable that may affect both the treatment and the outcome, but for which we have no direct measurement. Suppose cultural upbringing affects both whether someone gets married and whether they are happy. If this variable is not measured, we cannot accurately estimate a causal effect of marriage on happiness.\nM denotes a mediator or a variable along the path from exposure to outcome. For example, perhaps marriage causes wealth and wealth causes happiness. As we shall see, conditioning on “wealth” when estimating the effect of marriage on happiness will make it seem that marriage does not cause happiness when it does, through wealth.\n\\bar{X} denotes a sequence of variables, for example, a sequence of treatments. Imagine we were interested in the causal effect of marriage and remarriage on well-being. In this case, there are two treatments A_0 and A_1 and four potential contrasts. For the scenario of marriage and remarriage affecting well-being, we denote the potential outcomes as Y(a_0, a_1), where a_0 and a_1 represent the specific values taken by A_0 and A_1, respectively. Given two treatments, A_0 and A_1, four primary contrasts of interest correspond to the different combinations of these treatments. These contrasts allow us to compare the causal effects of being married versus not and remarried versus not on well-being. The potential outcomes under these conditions can be specified as follows:\n\nY(0, 0): The potential outcome when there is no marriage.\nY(0, 1): The potential outcome when there is marriage.\nY(1, 0): The potential outcome when there is divorce.\nY(1, 1): The potential outcome from marriage prevalence.\n\nEach of these outcomes allows for a specific contrast to be made, comparing the well-being under different scenarios of marriage and remarriage. Which do we want to contrast? Note, the question about ‘the causal effects of marriage on happiness’ is ambiguous because we have not stated the causal contrast we are interested in.\n\\mathcal{R} denotes a randomisation or a chance event.\n\n\nElements of our Causal Graphs\nThe conventions that describe components of our causal graphs are given in Figure 2.\n\n\n\n\n\n\nFigure 2: Nodes, Edges, Conditioning Conventions. This figure is adapted from (Bulbulia 2024a)\n\n\n\n\nTime indexing\nIn our causal diagrams, we will implement two conventions to accurately depict the temporal order of events.\nFirst, the layout of a causal diagram will be structured from left to right to reflect the sequence of causality as it unfolds in reality. This orientation is crucial because causal diagrams must inherently be acyclic and because causality itself is inherently temporal.\nSecond, we will enhance the representation of the event sequence within our diagrams by systematically indexing our nodes according to the relative timing of events. If an event represented by X_0 precedes another event represented by X_1, the indexing will indicate this chronological order.\n\n\nRepresenting uncertainty in timing explicitly\nIn settings in which the sequence of events is ambiguous or cannot be definitively known, particularly in the context of cross-sectional data where all measurements are taken at a single point in time, we adopt a specific convention to express causality under uncertainty: X_{\\phi t}. This notation allows us to propose a temporal order without clear, time-specific measurements, acknowledging our speculation.\nFor instance, when the timing between events is unclear, we denote an event that is presumed to occur first as X_{\\phi 0} and a subsequent event as X_{\\phi 1}, indicating a tentative ordering where X_{\\phi 0} is thought to precede X_{\\phi 1}. However, it is essential to underscore that this notation signals our uncertainty regarding the actual timing of events; our measurements do not give us the confidence to assert this sequence definitively.\n\n\nArrows\nAs indicated in Figure 2, black arrows denote causality, red arrows reveal an open backdoor path, dashed black arrows denote attenuation, and red dashed arrows denote bias in a true causal association between A and Y. Finally, a blue arrow with a circle point denotes effect-measure modification, also known as “effect modification.” We might be interested in treatment effect heterogeneity without evaluating the causality in the sources of this heterogeneity. For example, we cannot typically imagine any intervention in which people could be randomised into cultures. However, we may be interested in whether the effects of an intervention that might be manipulable, such as marriage, differ by culture. To clarify this interest, we require a non-causal arrow.\n\\mathcal{R}\\to A denotes a random treatment assignment.\n\n\nBoxes\nWe use a black box to denote conditioning that reduces confounding or that is inert.\nWe use a red box to describe settings in which conditioning on a variable introduces confounding bias.\nOccasionally we will use a dashed circle do denote a latent variable, that is, a variable that is either not measured or not conditioned upon.\n\n\nTerminology for Conditional Independence\nThe bottom panel of Figure 2 shows some mathematical notation. Do not be alarmed, we are safe! Part 1 of the course will not require more complicated math than this notation. And we shall see that the notation is a compact way to describe intuitions that can be expressed less compactly in words:\n\nStatistical Independence (\\coprod): in the context of causal inference, statistical independence between the treatment and potential outcomes, denoted as A \\coprod Y(a), means the treatment assignment is independent of the potential outcomes. This assumption is critical for estimating causal effects without bias.\nStatistical Dependence (\\cancel\\coprod): conversely, \\cancel\\coprod denotes statistical dependence, indicating that the distribution of one variable is influenced by the other. For example, A \\cancel\\coprod Y(a) implies that the treatment assignment is related to the potential outcomes, potentially introducing bias into causal estimates.\nConditioning (|): conditioning, denoted by the vertical line |, allows for specifying contexts or conditions under which independence or dependence holds.\n\nConditional Independence (A \\coprod Y(a)|L): This means that once we account for a set of variables L, the treatment and potential outcomes are independent. This condition is often the basis for strategies aiming to control for confounding.\nConditional Dependence (A \\cancel\\coprod Y(a)|L): States that potential outcomes and treatments are not independent after conditioning on L, indicating a need for careful consideration in the analysis to avoid biased causal inferences."
  },
  {
    "objectID": "content/02-causal-workflow.html#the-five-elementary-structures-of-causality",
    "href": "content/02-causal-workflow.html#the-five-elementary-structures-of-causality",
    "title": "Causal Diagramms (DAGs) and The Causal Workflow",
    "section": "The Five Elementary Structures of Causality",
    "text": "The Five Elementary Structures of Causality\nJudea Pearl proved that all elementary structures of causality can be represented graphically (Pearl 2009). Figure 3 presents this five elementary structures.\n\n\n\n\n\n\nFigure 3: Five elementary structures. This figure is adapted from (Bulbulia 2024a).\n\n\n\nThe structures are as follows:\n\nTwo Variables:\n\nCausality Absent: There is no causal effect between variables A and B. They do not influence each other, denoted as A \\coprod B, indicating they are statistically independent.\nCausality: Variable A causally affects variable B. This relationship suggests an association between them, denoted as A \\cancel\\coprod B, indicating they are statistically dependent.\n\nThree Variables:\n\nFork: Variable A causally affects both B and C. Variables B and C are conditionally independent given A, denoted as B \\coprod C | A. This structure implies that knowing A removes any association between B and C due to their common cause.\nChain: A causal chain exists where C is affected by B, which in turn is affected by A. Variables A and C are conditionally independent given B, denoted as A \\coprod C | B. This indicates that B mediates the effect of A on C, and knowing B breaks the association between A and C.\nCollider: Variable C is affected by both A and B, which are independent. However, conditioning on C induces an association between A and B, denoted as A \\cancel\\coprod B | C. This structure is unique because it suggests that A and B, while initially independent, become associated when we account for their common effect C.\n\n\nOnce we understand the basic relationships between two variables, we can build upon these to create more complex relationships. These structures help us see how statistical independences and dependencies emerge from the data, allowing us to clarify the causal relationships we presume exist. Such clarity is crucial for ensuring that confounders are balanced across treatment groups, given all measured confounders, so that Y(a) \\coprod A | L.\nYou might wonder, “If not from the data, where do our assumptions about causality come from?” This question will come up repeatedly throughout the course. The short answer is that our assumptions are based on existing knowledge. This reliance on current knowledge might seem counterintuitive for buiding scientific knowledge-— shouldn’t we use data to build knowledge, not the other way around? Yes, but it is not that straightforward. Data often hold the answers we’re looking for but can be ambiguous. When the causal structure is unclear, it is important to sketch out different causal diagrams, explore their implications, and, if necessary, conduct separate analyses based on these diagrams.\nOtto Neurath, an Austrian philosopher and a member of the Vienna Circle, famously used the metaphor of a ship that must be rebuilt at sea to describe the process of scientific theory and knowledge development.\n\nDuhem has shown … that every statement about any happening is saturated with hypotheses of all sorts and that these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. (Neurath 1973, 199)\n\nThis quotation emphasises the iterative process that accumulates scientific knowledge; new insights are cast from the foundation of existing knowledge. Causal diagrams are at home in Neurath’s boat. The tradition of science that believes that knowledge develops from the results of statistical tests applied to data should be resisted. The data alone typically do not contain the answers we seek."
  },
  {
    "objectID": "content/02-causal-workflow.html#the-four-rules-of-confounding-control",
    "href": "content/02-causal-workflow.html#the-four-rules-of-confounding-control",
    "title": "Causal Diagramms (DAGs) and The Causal Workflow",
    "section": "The Four Rules of Confounding Control",
    "text": "The Four Rules of Confounding Control\nFigure 4 describe the four elementary rules of confounding control:\n\n\n\n\n\n\nFigure 4: Four rules of confounding control\n\n\n\n\nCondition on Common Cause or its Proxy: this rule applies to settings in which the treatment (A) and the outcome (Y) share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps tp isolate the specific effect of A on Y. (We do not draw a path from $ A Y$ because we do not assume this path.)\nDo Not Condition on a Mediator: this rule applies to settings in which the variable L is a mediator of A \\to Y. Here, conditioning on a mediator will bias the total causal effect estimate. Later in the course, we will discuss the assumptions required for causal mediation. For now, if we are interested in total effect estimates, we must not condition on a mediator. Here we draw the path from A \\to Y to ensure that if such a path exists, it will not become biased from our conditioning strategy.\nDo Not Condition on a Collider: this rule applies to settings in which we L is a common effect of A and Y. Conditioning on a collider may invoke a spurious association. Last week we considered an example in which marriage caused wealth and happiness caused wealth. Conditioning on wealth in this setting will induce an association between happiness and marriage. Why? If we know the outcome, wealth, then we know there are at least two ways of wealth. Among those wealthy but low on happiness, we can predict that they are more likely to be married, for how else would they be wealthy? Similarly, among those who are wealthy and are not married, we can predict that they are happy, for how else would they be wealthy if not through marriage? These relationships are predictable entirely without a causal association between marriage and happiness!\nProxy Rule: Conditioning on a Descendent Is Akin to Conditioning on Its Parent: this rule applies to settings in which we L’ is an effect from another variable L. The graph considers when L’ is downstream of a collider. For example, suppose we condition on home ownership, which is an effect of wealth. Such conditioning will open up a non-causal path without causation because home ownership is a proxy for wealth. Consider, if someone owns a house but is not married, they are more likely to be happy, for how else could they accumulate the wealth required for home ownership? Likewise, if someone is unhappy and owns a house, we can infer that they are more likely to be married because how else would they be wealthy? Conditioning on a proxy for a collider here is akin to conditioning on the collider itself.\n\nHowever, we can also use the proxy rule to reduce bias. Return to the earlier example in which there is an unmeasured common cause of marriage and happiness, which we called “cultural upbringing” Suppose we have not measured this variable but have measured proxies for this variable, such as country of birth, childhood religion, number of languages one speaks, and others. By controlling for baseline values of these proxies, we can exert more control over unmeasured confounding. Even if bias is not eliminated, we should reduce bias wherever possible, which includes not introducing new biases, such as mediator bias, along the way. Later in the course, we will teach you how to perform sensitivity analyses to verify the robustness of your results to unmeasured confounding. Sensitivity analysis is critical because where the data are observational, we cannot entirely rule out unmeasured confounding."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Contact",
    "text": "Contact\nQuestions before the workshop? Email me at joseph.bulbulia@vuw.ac.nz."
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Setup",
    "text": "Setup\n\nInstall the current version of R (free) from https://cran.r-project.org/\nInstall RStudio Desktop (free) from https://posit.co/download/rstudio-desktop/\n\n(Or pick your favourite IDE – I use neovim Lazy – the code doesn’t care.)\n\nInstall the workshop packages:\n\nif (!requireNamespace(\"devtools\", quietly = TRUE)) {\n  install.packages(\"devtools\")\n}\n\ndevtools::install_github(\"go-bayes/causalworkshop\")\n\n# optional packages for extended demonstrations\ndevtools::install_github(\"go-bayes/margot\")\nOnce installed, run:\nlibrary(causalworkshop)\ncheck_workshop_prerequisites()\nDownload the R scripts:\nlibrary(causalworkshop)\n\nget_workshop_scripts()  # copies scripts into ./workshop-scripts\nlist_workshop_scripts()\nWork through scripts 01–05 Prior to the workshop (06 is optional)."
  },
  {
    "objectID": "content/ggdag-examples.html",
    "href": "content/ggdag-examples.html",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "",
    "text": "Note\n\n\n\nRequired\n\n(Bulbulia 2024) link\nsee simplified reading\n\nOptional\n\n(Hernan and Robins 2020) Chapter 6-9 link\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(VanderWeele and Hernán 2012) link"
  },
  {
    "objectID": "content/ggdag-examples.html#load-libraries",
    "href": "content/ggdag-examples.html#load-libraries",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Load libraries",
    "text": "Load libraries\nWe can use the ggdag package to evaluate confounding .\n\nOmitted Variable Bias Causal Graph\nLet’s use ggdag to identify confounding arising from omitting Z in our regression of X on Y.\nFirst we write out the DAG as follows:\n\n# code for creating a DAG\ngraph_fork &lt;- dagify(Y ~ L,\n                   A ~ L,\n                   exposure = \"A\",\n                   outcome = \"Y\") |&gt;\n  tidy_dagitty(layout = \"tree\")\n\n# plot the DAG\ngraph_fork |&gt;\n  ggdag() + theme_dag_blank() + labs(title = \"L is a common cause of A and Y\")\n\n\n\n\n\n\n\n\nNext we ask ggdag which variables we need to include if we are to obtain an unbiased estimate of the outcome from the exposure:\n\n# use this code\n\nggdag::ggdag_adjustment_set( graph_fork ) +  theme_dag_blank() + labs(title = \"{L} is the exclusive member of the confounder set for A and Y. Conditioning on L 'd-separates' A and Y \")\n\n\n\n\n\n\n\n\nThe causal graph tells us to obtain an unbiased estimate of A on Y we must condition on L.\nAnd indeed, when we included the omitted variable L in our simulated dateset it breaks the association between X and Y:\n\n# set seed\nset.seed(123)\n\n# number of observations\nN = 1000\n\n# confounder\nL = rnorm(N)\n\n# A is caused by \nA = rnorm(N, L)\n\n# Y draws randomly from L but is not caused by A\nY = rnorm(N, L)\n\n# note we did not need to make a data frame\n\n# regress Y on A without control\nfit_fork  &lt;- lm(Y ~ A)\n\n# A is \"significant\nparameters::model_parameters(fit_fork)\n\nParameter   | Coefficient |   SE |        95% CI | t(998) |      p\n------------------------------------------------------------------\n(Intercept) |       -0.03 | 0.04 | [-0.11, 0.04] |  -0.89 | 0.373 \nA           |        0.50 | 0.03 | [ 0.45, 0.54] |  19.72 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n# regress Y on A with control\nfit_fork_controlled  &lt;- lm(Y ~ A + L)\n\n# A and Y are no longer associated, conditioning worked.\nparameters::model_parameters(fit_fork_controlled)\n\nParameter   | Coefficient |   SE |        95% CI | t(997) |      p\n------------------------------------------------------------------\n(Intercept) |       -0.02 | 0.03 | [-0.08, 0.04] |  -0.68 | 0.499 \nA           |        0.03 | 0.03 | [-0.03, 0.09] |   0.89 | 0.372 \nL           |        0.95 | 0.05 | [ 0.86, 1.04] |  20.77 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation."
  },
  {
    "objectID": "content/ggdag-examples.html#mediation-and-causation",
    "href": "content/ggdag-examples.html#mediation-and-causation",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Mediation and causation",
    "text": "Mediation and causation\nSuppose we were interested in the causal effect of X on Y. We have a direct effect of X on Y as well as an indirect effect of X on Y through M. We use ggdag to draw the DAG:\n\ngraph_mediation &lt;- dagify(Y ~  M,\n                 M ~ A,\n                exposure = \"A\",\n                outcome = \"Y\") |&gt;\n  ggdag::tidy_dagitty(layout = \"tree\")\n\ngraph_mediation |&gt;\n  ggdag() +   \n  theme_dag_blank() + \n  labs(title = \"Mediation Graph\")\n\n\n\n\n\n\n\n\nHere is another way\n\ngraph_mediation_full &lt;- ggdag_mediation_triangle(x = \"A\", \n                         y = \"Y\", \n                         m = \"M\", \n                         x_y_associated = FALSE) \n\n\ngraph_mediation_full +  theme_dag_blank() + \n  labs(title = \"Fully Mediated Graph\")\n\n\n\n\n\n\n\n\nWhat should we condition on if we are interested in the causal effect of changes in X on changes in Y?\nWe can pose the question to ggdag:\n\n# ask ggdag which variables to condition on:\n\nggdag::ggdag_adjustment_set(graph_fork)\n\n\n\n\n\n\n\n\n‘Backdoor Paths Unconditionally Closed’ means that, assuming the DAG we have drawn is correct, we may obtain an unbiased estimate of X on Y without including additional variables.\nLater we shall understand why this is the case.1\n1 We shall see there is no “backdoor path” from X to Y that would bias our estimate, hence the estimate X-&gt;Y is an unbiased causal estimate – again, conditional on our DAG.For now, we can enrich our language for causal inference by considering the concept of d-connected and d-separated:\nTwo variables are d-connected if information flows between them (condional on the graph), and they are d-separated if they are conditionally independent of each other.\n\n# use this code to examine d-connectedness\nggdag::ggdag_dconnected(graph_mediation)\n\n\n\n\n\n\n\n\nIn this case, d-connection is a good thing because we can estimate the causal effect of A on Y.\nIn other cases, d-connection will spoil the model. We have seen this for omitted variable bias. A and Y are d-separated conditional on L, and that’s our motivation for including L. These concepts are tricky, but they get easier with practice.\nTo add some grit to our exploration of mediation lets simulate data that are consistent with our mediation DAG\n\nset.seed(123)\nN &lt;- 100\nx &lt;- rnorm(N)# sim x\nm &lt;- rnorm(N , x) # sim X -&gt; M\ny &lt;- rnorm(N , x + m) # sim M -&gt; Y\ndf &lt;- data.frame(x, m, y)\n\ndf &lt;- df |&gt;\n  dplyr::mutate(x_s = scale(x),\n                m_s = scale(m))\n\nFirst we ask, is X is related to Y?\n\nfit_mediation &lt;- lm(y ~ x_s, data = df)\nparameters::model_parameters(fit_mediation)\n\nParameter   | Coefficient |   SE |        95% CI | t(98) |      p\n-----------------------------------------------------------------\n(Intercept) |        0.19 | 0.14 | [-0.08, 0.47] |  1.41 | 0.161 \nx s         |        1.66 | 0.14 | [ 1.38, 1.93] | 12.00 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nYes.\nNext we ask, is A related to Y conditional on M?\n\nfit_total_mediated_effect &lt;- lm(y ~ x_s + m_s, data = df)\nparameters::model_parameters(fit_total_mediated_effect) |&gt; parameters::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n0.19\n0.10\n(4.92e-03, 0.38)\n2.04\n0.044\n\n\nx s\n0.77\n0.13\n(0.51, 1.02)\n6.00\n&lt; .001\n\n\nm s\n1.33\n0.13\n(1.07, 1.58)\n10.34\n&lt; .001\n\n\n\n\n\n\n\nYes, but notice this is a different question. The effect of X is attenuated because M contributes to the causal effect of Y.\n\nfit_total_effect &lt;- lm(y ~ x_s, data = df)\nparameters::model_parameters(fit_total_effect) |&gt; parameters::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(98)\np\n\n\n\n\n(Intercept)\n0.19\n0.14\n(-0.08, 0.47)\n1.41\n0.161\n\n\nx s\n1.66\n0.14\n(1.38, 1.93)\n12.00\n&lt; .001"
  },
  {
    "objectID": "content/ggdag-examples.html#pipe-confounding-full-mediation",
    "href": "content/ggdag-examples.html#pipe-confounding-full-mediation",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Pipe confounding (full mediation)",
    "text": "Pipe confounding (full mediation)\nSuppose we are interested in the effect of x on y, in a scenario when m fully mediates the relationship of x on y.\n\nmediation_triangle(\n  x = NULL,\n  y = NULL,\n  m = NULL,\n  x_y_associated = FALSE\n) |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nWhat variables do we need to include to obtain an unbiased estimate of X on Y?\nLet’s fill out this example out by imagining an experiment.\nSuppose we want to know whether a ritual action condition (X) influences charity (Y). We have good reason to assume the effect of X on Y happens entirely through perceived social cohesion (M):\nX\\toM\\toZ or ritual \\to social cohesion \\to charity\nLets simulate some data\n\nset.seed(123)\n# Participants\nN &lt;-100\n\n# initial charitable giving\nc0 &lt;- rnorm(N ,10 ,2)\n\n# assign treatments and simulate charitable giving and increase in social cohesion\nritual &lt;- rep( 0:1 , each = N/2 )\ncohesion &lt;- ritual * rnorm(N,.5,.2)\n\n# increase in charity\nc1 &lt;- c0 + ritual * cohesion \n\n# dataframe\nd &lt;- data.frame( c0 = c0 , \n                 c1=c1 , \n                 ritual = ritual , \n                 cohesion = cohesion )\nskimr::skim(d)\n\n\n\n\n\nName\nd\n\n\nNumber of rows\n100\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nc0\n0\n1\n10.18\n1.83\n5.38\n9.01\n10.12\n11.38\n14.37\n▁▃▇▅▂\n\n\nc1\n0\n1\n10.43\n1.85\n5.89\n9.17\n10.30\n11.57\n14.99\n▂▆▇▆▂\n\n\nritual\n0\n1\n0.50\n0.50\n0.00\n0.00\n0.50\n1.00\n1.00\n▇▁▁▁▇\n\n\ncohesion\n0\n1\n0.25\n0.29\n0.00\n0.00\n0.12\n0.48\n1.15\n▇▃▃▁▁\n\n\n\n\n\nDoes the ritual increase charity?\nIf we only include the ritual condition in the model, we find that ritual condition reliable predicts increases in charitable giving:\n\nparameters::model_parameters(\n  lm(c1 ~  c0 + ritual, data = d)\n  )\n\nParameter   | Coefficient |       SE |        95% CI |  t(97) |      p\n----------------------------------------------------------------------\n(Intercept) |        0.08 |     0.08 | [-0.07, 0.23] |   1.05 | 0.297 \nc0          |        0.99 | 7.26e-03 | [ 0.98, 1.01] | 136.74 | &lt; .001\nritual      |        0.51 |     0.03 | [ 0.46, 0.56] |  19.33 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nDoes the ritual increase charity adjusting for levels of social cohesion?\n\nparameters::model_parameters(\n  lm(c1 ~  c0 + ritual + cohesion, data = d)\n  )\n\nParameter   | Coefficient |       SE |         95% CI |    t(96) |      p\n-------------------------------------------------------------------------\n(Intercept) |   -5.94e-15 | 6.11e-16 | [ 0.00,  0.00] |    -9.72 | &lt; .001\nc0          |        1.00 | 5.89e-17 | [ 1.00,  1.00] | 1.70e+16 | &lt; .001\nritual      |    3.80e-16 | 4.68e-16 | [ 0.00,  0.00] |     0.81 | 0.419 \ncohesion    |        1.00 | 8.19e-16 | [ 1.00,  1.00] | 1.22e+15 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nThe answer is that the (direct) effect of ritual entirely drops out when we include both ritual and social cohesion. Why is this? The answer is that once our model knows m it does not obtain any new information by knowing x.\nIf we were interested in assessing x\\toy but x were to effect y through m (i.e x\\tom\\toy) then conditioning on m would block the path from x\\toy. Including m leads to Pipe Confounding.\nIn experiments we should never condition on a post-treatment variable."
  },
  {
    "objectID": "content/ggdag-examples.html#masked-relationships",
    "href": "content/ggdag-examples.html#masked-relationships",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Masked relationships",
    "text": "Masked relationships\nImagine two variables were to affect an outcome. Both are correlated with each other. One affects the outcome positively and the other affects the outcome negatively. How shall we investigate the causal role of the focal predictor?\nConsider two correlated variables that jointly predict Political conservatism (C), religion (R). Imagine that one variable has a positive effect and the other has a negative effect on distress (K6).\nFirst consider this relationship, where conservatism causes religion\n\nlibrary(ggdag)\ndag_m1 &lt;- dagify(K ~ C + R,\n                 R ~ C,\n                 exposure = \"C\",\n                 outcome = \"K\") |&gt;\n  tidy_dagitty(layout = \"tree\")\n\n# graph\ndag_m1|&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nWe can simulate the data:\n\n# C -&gt; K &lt;- R\n# C -&gt; R\nset.seed(123)\nn &lt;- 100\nC &lt;- rnorm( n )\nR &lt;- rnorm( n , C )\nK &lt;- rnorm( n , R - C )\n\nd_sim &lt;- data.frame(K=K,R=R,C=C)\n\nFirst we only condition on conservatism\n\nms1 &lt;- parameters::model_parameters(\n  lm(K  ~ C, data = d_sim)\n)\nplot(ms1)\n\n\n\n\n\n\n\nms1\n\nParameter   | Coefficient |   SE |        95% CI | t(98) |     p\n----------------------------------------------------------------\n(Intercept) |        0.03 | 0.14 | [-0.24, 0.30] |  0.22 | 0.829\nC           |       -0.19 | 0.15 | [-0.49, 0.11] | -1.24 | 0.219\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nNext, only religion:\n\nms2&lt;- parameters::model_parameters(\n  lm(K  ~ R, data = d_sim)\n)\nplot(ms2)\n\n\n\n\n\n\n\n\nWhen we add both C and R, we see them “pop” in opposite directions, as is typical of masking:\n\nms3&lt;- parameters::model_parameters(\n  lm(K  ~ C + R, data = d_sim)\n)\nplot(ms3)\n\n\n\n\n\n\n\n\nNote that when you ask ggdag to assess how to obtain an unbiased estimate of C on K it will tell you you don’t need to condition on R.\n\ndag_m1|&gt;\n  ggdag_adjustment_set()\n\n\n\n\n\n\n\n\nYet recall when we just assessed the relationship of C on K we got this:\n\nplot(ms1)\n\n\n\n\n\n\n\n\nIs the DAG wrong?\nNo. The fact that C\\toR is positive and R\\toK is negative means that if we were to increase C, we wouldn’t reliably increase K. The total effect of C just isn’t reliable"
  },
  {
    "objectID": "content/ggdag-examples.html#collider-confounding",
    "href": "content/ggdag-examples.html#collider-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Collider Confounding",
    "text": "Collider Confounding\nThe selection-distortion effect (Berkson’s paradox)\nThis example is from the book Statistical Rethinking. Imagine in science there is no relationship between the newsworthiness of science and its trustworthiness. Imagine further that selection committees make decisions on the basis of the both newsworthiness and the trustworthiness of scientific proposals.\nThis presents us with the following graph\n\ndag_sd &lt;- dagify(S ~ N,\n                 S ~ T,\n                 labels = c(\"S\" = \"Selection\",\n                            \"N\" = \"Newsworthy\",\n                            \"T\" = \"Trustworthy\")) |&gt;\n  tidy_dagitty(layout = \"nicely\")\n\n# Graph\ndag_sd |&gt;\n  ggdag(text = FALSE, use_labels = \"label\") + theme_dag_blank()\n\n\n\n\n\n\n\n\nWhen two arrows enter into an variable, it opens a path of information between the two variables.\nVery often this openning of information has disasterous implications. In the human sciences, included variable bias is a woefully underrated problem.\n\nggdag_dseparated(\n  dag_sd,\n  from = \"T\",\n  to = \"N\",\n  controlling_for = \"S\",\n  text = FALSE,\n  use_labels = \"label\"\n) + theme_dag_blank()\n\n\n\n\n\n\n\n\nWe can use the ggdag package to find colliders among our variables:\n\n# code for finding colliders\n\nggdag::ggdag_collider(dag_sd,\n                      text = FALSE,\n                      use_labels = \"label\")\n\n\n\n\n\n\n\n\nThe following simulation (by Solomon Kurz) illustrates the selection-distortion effect, which Richard McElreath discusses in Statistical Rethinking:\nFirst simulated uncorrelated variables and a process of selection for sub-populations score high on both indicators.\n\n# simulate selection distortion effect, following Solomon Kurz\n# https://bookdown.org/content/4857/the-haunted-dag-the-causal-terror.html\nset.seed(123)\nn &lt;- 1000  # number of grant proposals\np &lt;- 0.05  # proportion to select\n\nd &lt;-\n  # uncorrelated newsworthiness and trustworthiness\n  dplyr::tibble(\n    newsworthiness  = rnorm(n, mean = 0, sd = 1),\n    trustworthiness = rnorm(n, mean = 0, sd = 1)\n  ) |&gt;\n  # total_score\n  dplyr::mutate(total_score = newsworthiness + trustworthiness) |&gt;\n  # select top 10% of combined scores\n  dplyr::mutate(selected = ifelse(total_score &gt;= quantile(total_score, 1 - p), TRUE, FALSE))\n\nNext filter out the high scoring examples, and assess their correlation.\nNote that the act of selection induces a correlation within our dataset.\n\nd |&gt; \n  dplyr::filter(selected == TRUE) |&gt; \n  dplyr::select(newsworthiness, trustworthiness) |&gt; \n  cor()\n\n                newsworthiness trustworthiness\nnewsworthiness       1.0000000      -0.7318408\ntrustworthiness     -0.7318408       1.0000000\n\n\nThis makes it seems as if there is a relationship between Trustworthiness and Newsworthiness in science, even when there isn’t any.\n\n# we'll need this for the annotation\nlibrary(ggplot2)\ntext &lt;-\n  dplyr::tibble(\n    newsworthiness  = c(2, 1),\n    trustworthiness = c(2.25, -2.5),\n    selected        = c(TRUE, FALSE),\n    label           = c(\"selected\", \"rejected\")\n  )\n\nd |&gt;\n  ggplot2::ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +\n  ggplot2::geom_point(aes(shape = selected), alpha = 3 / 4) +\n  ggplot2::geom_text(data = text,\n            aes(label = label)) +\n  ggplot2::geom_smooth(\n    data = d |&gt; filter(selected == TRUE),\n    method = \"lm\",\n    fullrange = T,\n    color = \"lightblue\",\n    se = F,\n    size = 1\n  ) +\n  # scale_color_manual(values = c(\"black\", \"lightblue\")) +\n  ggplot2::scale_shape_manual(values = c(1, 19)) +\n  ggplot2::scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +\n  ggplot2::coord_cartesian(ylim = range(d$trustworthiness)) +\n  ggplot2::theme(legend.position = \"none\") +\n  ggplot2::xlab(\"Newsworthy\") +\n  ggplot2::ylab(\"Trustworthy\") + theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOnce we know a proposal has been selected, if it is newsworthy we can predict that it is less trustworthy. Our simulation produces this prediction even though we simulated a world in which there is no relationship between trustworthiness and newsworthiness.\nSelection bias is commonplace."
  },
  {
    "objectID": "content/ggdag-examples.html#collider-bias-within-experiments",
    "href": "content/ggdag-examples.html#collider-bias-within-experiments",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Collider bias within experiments",
    "text": "Collider bias within experiments\nWe noted that conditioning on a post-treatment variable can induce bias by blocking the path between the experimental manipulation and the outcome. However, such conditioning can open a path even when there is no experimental effect.\n\ndag_ex2 &lt;- dagify(\n  C1 ~ C0 + U,\n  Ch ~ U + R,\n  labels = c(\n    \"R\" = \"Ritual\",\n    \"C1\" = \"Charity-post\",\n    \"C0\" = \"Charity-pre\",\n    \"Ch\" = \"Cohesion\",\n    \"U\" = \"Religiousness (Unmeasured)\"\n  ),\n  exposure = \"R\",\n  outcome = \"C1\",\n  latent = \"U\"\n) |&gt;\n  control_for(c(\"Ch\",\"C0\"))  \n\ndag_ex2 |&gt;\n  ggdag( text = FALSE,\n    use_labels = \"label\")\n\n\n\n\n\n\n\n\nHow do we avoid collider-bias here?\nNote what happens if we condition on cohesion?\n\ndag_ex2 |&gt;\n  ggdag_collider(\n    text = FALSE,\n    use_labels = \"label\"\n  )  +\n  ggtitle(\"Cohesion is a collider that opens a path from ritual to charity\")\n\n\n\n\n\n\n\n\nDon’t condition on a post-treatment variable!\n\ndag_ex3 &lt;- dagify(\n  C1 ~ C0,\n  C1 ~ U,\n  Ch ~ U + R,\n  labels = c(\n    \"R\" = \"Ritual\",\n    \"C1\" = \"Charity-post\",\n    \"C0\" = \"Charity-pre\",\n    \"Ch\" = \"Cohesion\",\n    \"U\" = \"Religiousness (Unmeasured)\"\n  ),\n  exposure = \"R\",\n  outcome = \"C1\",\n  latent = \"U\"\n)\nggdag_adjustment_set(dag_ex3)"
  },
  {
    "objectID": "content/ggdag-examples.html#taxonomy-of-confounding",
    "href": "content/ggdag-examples.html#taxonomy-of-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Taxonomy of confounding",
    "text": "Taxonomy of confounding\nThere is good news. Remember, ultimately are only four basic types of confounding:\n\nThe Fork (omitted variable bias)\n\nconfounder_triangle(x = \"Coffee\",\n                    y = \"Lung Cancer\",\n                    z = \"Smoking\") |&gt;\n  ggdag_dconnected(text = FALSE,\n                   use_labels = \"label\")\n\n\n\n\n\n\n\n\n\n\nThe Pipe (fully mediated effects)\n\nmediation_triangle(\n  x = NULL,\n  y = NULL,\n  m = NULL,\n  x_y_associated = FALSE\n) |&gt;\n  tidy_dagitty(layout = \"nicely\") |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\n\n\nThe Collider\n\ncollider_triangle() |&gt;\n  ggdag_dseparated(controlling_for = \"m\")\n\n\n\n\n\n\n\n\n\n\nConfounding by proxy\nIf we “control for” a descendant of a collider, we will introduce collider bias.\n\ndag_sd &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  D ~ Z,\n  labels = c(\n    \"Z\" = \"Collider\",\n    \"D\" = \"Descendant\",\n    \"X\" = \"X\",\n    \"Y\" = \"Y\"\n  ),\n  exposure = \"X\",\n  outcome = \"Y\"\n) |&gt;\n  control_for(\"D\") \n\ndag_sd |&gt;\n  ggdag_dseparated(\n    from = \"X\",\n    to = \"Y\",\n    controlling_for = \"D\",\n    text = FALSE,\n    use_labels = \"label\"\n  )  +\n  ggtitle(\"X --&gt; Y, controlling for D\",\n          subtitle = \"D induces collider bias\")"
  },
  {
    "objectID": "content/ggdag-examples.html#rules-for-avoiding-confounding",
    "href": "content/ggdag-examples.html#rules-for-avoiding-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Rules for avoiding confounding",
    "text": "Rules for avoiding confounding\nFrom Statistical Rethinking, p.286\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\n\n\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\n\n\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\n\n\nIf there are any open backdoor paths, decide which variable(s) to condition on to close it (if possible).\n\n\n# Examle\n# call ggdag model\n# write relationships:\n\nlibrary(ggdag)\ndg_1 &lt;- ggdag::dagify(\n  b ~  im + ordr + rel + sr  + st,\n  rel ~  age + ses + edu + male + cny,\n  ses ~ cny + edu + age,\n  edu ~ cny + male + age,\n  im ~ mem + rel + cny,\n  mem ~ age + edu + ordr,\n  exposure = \"sr\",\n  outcome = \"b\",\n  labels = c(\n    \"b\" = \"statement credibility\",\n    \"sr\" = \"source\",\n    \"st\" = \"statement\",\n    \"im\" = \"importance\",\n    \"mem\" = \"memory\",\n    \"s\" = \"source\",\n    \"rel\" = \"religious\",\n    \"cny\" = \"country\",\n    \"mem\" = \"memory\",\n    \"male\" = \"male\",\n    \"ordr\" = \"presentation order\",\n    \"ses\" = \"perceived SES\",\n    \"edu\" = \"education\",\n    \"age\" = \"age\"\n  )\n) |&gt;\n  control_for(\"rel\")\n\nggdag::ggdag_collider(dg_1, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n\nNote the colliders induced from the “controls” that we had included in the study:\n\np3 &lt;- ggdag::ggdag_dseparated(\n  dg_1,\n  from = \"sr\",\n  to = \"b\",\n  controlling_for = c(\"ses\", \"age\", \"cny\", \"im\", \"edu\", \"mem\", \"male\", \"rel\"),\n  text = FALSE,\n  use_labels  = \"label\"\n) +\n  theme_dag_blank() +\n  labs(title = \"Collider Confounding occurs when we `control for` a bunch of variables\")\np3\n\n\n\n\n\n\n\n\nHow do we fix the problem? Think hard about the causal network and let ggdag do the work.\n\n# find adjustment set\np2 &lt;- ggdag::ggdag_adjustment_set(dg_1,\n                                  text = FALSE,\n                                  use_labels  = \"label\") +\n  theme_dag_blank() +\n  labs(title = \"Adjustment set\",\n       subtite = \"Model for Source credibility from belief \")\np2\n\nIgnoring unknown labels:\n• subtite : \"Model for Source credibility from belief \""
  },
  {
    "objectID": "content/ggdag-examples.html#inference-depends-on-assumptions-that-are-not-contained-in-the-data.",
    "href": "content/ggdag-examples.html#inference-depends-on-assumptions-that-are-not-contained-in-the-data.",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Inference depends on assumptions that are not contained in the data.",
    "text": "Inference depends on assumptions that are not contained in the data.\n\nregression itself does not provide the evidence you need to justify a causal model. Instead, you need some science.” – Richard McElreath: “Statistical Rethinking, Chapter 6”\n\n\n“…the data alone can never tell you which causal model is correct”- Richard McElreath: “Statistical Rethinking” Chapter 5\n\n\n“The parameter estimates will always depend upon what you believe about the causal model, because typically several (or very many) causal models are consistent with any one set of parameter estimates.” “Statistical Rethinking” Chapter 5\n\nSuppose we assume that the source condition affects religion, say through priming. We then have the following dag:\n\n## adding religion to effect on edu\ndg_3 &lt;- ggdag::dagify(\n  b ~  im + ordr + rel  + st + sr,\n  rel ~  age + ses + edu + male + cny + sr,\n  ses ~ cny + edu + age,\n  edu ~ cny + male + age,\n  im ~ mem + rel + cny,\n  mem ~ age + edu + ordr,\n  exposure = \"rel\",\n  outcome = \"b\",\n  labels = c(\n    \"b\" = \"statement credibility\",\n    \"sr\" = \"source\",\n    \"st\" = \"statement\",\n    \"im\" = \"importance\",\n    \"mem\" = \"memory\",\n    \"s\" = \"source\",\n    \"rel\" = \"religious\",\n    \"cny\" = \"country\",\n    \"mem\" = \"memory\",\n    \"male\" = \"male\",\n    \"ordr\" = \"presentation order\",\n    \"ses\" = \"perceived SES\",\n    \"edu\" = \"education\",\n    \"age\" = \"age\"\n   )\n)|&gt;\n  control_for(\"rel\")\n\nggdag(dg_3, text = FALSE, use_labels  = \"label\")\n\n\n\n\n\n\n\n\nWe turn to our trusted oracle, and and ask: “What do we condition on to obtain an unbiased causal estimate?”\nThe oracle replies:\n\nggdag::ggdag_adjustment_set(\n  dg_3,\n  exposure = \"sr\",\n  outcome = \"b\",\n  text = FALSE,\n  use_labels  = \"label\"\n) +\n  theme_dag_blank() +\n  labs(title = \"Adjustment set\",\n       subtite = \"Model for Source credibility from belief \")\n\nWarning in dag_adjustment_sets(., exposure = exposure, outcome = outcome, : Failed to close backdoor paths. Common reasons include:\n            * graph is not acyclic\n            * backdoor paths are not closeable with given set of variables\n            * necessary variables are unmeasured (latent)\n\n\nIgnoring unknown labels:\n• subtite : \"Model for Source credibility from belief \"\n\n\n\n\n\n\n\n\n\nYour data cannot answer your question."
  },
  {
    "objectID": "content/ggdag-examples.html#more-examples-of-counfoundingde-confounding",
    "href": "content/ggdag-examples.html#more-examples-of-counfoundingde-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "More examples of counfounding/de-confounding",
    "text": "More examples of counfounding/de-confounding\nHere’s another example from recent NZAVS research\n\ntidy_ggdag &lt;- dagify(\n  WB ~ belief + age_within + age_between + partner + nzdep + urban + male + pols + empl,\n  WB ~~ partner,\n  belief ~ age_within + age_between + male + ethn,\n  partner ~ nzdep + age_within + age_between + belief, \n  nzdep ~ empl + age_within + age_between,\n  pols ~ age_within + age_between + empl + ethn,\n  empl ~  edu + ethn + age_within + age_between,\n  exposure =  \"belief\",\n  outcome =   \"WB\")|&gt;\n  tidy_dagitty()\n\n# graph\ntidy_ggdag |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nWe can examine which variables to select, conditional on the causal assumptions of this dag\n\n# graph adjustment sets\nggdag::ggdag_adjustment_set(tidy_ggdag, node_size = 14) + \n  theme(legend.position = \"bottom\") + theme_dag_blank()\n\n\n\n\n\n\n\n\nThis method reveals two adjustments sets: {age, employment, male, political conservativism, and time}, and {age, ethnicty, male, and time.} We report the second set because employment is likely to contain more measurement error: some are not employed because they cannot find employment, others because they are not seeking employment (e.g. retirement).\n\nUnmeasured causes\nReturn to the previous example of R and C on K6 distress, but imagine an underlying common cause of both C and R (say childhood upbringing) called “U”:\n\ndag_m3 &lt;- dagify(\n  K ~ C + R,\n  C ~ U,\n  R ~ U,\n  exposure = \"C\",\n  outcome = \"K\",\n  latent = \"U\"\n) |&gt;\n  tidy_dagitty(layout = \"nicely\")\n\ndag_m3 |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nHow do we assess the relationship of C on K?\nWe can close the backdoor from U through R by conditioning on R\n\nggdag::ggdag_adjustment_set(dag_m3)\n\n\n\n\n\n\n\n\nAside, we can simulate this relationship using the following code:\n\n# C -&gt; K &lt;- R\n# C &lt;- U -&gt; R\nn &lt;- 100\nU &lt;- rnorm( n )\nR &lt;- rnorm( n , U )\nC &lt;- rnorm( n , U )\nK &lt;- rnorm( n , R - C )\nd_sim3 &lt;- data.frame(K = K, R = R, U = U, C = C )\n\n\n\nWhat is the relationship between smoking and cardiac arrest?\nThis example is from the ggdag package, by Malcolm Barrett here\n\nsmoking_ca_dag &lt;- dagify(\n  cardiacarrest ~ cholesterol,\n  cholesterol ~ smoking + weight,\n  smoking ~ unhealthy,\n  weight ~ unhealthy,\n  labels = c(\n    \"cardiacarrest\" = \"Cardiac\\n Arrest\",\n    \"smoking\" = \"Smoking\",\n    \"cholesterol\" = \"Cholesterol\",\n    \"unhealthy\" = \"Unhealthy\\n Lifestyle\",\n    \"weight\" = \"Weight\"\n  ),\n  latent = \"unhealthy\",\n  exposure = \"smoking\",\n  outcome = \"cardiacarrest\"\n)\n\nggdag(smoking_ca_dag,\n      text = FALSE,\n      use_labels = \"label\")\n\n\n\n\n\n\n\n\nWhat do we condition on to close any open backdoor paths, while avoiding colliders? We imagine that unhealthy lifestyle is unmeasured.\n\nggdag_adjustment_set(\n  smoking_ca_dag,\n  text = FALSE,\n  use_labels = \"label\",\n  shadow = TRUE\n)\n\n\n\n\n\n\n\n\nWhat if we control for cholesterol?\n\nggdag_dseparated(\n  smoking_ca_dag,\n  controlling_for = c(\"weight\", \"cholesterol\"),\n  text = FALSE,\n  use_labels = \"label\",\n  collider_lines = FALSE\n)\n\n\n\n\n\n\n\n\n\nControlling for intermediate variables may also induce bias, because it decomposes the total effect of x on y into its parts. (ggdag documentation)\n\n\n\nSelection bias in sampling\nThis example is from https://ggdag.malco.io/articles/bias-structures.html\n\nLet’s say we’re doing a case-control study and want to assess the effect of smoking on glioma, a type of brain cancer. We have a group of glioma patients at a hospital and want to compare them to a group of controls, so we pick people in the hospital with a broken bone, since that seems to have nothing to do with brain cancer. However, perhaps there is some unknown confounding between smoking and being in the hospital with a broken bone, like being prone to reckless behavior. In the normal population, there is no causal effect of smoking on glioma, but in our case, we’re selecting on people who have been hospitalized, which opens up a back-door path:\n\n\ncoords_mine &lt;- tibble::tribble(\n  ~name,           ~x,  ~y,\n  \"glioma\",         1,   2,\n  \"hospitalized\",   2,   3,\n  \"broken_bone\",    3,   2,\n  \"reckless\",       4,   1,\n  \"smoking\",        5,   2\n)\n\ndagify(hospitalized ~ broken_bone + glioma,\n       broken_bone ~ reckless,\n       smoking ~ reckless,\n       labels = c(hospitalized = \"Hospitalization\",\n                  broken_bone = \"Broken Bone\",\n                  glioma = \"Glioma\",\n                  reckless = \"Reckless \\nBehavior\",\n                  smoking = \"Smoking\"),\n       coords = coords_mine) |&gt; \n  ggdag_dconnected(\"glioma\", \"smoking\", controlling_for = \"hospitalized\", \n                   text = FALSE, use_labels = \"label\", collider_lines = FALSE)\n\n\n\n\n\n\n\n\n\nEven though smoking doesn’t actually cause glioma, it will appear as if there is an association. Actually, in this case, it may make smoking appear to be protective against glioma, since controls are more likely to be smokers.\n\n\n\nSelection bias in longitudinal research\nSuppose we want to estimate the effect of ethnicity on ecological orientation in a longitudinal dataset where there is selection bias from homeownership (it is easier to reach homeowners by the mail.)\nSuppose the following DAG:\n\ndag_sel &lt;- dagify(\n  retained ~ homeowner,\n  homeowner ~ income + ethnicity,\n  ecologicalvalues ~  ethnicity + income,\n  labels = c(\n    retained = \"retained\",\n    homeowner = \"homeowner\",\n    ethnicity = \"ethnicity\",\n    income = \"income\",\n    ecologicalvalues = \"Ecological \\n Orientation\"\n  ),\n  exposure = \"ethnicity\",\n  outcome = \"ecologicalvalues\"\n) |&gt;\n  control_for(\"retained\")\n\n\ndag_sel |&gt;\n  ggdag_adjust(\n    \"retained\",\n    layout = \"mds\",\n    text = FALSE,\n    use_labels = \"label\",\n    collider_lines = FALSE\n  )\n\n\n\n\n\n\n\n\nNotice that “retained” falls downstream from a collider, “home ownership”\n\nggdag_collider(dag_sel)\n\n\n\n\n\n\n\n\nBecause we are stratifying on “retained”, we introduce collider bias in our estimate of ethnicity on ecological values.\n\nggdag_dseparated(\n  dag_sel,\n  controlling_for = \"retained\",\n  text = FALSE,\n  use_labels = \"label\",\n  collider_lines = TRUE\n)\n\n\n\n\n\n\n\n\nHowever we have an adjustment set\n\nggdag_adjustment_set(dag_sel)"
  },
  {
    "objectID": "content/ggdag-examples.html#workflow",
    "href": "content/ggdag-examples.html#workflow",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Workflow",
    "text": "Workflow\n\nImport your data\nCheck that data types are correct\nGraph your data\nConsider your question\nIf causal, draw your DAG/S\nExplain your DAG’s\nWrite your model\nRun your model\nGraph and interpret your results\nReturn to your question, and assess what you have learned.\n\n(Typically there are multiple iterations between these steps in your workflow. Annotate your scripts; keep track of your decisions)"
  },
  {
    "objectID": "content/ggdag-examples.html#summary",
    "href": "content/ggdag-examples.html#summary",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Summary",
    "text": "Summary\n\nWe control for variables to avoid omitted variable bias\nOmitted variable bias is real, but also commonplace is included variable bias\nIncluded variable biases arise from “pipes”, “colliders”, and conditioning on descendant of colliders.\nThe ggdag package can help you to obtain causal inference, but it relies on assumptions that are not part of your data.\nClarify your assumption.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Barrett M (2024). _ggdag: Analyze and Create Elegant Directed Acyclic Graphs_. doi:10.32614/CRAN.package.ggdag &lt;https://doi.org/10.32614/CRAN.package.ggdag&gt;, R package version 0.2.13, &lt;https://CRAN.R-project.org/package=ggdag&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Patil I, Ben-Shachar M, Wiernik B, Waggoner P, Makowski D (2021). \"see: An R Package for Visualizing Statistical Models.\" _Journal of Open Source Software_, *6*(64), 3393. doi:10.21105/joss.03393 &lt;https://doi.org/10.21105/joss.03393&gt;.\n  - Müller K, Wickham H (2025). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble &lt;https://doi.org/10.32614/CRAN.package.tibble&gt;, R package version 3.3.0, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2025). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats &lt;https://doi.org/10.32614/CRAN.package.forcats&gt;, R package version 1.0.1, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2025). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr &lt;https://doi.org/10.32614/CRAN.package.stringr&gt;, R package version 1.5.2, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr &lt;https://doi.org/10.32614/CRAN.package.dplyr&gt;, R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr &lt;https://doi.org/10.32614/CRAN.package.purrr&gt;, R package version 1.1.0, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr &lt;https://doi.org/10.32614/CRAN.package.readr&gt;, R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr &lt;https://doi.org/10.32614/CRAN.package.tidyr&gt;, R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n\n\n\nOTHER MATERIAL:\nSelection bias is commonplace."
  },
  {
    "objectID": "content/ggdag-examples.html#collider-bias-within-experiments-1",
    "href": "content/ggdag-examples.html#collider-bias-within-experiments-1",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Collider bias within experiments",
    "text": "Collider bias within experiments\nWe noted that conditioning on a post-treatment variable can induce bias by blocking the path between the experimental manipulation and the outcome. However, such conditioning can open a path even when there is no experimental effect.\n\ndag_ex2 &lt;- dagify(\n  C1 ~ C0 + U,\n  Ch ~ U + R,\n  labels = c(\n    \"R\" = \"Ritual\",\n    \"C1\" = \"Charity-post\",\n    \"C0\" = \"Charity-pre\",\n    \"Ch\" = \"Cohesion\",\n    \"U\" = \"Religiousness (Unmeasured)\"\n  ),\n  exposure = \"R\",\n  outcome = \"C1\",\n  latent = \"U\"\n) |&gt;\n  control_for(c(\"Ch\",\"C0\"))  \n\ndag_ex2 |&gt;\n  ggdag( text = FALSE,\n    use_labels = \"label\")\n\n\n\n\n\n\n\n\nHow do we avoid collider-bias here?\nNote what happens if we condition on cohesion?\n\ndag_ex2 |&gt;\n  ggdag_collider(\n    text = FALSE,\n    use_labels = \"label\"\n  )  +\n  ggtitle(\"Cohesion is a collider that opens a path from ritual to charity\")\n\n\n\n\n\n\n\n\nDon’t condition on a post-treatment variable!\n\ndag_ex3 &lt;- dagify(\n  C1 ~ C0,\n  C1 ~ U,\n  Ch ~ U + R,\n  labels = c(\n    \"R\" = \"Ritual\",\n    \"C1\" = \"Charity-post\",\n    \"C0\" = \"Charity-pre\",\n    \"Ch\" = \"Cohesion\",\n    \"U\" = \"Religiousness (Unmeasured)\"\n  ),\n  exposure = \"R\",\n  outcome = \"C1\",\n  latent = \"U\"\n)\nggdag_adjustment_set(dag_ex3)"
  },
  {
    "objectID": "content/ggdag-examples.html#taxonomy-of-confounding-1",
    "href": "content/ggdag-examples.html#taxonomy-of-confounding-1",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Taxonomy of confounding",
    "text": "Taxonomy of confounding\nThere is good news. Remember, ultimately are only four basic types of confounding:\n\nThe Fork (omitted variable bias)\n\nconfounder_triangle(x = \"Coffee\",\n                    y = \"Lung Cancer\",\n                    z = \"Smoking\") |&gt;\n  ggdag_dconnected(text = FALSE,\n                   use_labels = \"label\")\n\n\n\n\n\n\n\n\n\n\nThe Pipe (fully mediated effects)\n\nmediation_triangle(\n  x = NULL,\n  y = NULL,\n  m = NULL,\n  x_y_associated = FALSE\n) |&gt;\n  tidy_dagitty(layout = \"nicely\") |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\n\n\nThe Collider\n\n\nConfounding by proxy\nIf we “control for” a descendant of a collider, we will introduce collider bias.\n\ndag_sd &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  D ~ Z,\n  labels = c(\n    \"Z\" = \"Collider\",\n    \"D\" = \"Descendant\",\n    \"X\" = \"X\",\n    \"Y\" = \"Y\"\n  ),\n  exposure = \"X\",\n  outcome = \"Y\"\n) |&gt;\n  control_for(\"D\") \n\ndag_sd |&gt;\n  ggdag_dseparated(\n    from = \"X\",\n    to = \"Y\",\n    controlling_for = \"D\",\n    text = FALSE,\n    use_labels = \"label\"\n  )  +\n  ggtitle(\"X --&gt; Y, controlling for D\",\n          subtitle = \"D induces collider bias\")"
  },
  {
    "objectID": "content/S1-multiple-treatment-versions.html",
    "href": "content/S1-multiple-treatment-versions.html",
    "title": "Causal Consistency",
    "section": "",
    "text": "To illustrate how causal consistency can fail, consider a debate in evolutionary human science: do beliefs in “big Gods” influence the development of social complexity? (Whitehouse et al. 2023; Slingerland et al. 2020; Beheim et al. 2021; Watts et al. 2015; Sheehan et al. 2022; Johnson 2015; Norenzayan et al. 2016). Such beliefs vary across time and cultures in intensity, interpretation, institutional management, and ritual practice (De Coulanges 1903; Wheatley 1971; Bulbulia, J. et al. 2013; Geertz et al. 2013), and this variation could itself affect social complexity. Moreover, the beliefs of one society may influence those of others, producing spill-over effects in the treatments to be compared (Murray, Marshall, and Buchanan 2021; Shiba et al. 2023).\nVanderWeele and Hernán’s theory of causal inference under multiple versions of treatment addresses this heterogeneity (Tyler J. VanderWeele 2009, 2018; Tyler J. VanderWeele and Hernan 2013). They show that if treatment variations K are conditionally independent of potential outcomes Y(k) given covariates L, then conditioning on L yields consistent estimates:\n\nK \\coprod Y(k) \\mid L\n Here, K is a “coarsened indicator” for A. While the theory extends to treatment-effect dependencies (i.e., SUTVA violations; Rubin (1980)), its practical application can be difficult.\nFor example, whether weight loss affects health (M. A. Hernán and Taubman 2008) depends on how it occurs. Weight loss from regular exercise or caloric restriction tends to improve health, whereas loss due to disease, depression, famine, or amputation does not. Even if we could consistently estimate the effect of “weight loss” after adjusting for L, interpretation would remain unclear. Precision in defining the intervention—e.g., “weight loss via aerobic exercise sustained for five years, versus no weight loss”—and the outcomes (Miguel A. Hernán, Wang, and Leaf 2022; Murray, Marshall, and Buchanan 2021; M. A. Hernán and Taubman 2008) is essential for both unbiased and interpretable estimates.\nA further challenge is that we cannot confirm from data alone whether measured covariates L suffice to make the multiple versions of treatment independent of counterfactual outcomes. This difficulty intensifies under interference, where effects depend on the distribution of treatments in the population (Bulbulia et al. 2023; Ogburn et al. 2022; Tyler J. VanderWeele and Hernan 2013).\nIn sum, the multiple-versions framework offers a formal path to consistent estimation in observational studies, but treatment heterogeneity and interference remain practical threats. Causal consistency should generally be treated as implausible unless justified. It nonetheless provides a theoretical starting point, identifying half the missing counterfactuals needed for causal contrasts; the remaining half can be addressed through conditional exchangeability."
  },
  {
    "objectID": "content/S1-multiple-treatment-versions.html#s1.-causal-consistency-under-multiple-versions-of-treatment",
    "href": "content/S1-multiple-treatment-versions.html#s1.-causal-consistency-under-multiple-versions-of-treatment",
    "title": "Causal Consistency",
    "section": "",
    "text": "To illustrate how causal consistency can fail, consider a debate in evolutionary human science: do beliefs in “big Gods” influence the development of social complexity? (Whitehouse et al. 2023; Slingerland et al. 2020; Beheim et al. 2021; Watts et al. 2015; Sheehan et al. 2022; Johnson 2015; Norenzayan et al. 2016). Such beliefs vary across time and cultures in intensity, interpretation, institutional management, and ritual practice (De Coulanges 1903; Wheatley 1971; Bulbulia, J. et al. 2013; Geertz et al. 2013), and this variation could itself affect social complexity. Moreover, the beliefs of one society may influence those of others, producing spill-over effects in the treatments to be compared (Murray, Marshall, and Buchanan 2021; Shiba et al. 2023).\nVanderWeele and Hernán’s theory of causal inference under multiple versions of treatment addresses this heterogeneity (Tyler J. VanderWeele 2009, 2018; Tyler J. VanderWeele and Hernan 2013). They show that if treatment variations K are conditionally independent of potential outcomes Y(k) given covariates L, then conditioning on L yields consistent estimates:\n\nK \\coprod Y(k) \\mid L\n Here, K is a “coarsened indicator” for A. While the theory extends to treatment-effect dependencies (i.e., SUTVA violations; Rubin (1980)), its practical application can be difficult.\nFor example, whether weight loss affects health (M. A. Hernán and Taubman 2008) depends on how it occurs. Weight loss from regular exercise or caloric restriction tends to improve health, whereas loss due to disease, depression, famine, or amputation does not. Even if we could consistently estimate the effect of “weight loss” after adjusting for L, interpretation would remain unclear. Precision in defining the intervention—e.g., “weight loss via aerobic exercise sustained for five years, versus no weight loss”—and the outcomes (Miguel A. Hernán, Wang, and Leaf 2022; Murray, Marshall, and Buchanan 2021; M. A. Hernán and Taubman 2008) is essential for both unbiased and interpretable estimates.\nA further challenge is that we cannot confirm from data alone whether measured covariates L suffice to make the multiple versions of treatment independent of counterfactual outcomes. This difficulty intensifies under interference, where effects depend on the distribution of treatments in the population (Bulbulia et al. 2023; Ogburn et al. 2022; Tyler J. VanderWeele and Hernan 2013).\nIn sum, the multiple-versions framework offers a formal path to consistent estimation in observational studies, but treatment heterogeneity and interference remain practical threats. Causal consistency should generally be treated as implausible unless justified. It nonetheless provides a theoretical starting point, identifying half the missing counterfactuals needed for causal contrasts; the remaining half can be addressed through conditional exchangeability."
  },
  {
    "objectID": "index.html#venue-location",
    "href": "index.html#venue-location",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Venue Location",
    "text": "Venue Location\nWe’ll meet on Level 5 of the Redmond Barry Building (Building 115). See the campus map for map for detail by clicking the arrow in the centre of the OpenStreetMap."
  },
  {
    "objectID": "index.html#why-is-causal-inference-important",
    "href": "index.html#why-is-causal-inference-important",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "Why is causal inference important?",
    "text": "Why is causal inference important?\nSuppose you observe that psychology graduate students who own expensive espresso machines publish more papers. Should your department invest in high-end coffee equipment for all incoming students?\nProbably not. This is the difference between prognostic and causal knowledge.\nObservational psychology typically recovers prognostic information – which factors predict which others. The word ‘predictor’ is engrained in our vocabulary. But predictions don’t tell us what would happen if we intervened. Owning an expensive coffee machine predicts publication success, but giving someone an expensive machine wouldn’t cause their productivity to increase. The machine is likely a marker for other factors: being further along in candidature, having more disposable income, or possessing pre-existing caffeinated ambition. Of course the machines might help, but how much?\nThis distinction between prediction and causation has serious implications for intervention design:\n\nWe might observe that adolescents with higher self-esteem have better academic outcomes. But if we were to intervene to boost self-esteem directly (through positive affirmations or praise), would grades improve? Or is self-esteem mostly a marker for other causal factors like stable family environments or prior academic success?\nWe might find that people who practice mindfulness meditation report lower anxiety. But does meditation cause reduced anxiety, or are less anxious people simply more likely to maintain a meditation practice – i.e. to sit, for long periods of time, still?\nWe might observe that children who attend preschool show better social skills. But before recommending universal preschool, we need to ask: would sending any child to preschool produce these benefits, or do families who can afford and prioritise preschool differ in ways that independently foster social development?\n\nPrognostic models tell us what to expect. Causal models tell us what we can change. For psychologists aiming to improve human welfare, that distinction is everything."
  },
  {
    "objectID": "index.html#the-workshop",
    "href": "index.html#the-workshop",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "The workshop",
    "text": "The workshop\nThis full-day, practice-oriented workshop introduces modern causal inference for observational data, with an emphasis on discovering for whom effects are strongest (afternoon).\nWe will:\n\nUnveil a causal workflow that starts with asking a clearly formulated causal question.\nBefore answering a causal question we must consider and explicate identification assumptions. We will consider these assumptions and introduce graphical tools for addressing them. You will discover that clarifying assumptions is not a statistical task.\nOnly after stating our questions and assumptions can we turn to the data and its analysis. By the end of the workshop, you will conduct an analysis using doubly robust machine learning – which has many advantages over standard estimators.\n\nAfter estimation there is communication. I’ll describe how to graphical tools to clarify results audiences with applied interests.\n\n\nObjectives\nYou will leave with:\n\nAn understanding of how to ask, and answer, causal questions.\nAnnotated R code for simulating and estimating average and heterogeneous treatment effects.\n\nA curated reading list for self-study.\n\nExamples of how to communicate causal results to academic, policy, and organisational audiences."
  },
  {
    "objectID": "index.html#my-assumptions-about-you.",
    "href": "index.html#my-assumptions-about-you.",
    "title": "Beyond Correlation: A Practical Introduction to Causal Inference in Observational Social Psychology",
    "section": "My assumptions about you.",
    "text": "My assumptions about you.\nThe workshop assumes familiarity with regression and R, but no prior training in causal inference.\n\n\n\n\n\n\nTipA note on pace\n\n\n\nPlease do not worry if the math or code looks intimidating at first glance. We’ll slow everything down, build each idea together from first principles, and keep the focus on intuition before symbols. Bring your curiosity. If you want to do the data exercise, bring a computer pre-loaded with the workshop R (see👇). By the end of the day you’ll understand that causal workflow is manageable, and even fun, when we unpack it step by step."
  },
  {
    "objectID": "content/00-background.html#references",
    "href": "content/00-background.html#references",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "workshop-scripts/06-interpretation-2.html",
    "href": "workshop-scripts/06-interpretation-2.html",
    "title": "Estimating ATE and CATE using Causal Forests",
    "section": "",
    "text": "[1] '1.0.262'\n\n\nObject read from: /Users/joseph/GIT/2025-workshop-ci-sasp/data/data_standardised.rds\nObject size: 1.45 MB\n👍 Read operation completed successfully!\n\n\nObject read from: /Users/joseph/GIT/2025-workshop-ci-sasp/results/label_mapping.rds\nObject size: 0.00 MB\n👍 Read operation completed successfully!\n\n\nObject read from: /Users/joseph/GIT/2025-workshop-ci-sasp/results/models_binary_cate.qs\nObject size: 188.96 MB\n👍 Read operation completed successfully!\n\n\nWarning in rm(flipped_names): object 'flipped_names' not found\n\n\n                     ATE 2.5 % 97.5 % E_Value E_Val_bound\nVolunteering Hours 0.299 0.270  0.328   1.953       1.876\nCharitable Giving  0.182 0.163  0.201   1.641       1.589\n\n\nConfidence intervals were adjusted for multiple comparisons using Bonferroni correction ($\\alpha$ = 0.05). E‑values were also adjusted using Bonferroni correction ($\\alpha$ = 0.05).\n\nThe following outcomes present reliable causal evidence for average treatment effects (E‑value lower bound &gt; 1.2):\n\n- Volunteering Hours: 0.299(0.270,0.328); on the original scale, 0.388(0.350,0.426). E‑value bound = 1.88\n- Charitable Giving: 0.182(0.163,0.201); on the original scale, 0.228(0.204,0.252). E‑value bound = 1.59\n\nInterpretation is per the stated policy contrast; estimates reflect $\\mathbf{E}[Y(1)] - \\mathbf{E}[Y(0)]$ on standardized outcome units. For continuous outcomes, E-values use an OLS-to-RR approximation with exposure contrast $\\delta = 1$ and outcome SD = 1.\n\n\n### Comparison of targeting operating characteristic (TOC) by rank average treatment effect (RATE): AUTOC vs QINI\n\nWe applied two TOC by RATE methods to the same causal-forest $\\tau(x)$ estimates. AUTOC intensifies focus on top responders via logarithmic weighting, while QINI balances effect size and prevalence via linear weighting.\n\nWe treated the RATE analysis as exploratory and controlled the false discovery rate at q=0.20 over 2 outcomes.\n\nBoth methods yield positive RATE estimates for **Charitable Giving** (AUTOC: 0.259 [95% CI: 0.245, 0.273]; QINI: 0.082 [95% CI: 0.078, 0.086]), indicating robust evidence of treatment effect heterogeneity through this concordance.\n\n**Caution**: We found reliably negative RATE estimates for Volunteering Hours (AUTOC: -0.156 [95% CI: -0.178, -0.134]; QINI: -0.047 [95% CI: -0.053, -0.041]), which strongly caution against CATE-based prioritisation for these outcomes. Targeting individuals based on predicted treatment effects would worsen outcomes compared to random assignment in these cases. \n\n\n'data.frame':   20000 obs. of  3 variables:\n $ proportion: num  0e+00 1e-04 2e-04 3e-04 4e-04 ...\n $ gain      : num  0.000381 0.000766 0.00124 0.001461 0.001312 ...\n $ curve     : chr  \"cate\" \"cate\" \"cate\" \"cate\" ...\nNULL\n\n\n'data.frame':   20000 obs. of  3 variables:\n $ proportion: num  0e+00 1e-04 2e-04 3e-04 4e-04 ...\n $ gain      : num  -0.00027 -0.000216 -0.000228 -0.000336 -0.000259 ...\n $ curve     : chr  \"cate\" \"cate\" \"cate\" \"cate\" ...\nNULL\n\n\n\n\n\nModel\nSpend 10%\nSpend 40%\n\n\n\n\nCharitable Giving\n0.05 [0.05, 0.06]\n0.12 [0.10, 0.13]\n\n\nVolunteering Hours\n-0.01 [-0.02, -0.00]\n-0.01 [-0.03, 0.00]\n\n\n\n\n\nThe QINI curve compares targeted treatment allocation (based on individual treatment effects) versus uniform allocation (based on average treatment effect). Small differences in the expected values of the treatment after the entire population is treated are expected due to out of sample cross-validation (all estimates are tested on data the model has not seen). We computed expected policy effects from prioritising individuals by CATE at 10% and 40% spend levels.\n\n**Charitable Giving**\nAt 10% spend: CATE prioritisation is beneficial (diff: 0.05 [95% CI 0.05, 0.06]). At 40% spend: CATE prioritisation is beneficial (diff: 0.12 [95% CI 0.10, 0.13]).\n\n**Volunteering Hours**\nAt 10% spend: CATE prioritisation worsens outcomes compared to ATE. At 40% spend: No reliable benefits from CATE prioritisation. \n\n\n'data.frame':   20000 obs. of  3 variables:\n $ proportion: num  0e+00 1e-04 2e-04 3e-04 4e-04 ...\n $ gain      : num  0.000381 0.000766 0.00124 0.001461 0.001312 ...\n $ curve     : chr  \"cate\" \"cate\" \"cate\" \"cate\" ...\nNULL\n\n\n#### Findings for Charitable Giving at the end of study\n\nThe policy-tree analysis divides cases on baseline Age. Those who score ≤ -0.726 (original: -0.726) are advised Control. Those above -0.726 (original: -0.726) are advised Treated.\n\n#### Treatment-effect heterogeneity\n\nThe policy tree produces two terminal leaves. Conditional average treatment effects (CATEs) are estimated within each leaf:\n\n* **Leaf 1—baseline age ≤ -0.726** (n = 2,357; 23.6% of the test set): 68.1% were recommended control. The mean outcome under control was -0.101, the mean under treatment was 0.090, yielding a CATE of 0.191.\n\n* **Leaf 2—baseline age &gt; -0.726** (n = 7,643; 76.4% of the test set): 97.1% were recommended treated. The mean outcome under control was -0.108, the mean under treatment was 0.088, yielding a CATE of 0.196.\n\n\n#### Overall policy performance\n\nAcross the full test set (N = 10,000), the policy prescribes control for 1,826 participants (18.3%) and treated for 8,174 participants (81.7%).\n\n\n### Policy Tree Interpretations (depth 1)\n\n#### Findings for Charitable Giving at the end of study\n\nThe policy-tree analysis divides cases on baseline Age. Those who score ≤ -0.726 (original: -0.726) are advised Control. Those above -0.726 (original: -0.726) are advised Treated.\n\n#### Treatment-effect heterogeneity\n\nThe policy tree produces two terminal leaves. Conditional average treatment effects (CATEs) are estimated within each leaf:\n\n* **Leaf 1—baseline age ≤ -0.726** (n = 2,357; 23.6% of the test set): 68.1% were recommended control. The mean outcome under control was -0.101, the mean under treatment was 0.090, yielding a CATE of 0.191.\n\n* **Leaf 2—baseline age &gt; -0.726** (n = 7,643; 76.4% of the test set): 97.1% were recommended treated. The mean outcome under control was -0.108, the mean under treatment was 0.088, yielding a CATE of 0.196.\n\n\n#### Overall policy performance\n\nAcross the full test set (N = 10,000), the policy prescribes control for 1,826 participants (18.3%) and treated for 8,174 participants (81.7%)."
  },
  {
    "objectID": "reading.html#methods-articles",
    "href": "reading.html#methods-articles",
    "title": "Readings",
    "section": "",
    "text": "I’ve written four methods articles. Check the links for further readings. Also my lab resouces at the bottom 👇.\n\nPart 1: Causal Diagrams and Confounding\nPart 2: Interaction, Mediation, and Time-Varying Treatments\nPart 3: Measurement, Measurement Error, Cultural Comparisons\nPart 4: Experiments: What Randomisation Ensures and Does not Ensure\n\n\nFor more resources on causal inference, visit the EPIC Lab Resources page."
  }
]